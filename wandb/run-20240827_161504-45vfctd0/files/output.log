[[36m2024-08-27 16:15:06,686[39m][[34msrc.training_pipeline[39m][[32mINFO[39m] - Starting training!
[[36m2024-08-27 16:15:06,719[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[[36m2024-08-27 16:15:06,720[39m][[34mpytorch_lightning.accelerators.cuda[39m][[32mINFO[39m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓
┃[1m    [22m┃[1m Name                    [22m┃[1m Type                                 [22m┃[1m Params [22m┃[1m Mode  [22m┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩
│ 0  │ net                     │ MLP                                  │ 93.2 K │ train │
│ 1  │ net.hiddens             │ ModuleList                           │ 93.2 K │ train │
│ 2  │ net.hiddens.0           │ Linear                               │ 26.9 K │ train │
│ 3  │ net.hiddens.1           │ Linear                               │ 65.8 K │ train │
│ 4  │ net.hiddens.2           │ Linear                               │    514 │ train │
│ 5  │ train_acc               │ BinaryAccuracy                       │      0 │ train │
│ 6  │ val_acc                 │ BinaryAccuracy                       │      0 │ train │
│ 7  │ test_acc                │ BinaryAccuracy                       │      0 │ train │
│ 8  │ train_ece               │ MulticlassCalibrationError           │      0 │ train │
│ 9  │ val_ece                 │ MulticlassCalibrationError           │      0 │ train │
│ 10 │ test_ece                │ MulticlassCalibrationError           │      0 │ train │
│ 11 │ train_entropy           │ ShannonEntropyError                  │      0 │ train │
│ 12 │ val_entropy             │ ShannonEntropyError                  │      0 │ train │
│ 13 │ test_entropy            │ ShannonEntropyError                  │      0 │ train │
│ 14 │ test_kcal               │ ClassificationKernelCalibrationError │      0 │ train │
│ 15 │ val_acc_best            │ MaxMetric                            │      0 │ train │
│ 16 │ val_ece_best            │ MinMetric                            │      0 │ train │
│ 17 │ val_entropy_best        │ MinMetric                            │      0 │ train │
│ 18 │ test_calibrated_acc     │ BinaryAccuracy                       │      0 │ train │
│ 19 │ test_calibrated_ece     │ MulticlassCalibrationError           │      0 │ train │
│ 20 │ test_calibrated_entropy │ ShannonEntropyError                  │      0 │ train │
│ 21 │ test_calibrated_kcal    │ ClassificationKernelCalibrationError │      0 │ train │
└────┴─────────────────────────┴──────────────────────────────────────┴────────┴───────┘
[1mTrainable params[22m: 93.2 K
[1mNon-trainable params[22m: 0
[1mTotal params[22m: 93.2 K
[1mTotal estimated model params size (MB)[22m: 0
[1mModules in train mode[22m: 22
[1mModules in eval mode[22m: 0
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers
which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(68)__call__()
-> for op in self.operands:
[1m([22mPdb[1m)
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /local/scratch/a/ko120/DM-Benchmark/checkpoints exists and is not empty.
['x', 'y']
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(69)__call__()
-> scaler = self.scalers[op]
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(70)__call__()
-> bandwidth = self.bandwidths[op]
[1m([22mPdb[1m)
1.0
[1m([22mPdb[1m)
*** NameError: name 'v' is not defined
[1m([22mPdb[1m)
10.0
[1m([22mPdb[1m)
2
[1m([22mPdb[1m)
tensor([[-1.0231e+00, -5.8862e-02, -1.2241e+00, -1.4744e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01,  5.2707e+00,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01, -6.9585e-01,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02, -9.3475e-01, -1.1144e-01,  1.4495e+00, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02,  2.5464e+00, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
         -8.3914e-01, -5.8682e-01, -1.7427e-01,  2.3987e+00, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [ 6.5188e-01,  1.0059e+00, -4.7574e-02, -1.4744e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01, -6.9585e-01,
         -2.3878e-01, -3.8655e-02, -1.3527e-01,  1.8753e+00, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
          2.5435e+00, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [ 3.3166e+00, -1.1203e+00, -4.3974e-01, -1.4744e-01, -2.1859e-01,
         -1.7472e+00, -1.7965e-01, -2.7124e-01, -1.6821e+00, -1.9215e-01,
          3.3271e+00, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01,  1.4371e+00,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01, -3.9057e-01,
          5.4312e+00, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [ 4.2347e-01,  6.3901e-02,  1.1289e+00,  8.3794e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01,  2.2315e+00, -1.1220e-01, -6.9585e-01,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02,  2.5464e+00, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [ 1.4132e+00,  2.0721e-02, -4.7574e-02, -1.4744e-01, -2.1859e-01,
          7.5701e-01, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01, -6.9585e-01,
         -2.3878e-01, -3.8655e-02, -1.3527e-01,  1.8753e+00, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01,  6.7702e+00, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [-5.6629e-01, -9.2879e-01, -4.3974e-01, -1.4744e-01, -2.1859e-01,
          1.5917e+00, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01,  1.4371e+00,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02, -9.3475e-01, -1.1144e-01,  1.4495e+00, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01,  2.5604e+00,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
         -8.3914e-01, -5.8682e-01, -1.7427e-01,  2.3987e+00, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [-5.6629e-01, -1.2372e+00, -4.3974e-01, -1.4744e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01,  1.4371e+00,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01,  2.6657e+00, -1.7277e-02, -3.9270e-01, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [-1.0948e-01,  2.3610e-01, -4.3974e-01, -1.4744e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01,  1.4371e+00,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01, -3.9057e-01,
         -1.8412e-01, -2.1646e-01,  3.7871e+00, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02]], device='cuda:0')
[1m([22mPdb[1m)
torch.Size([8, 104])
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(71)__call__()
-> if op == 'x':
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(73)__call__()
-> assert x.dim() == 2
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(74)__call__()
-> loss_mat = loss_mat2 = loss_mat3 = scaler * self.kernel_fun[op](x, x, bandwidth)
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(94)__call__()
-> for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(95)__call__()
-> if loss_mats[i] is None:
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(96)__call__()
-> loss_mats[i] = value
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(94)__call__()
-> for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(95)__call__()
-> if loss_mats[i] is None:
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(96)__call__()
-> loss_mats[i] = value
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(94)__call__()
-> for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(95)__call__()
-> if loss_mats[i] is None:
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(96)__call__()
-> loss_mats[i] = value
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(94)__call__()
-> for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(68)__call__()
-> for op in self.operands:
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(69)__call__()
-> scaler = self.scalers[op]
[1m([22mPdb[1m)
<module 'torch.nn' from '/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/torch/nn/__init__.py'>
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(70)__call__()
-> bandwidth = self.bandwidths[op]
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(71)__call__()
-> if op == 'x':
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(75)__call__()
-> elif op == 'y':
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(77)__call__()
-> num_classes = logits.shape[-1]
[1m([22mPdb[1m)
2
[1m([22mPdb[1m)
tensor([[-0.0069, -0.0057],
        [-0.0029, -0.0040],
        [-0.0004, -0.0020],
        [-0.0023, -0.0008],
        [ 0.2789, -0.0063],
        [-0.0020, -0.0030],
        [-0.0022, -0.0029],
        [-0.0017, -0.0016]], device='cuda:0')
[1m([22mPdb[1m)
*** NameError: name 'num_classes' is not defined
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(78)__call__()
-> y_all = torch.eye(num_classes).to(logits.device)
[1m([22mPdb[1m)
tensor([[1., 0.],
        [0., 1.]])
[1m([22mPdb[1m)
*** NameError: name 'y_all' is not defined
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(79)__call__()
-> k_yy = self.kernel_fun[op](y_all, y_all, bandwidth)
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(80)__call__()
-> q_y = F.softmax(logits, dim=-1)
[1m([22mPdb[1m)
tensor([[1., 0.],
        [0., 1.]], device='cuda:0')
[1m([22mPdb[1m)
<module 'torch.nn' from '/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/torch/nn/__init__.py'>
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(81)__call__()
-> q_yy = torch.einsum('ic,jd->ijcd', q_y, q_y)
[1m([22mPdb[1m)
tensor([[0.4997, 0.5003],
        [0.5003, 0.4997],
        [0.5004, 0.4996],
        [0.4996, 0.5004],
        [0.5708, 0.4292],
        [0.5002, 0.4998],
        [0.5002, 0.4998],
        [0.5000, 0.5000]], device='cuda:0')
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(82)__call__()
-> total_yy = q_yy * k_yy.unsqueeze(0)
[1m([22mPdb[1m)
tensor([[[[0.2497, 0.2500],
          [0.2500, 0.2503]],
         [[0.2500, 0.2497],
          [0.2503, 0.2500]],
         [[0.2500, 0.2497],
          [0.2503, 0.2500]],
         [[0.2497, 0.2500],
          [0.2500, 0.2503]],
         [[0.2852, 0.2145],
          [0.2856, 0.2147]],
         [[0.2500, 0.2497],
          [0.2503, 0.2500]],
         [[0.2499, 0.2498],
          [0.2502, 0.2501]],
         [[0.2498, 0.2499],
          [0.2501, 0.2502]]],
        [[[0.2500, 0.2503],
          [0.2497, 0.2500]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2500, 0.2503],
          [0.2497, 0.2500]],
         [[0.2856, 0.2147],
          [0.2852, 0.2145]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2502, 0.2501],
          [0.2499, 0.2498]],
         [[0.2501, 0.2502],
          [0.2498, 0.2499]]],
        [[[0.2500, 0.2503],
          [0.2497, 0.2500]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2504, 0.2500],
          [0.2500, 0.2496]],
         [[0.2500, 0.2504],
          [0.2496, 0.2500]],
         [[0.2856, 0.2148],
          [0.2852, 0.2144]],
         [[0.2503, 0.2501],
          [0.2499, 0.2497]],
         [[0.2503, 0.2501],
          [0.2499, 0.2497]],
         [[0.2502, 0.2502],
          [0.2498, 0.2498]]],
        [[[0.2497, 0.2500],
          [0.2500, 0.2503]],
         [[0.2500, 0.2497],
          [0.2503, 0.2500]],
         [[0.2500, 0.2496],
          [0.2504, 0.2500]],
         [[0.2496, 0.2500],
          [0.2500, 0.2504]],
         [[0.2852, 0.2144],
          [0.2856, 0.2148]],
         [[0.2499, 0.2497],
          [0.2503, 0.2501]],
         [[0.2499, 0.2497],
          [0.2503, 0.2501]],
         [[0.2498, 0.2498],
          [0.2502, 0.2502]]],
        [[[0.2852, 0.2856],
          [0.2145, 0.2147]],
         [[0.2856, 0.2852],
          [0.2147, 0.2145]],
         [[0.2856, 0.2852],
          [0.2148, 0.2144]],
         [[0.2852, 0.2856],
          [0.2144, 0.2148]],
         [[0.3258, 0.2450],
          [0.2450, 0.1842]],
         [[0.2855, 0.2853],
          [0.2147, 0.2145]],
         [[0.2855, 0.2853],
          [0.2147, 0.2145]],
         [[0.2854, 0.2854],
          [0.2146, 0.2146]]],
        [[[0.2500, 0.2503],
          [0.2497, 0.2500]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2503, 0.2499],
          [0.2501, 0.2497]],
         [[0.2499, 0.2503],
          [0.2497, 0.2501]],
         [[0.2855, 0.2147],
          [0.2853, 0.2145]],
         [[0.2502, 0.2500],
          [0.2500, 0.2498]],
         [[0.2502, 0.2500],
          [0.2500, 0.2498]],
         [[0.2501, 0.2501],
          [0.2499, 0.2499]]],
        [[[0.2499, 0.2502],
          [0.2498, 0.2501]],
         [[0.2502, 0.2499],
          [0.2501, 0.2498]],
         [[0.2503, 0.2499],
          [0.2501, 0.2497]],
         [[0.2499, 0.2503],
          [0.2497, 0.2501]],
         [[0.2855, 0.2147],
          [0.2853, 0.2145]],
         [[0.2502, 0.2500],
          [0.2500, 0.2498]],
         [[0.2502, 0.2500],
          [0.2500, 0.2498]],
         [[0.2501, 0.2501],
          [0.2499, 0.2499]]],
        [[[0.2498, 0.2501],
          [0.2499, 0.2502]],
         [[0.2501, 0.2498],
          [0.2502, 0.2499]],
         [[0.2502, 0.2498],
          [0.2502, 0.2498]],
         [[0.2498, 0.2502],
          [0.2498, 0.2502]],
         [[0.2854, 0.2146],
          [0.2854, 0.2146]],
         [[0.2501, 0.2499],
          [0.2501, 0.2499]],
         [[0.2501, 0.2499],
          [0.2501, 0.2499]],
         [[0.2500, 0.2500],
          [0.2500, 0.2500]]]], device='cuda:0')
[1m([22mPdb[1m)
torch.Size([8, 8, 2, 2])
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(84)__call__()
-> k_yj = k_yy[:,y].T
[1m([22mPdb[1m)
tensor([[[[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2852, 0.0000],
          [0.0000, 0.2147]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2856, 0.0000],
          [0.0000, 0.2145]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2504, 0.0000],
          [0.0000, 0.2496]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2856, 0.0000],
          [0.0000, 0.2144]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]]],
        [[[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2496, 0.0000],
          [0.0000, 0.2504]],
         [[0.2852, 0.0000],
          [0.0000, 0.2148]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]]],
        [[[0.2852, 0.0000],
          [0.0000, 0.2147]],
         [[0.2856, 0.0000],
          [0.0000, 0.2145]],
         [[0.2856, 0.0000],
          [0.0000, 0.2144]],
         [[0.2852, 0.0000],
          [0.0000, 0.2148]],
         [[0.3258, 0.0000],
          [0.0000, 0.1842]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2854, 0.0000],
          [0.0000, 0.2146]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2498, 0.0000],
          [0.0000, 0.2502]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]],
         [[0.2854, 0.0000],
          [0.0000, 0.2146]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]]]], device='cuda:0')
[1m([22mPdb[1m)
tensor([0, 0, 0, 1, 0, 0, 0, 0], device='cuda:0')
[1m([22mPdb[1m)
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
[1m([22mPdb[1m)
tensor([[0.4997, 0.5003],
        [0.5003, 0.4997],
        [0.5004, 0.4996],
        [0.4996, 0.5004],
        [0.5708, 0.4292],
        [0.5002, 0.4998],
        [0.5002, 0.4998],
        [0.5000, 0.5000]], device='cuda:0')
[1m([22mPdb[1m)
tensor([[[1., 0.],
         [0., 1.]]], device='cuda:0')
[1m([22mPdb[1m)
tensor([[[[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2852, 0.0000],
          [0.0000, 0.2147]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2856, 0.0000],
          [0.0000, 0.2145]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2504, 0.0000],
          [0.0000, 0.2496]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2856, 0.0000],
          [0.0000, 0.2144]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]]],
        [[[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2496, 0.0000],
          [0.0000, 0.2504]],
         [[0.2852, 0.0000],
          [0.0000, 0.2148]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]]],
        [[[0.2852, 0.0000],
          [0.0000, 0.2147]],
         [[0.2856, 0.0000],
          [0.0000, 0.2145]],
         [[0.2856, 0.0000],
          [0.0000, 0.2144]],
         [[0.2852, 0.0000],
          [0.0000, 0.2148]],
         [[0.3258, 0.0000],
          [0.0000, 0.1842]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2854, 0.0000],
          [0.0000, 0.2146]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2498, 0.0000],
          [0.0000, 0.2502]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]],
         [[0.2854, 0.0000],
          [0.0000, 0.2146]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]]]], device='cuda:0')
