[[36m2024-08-27 16:15:06,686[39m][[34msrc.training_pipeline[39m][[32mINFO[39m] - Starting training!
[[36m2024-08-27 16:15:06,719[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[[36m2024-08-27 16:15:06,720[39m][[34mpytorch_lightning.accelerators.cuda[39m][[32mINFO[39m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1m    [22mâ”ƒ[1m Name                    [22mâ”ƒ[1m Type                                 [22mâ”ƒ[1m Params [22mâ”ƒ[1m Mode  [22mâ”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net                     â”‚ MLP                                  â”‚ 93.2 K â”‚ train â”‚
â”‚ 1  â”‚ net.hiddens             â”‚ ModuleList                           â”‚ 93.2 K â”‚ train â”‚
â”‚ 2  â”‚ net.hiddens.0           â”‚ Linear                               â”‚ 26.9 K â”‚ train â”‚
â”‚ 3  â”‚ net.hiddens.1           â”‚ Linear                               â”‚ 65.8 K â”‚ train â”‚
â”‚ 4  â”‚ net.hiddens.2           â”‚ Linear                               â”‚    514 â”‚ train â”‚
â”‚ 5  â”‚ train_acc               â”‚ BinaryAccuracy                       â”‚      0 â”‚ train â”‚
â”‚ 6  â”‚ val_acc                 â”‚ BinaryAccuracy                       â”‚      0 â”‚ train â”‚
â”‚ 7  â”‚ test_acc                â”‚ BinaryAccuracy                       â”‚      0 â”‚ train â”‚
â”‚ 8  â”‚ train_ece               â”‚ MulticlassCalibrationError           â”‚      0 â”‚ train â”‚
â”‚ 9  â”‚ val_ece                 â”‚ MulticlassCalibrationError           â”‚      0 â”‚ train â”‚
â”‚ 10 â”‚ test_ece                â”‚ MulticlassCalibrationError           â”‚      0 â”‚ train â”‚
â”‚ 11 â”‚ train_entropy           â”‚ ShannonEntropyError                  â”‚      0 â”‚ train â”‚
â”‚ 12 â”‚ val_entropy             â”‚ ShannonEntropyError                  â”‚      0 â”‚ train â”‚
â”‚ 13 â”‚ test_entropy            â”‚ ShannonEntropyError                  â”‚      0 â”‚ train â”‚
â”‚ 14 â”‚ test_kcal               â”‚ ClassificationKernelCalibrationError â”‚      0 â”‚ train â”‚
â”‚ 15 â”‚ val_acc_best            â”‚ MaxMetric                            â”‚      0 â”‚ train â”‚
â”‚ 16 â”‚ val_ece_best            â”‚ MinMetric                            â”‚      0 â”‚ train â”‚
â”‚ 17 â”‚ val_entropy_best        â”‚ MinMetric                            â”‚      0 â”‚ train â”‚
â”‚ 18 â”‚ test_calibrated_acc     â”‚ BinaryAccuracy                       â”‚      0 â”‚ train â”‚
â”‚ 19 â”‚ test_calibrated_ece     â”‚ MulticlassCalibrationError           â”‚      0 â”‚ train â”‚
â”‚ 20 â”‚ test_calibrated_entropy â”‚ ShannonEntropyError                  â”‚      0 â”‚ train â”‚
â”‚ 21 â”‚ test_calibrated_kcal    â”‚ ClassificationKernelCalibrationError â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 93.2 K
[1mNon-trainable params[22m: 0
[1mTotal params[22m: 93.2 K
[1mTotal estimated model params size (MB)[22m: 0
[1mModules in train mode[22m: 22
[1mModules in eval mode[22m: 0
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers
which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(68)__call__()
-> for op in self.operands:
[1m([22mPdb[1m)
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /local/scratch/a/ko120/DM-Benchmark/checkpoints exists and is not empty.
['x', 'y']
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(69)__call__()
-> scaler = self.scalers[op]
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(70)__call__()
-> bandwidth = self.bandwidths[op]
[1m([22mPdb[1m)
1.0
[1m([22mPdb[1m)
*** NameError: name 'v' is not defined
[1m([22mPdb[1m)
10.0
[1m([22mPdb[1m)
2
[1m([22mPdb[1m)
tensor([[-1.0231e+00, -5.8862e-02, -1.2241e+00, -1.4744e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01,  5.2707e+00,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01, -6.9585e-01,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02, -9.3475e-01, -1.1144e-01,  1.4495e+00, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02,  2.5464e+00, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
         -8.3914e-01, -5.8682e-01, -1.7427e-01,  2.3987e+00, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [ 6.5188e-01,  1.0059e+00, -4.7574e-02, -1.4744e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01, -6.9585e-01,
         -2.3878e-01, -3.8655e-02, -1.3527e-01,  1.8753e+00, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
          2.5435e+00, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [ 3.3166e+00, -1.1203e+00, -4.3974e-01, -1.4744e-01, -2.1859e-01,
         -1.7472e+00, -1.7965e-01, -2.7124e-01, -1.6821e+00, -1.9215e-01,
          3.3271e+00, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01,  1.4371e+00,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01, -3.9057e-01,
          5.4312e+00, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [ 4.2347e-01,  6.3901e-02,  1.1289e+00,  8.3794e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01,  2.2315e+00, -1.1220e-01, -6.9585e-01,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02,  2.5464e+00, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [ 1.4132e+00,  2.0721e-02, -4.7574e-02, -1.4744e-01, -2.1859e-01,
          7.5701e-01, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01, -6.9585e-01,
         -2.3878e-01, -3.8655e-02, -1.3527e-01,  1.8753e+00, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01,  6.7702e+00, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [-5.6629e-01, -9.2879e-01, -4.3974e-01, -1.4744e-01, -2.1859e-01,
          1.5917e+00, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01,  1.4371e+00,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02, -9.3475e-01, -1.1144e-01,  1.4495e+00, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01,  2.5604e+00,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
         -8.3914e-01, -5.8682e-01, -1.7427e-01,  2.3987e+00, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [-5.6629e-01, -1.2372e+00, -4.3974e-01, -1.4744e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01,  1.4371e+00,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01,  2.6657e+00, -1.7277e-02, -3.9270e-01, -3.9057e-01,
         -1.8412e-01, -2.1646e-01, -2.6406e-01, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02],
        [-1.0948e-01,  2.3610e-01, -4.3974e-01, -1.4744e-01, -2.1859e-01,
         -7.7734e-02, -1.7965e-01, -2.7124e-01,  5.9448e-01, -1.9215e-01,
         -3.0056e-01, -2.1043e-01, -2.1549e-02, -1.6717e-01, -1.8973e-01,
         -1.1251e-01, -7.0933e-02, -9.8186e-02, -1.3717e-01, -1.2376e-01,
         -1.8594e-01, -2.1283e-01, -4.4812e-01, -1.1220e-01,  1.4371e+00,
         -2.3878e-01, -3.8655e-02, -1.3527e-01, -5.3326e-01, -4.0299e-01,
         -2.6396e-02,  1.0698e+00, -1.1144e-01, -6.8987e-01, -1.7925e-01,
         -1.6790e-01, -3.7514e-01, -1.7277e-02, -3.9270e-01, -3.9057e-01,
         -1.8412e-01, -2.1646e-01,  3.7871e+00, -3.4523e-01, -6.9019e-02,
         -3.9315e-01, -1.4771e-01, -3.6722e-01, -1.7658e-01, -2.3449e-01,
          1.1917e+00, -5.8682e-01, -1.7427e-01, -4.1689e-01, -3.4523e-01,
         -2.2112e-01, -9.7841e-02, -1.7487e-01, -3.2096e-01, -8.7851e-02,
          4.0382e-01, -6.9281e-01,  6.9281e-01, -2.4436e-02, -5.9667e-02,
         -4.7535e-02, -4.3129e-02, -5.5313e-02, -4.7184e-02, -2.9933e-02,
         -5.7675e-02, -5.3474e-02, -2.9933e-02, -6.5283e-02, -3.1023e-02,
         -4.5750e-02, -3.7342e-02, -5.7581e-03, -1.9950e-02, -2.5106e-02,
         -2.0765e-02, -5.7675e-02, -3.7342e-02, -2.8219e-02, -4.7535e-02,
         -5.1569e-02, -4.4271e-02, -2.3747e-02, -1.4367e-01, -3.3095e-02,
         -2.1549e-02, -3.1553e-02, -7.9197e-02, -4.3129e-02, -3.3593e-02,
         -6.0224e-02, -1.9101e-02, -4.8575e-02, -3.7342e-02, -2.3747e-02,
         -2.4436e-02,  3.1087e-01, -4.6113e-02, -2.3038e-02]], device='cuda:0')
[1m([22mPdb[1m)
torch.Size([8, 104])
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(71)__call__()
-> if op == 'x':
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(73)__call__()
-> assert x.dim() == 2
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(74)__call__()
-> loss_mat = loss_mat2 = loss_mat3 = scaler * self.kernel_fun[op](x, x, bandwidth)
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(94)__call__()
-> for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(95)__call__()
-> if loss_mats[i] is None:
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(96)__call__()
-> loss_mats[i] = value
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(94)__call__()
-> for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(95)__call__()
-> if loss_mats[i] is None:
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(96)__call__()
-> loss_mats[i] = value
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(94)__call__()
-> for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(95)__call__()
-> if loss_mats[i] is None:
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(96)__call__()
-> loss_mats[i] = value
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(94)__call__()
-> for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(68)__call__()
-> for op in self.operands:
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(69)__call__()
-> scaler = self.scalers[op]
[1m([22mPdb[1m)
<module 'torch.nn' from '/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/torch/nn/__init__.py'>
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(70)__call__()
-> bandwidth = self.bandwidths[op]
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(71)__call__()
-> if op == 'x':
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(75)__call__()
-> elif op == 'y':
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(77)__call__()
-> num_classes = logits.shape[-1]
[1m([22mPdb[1m)
2
[1m([22mPdb[1m)
tensor([[-0.0069, -0.0057],
        [-0.0029, -0.0040],
        [-0.0004, -0.0020],
        [-0.0023, -0.0008],
        [ 0.2789, -0.0063],
        [-0.0020, -0.0030],
        [-0.0022, -0.0029],
        [-0.0017, -0.0016]], device='cuda:0')
[1m([22mPdb[1m)
*** NameError: name 'num_classes' is not defined
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(78)__call__()
-> y_all = torch.eye(num_classes).to(logits.device)
[1m([22mPdb[1m)
tensor([[1., 0.],
        [0., 1.]])
[1m([22mPdb[1m)
*** NameError: name 'y_all' is not defined
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(79)__call__()
-> k_yy = self.kernel_fun[op](y_all, y_all, bandwidth)
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(80)__call__()
-> q_y = F.softmax(logits, dim=-1)
[1m([22mPdb[1m)
tensor([[1., 0.],
        [0., 1.]], device='cuda:0')
[1m([22mPdb[1m)
<module 'torch.nn' from '/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/torch/nn/__init__.py'>
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(81)__call__()
-> q_yy = torch.einsum('ic,jd->ijcd', q_y, q_y)
[1m([22mPdb[1m)
tensor([[0.4997, 0.5003],
        [0.5003, 0.4997],
        [0.5004, 0.4996],
        [0.4996, 0.5004],
        [0.5708, 0.4292],
        [0.5002, 0.4998],
        [0.5002, 0.4998],
        [0.5000, 0.5000]], device='cuda:0')
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(82)__call__()
-> total_yy = q_yy * k_yy.unsqueeze(0)
[1m([22mPdb[1m)
tensor([[[[0.2497, 0.2500],
          [0.2500, 0.2503]],
         [[0.2500, 0.2497],
          [0.2503, 0.2500]],
         [[0.2500, 0.2497],
          [0.2503, 0.2500]],
         [[0.2497, 0.2500],
          [0.2500, 0.2503]],
         [[0.2852, 0.2145],
          [0.2856, 0.2147]],
         [[0.2500, 0.2497],
          [0.2503, 0.2500]],
         [[0.2499, 0.2498],
          [0.2502, 0.2501]],
         [[0.2498, 0.2499],
          [0.2501, 0.2502]]],
        [[[0.2500, 0.2503],
          [0.2497, 0.2500]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2500, 0.2503],
          [0.2497, 0.2500]],
         [[0.2856, 0.2147],
          [0.2852, 0.2145]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2502, 0.2501],
          [0.2499, 0.2498]],
         [[0.2501, 0.2502],
          [0.2498, 0.2499]]],
        [[[0.2500, 0.2503],
          [0.2497, 0.2500]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2504, 0.2500],
          [0.2500, 0.2496]],
         [[0.2500, 0.2504],
          [0.2496, 0.2500]],
         [[0.2856, 0.2148],
          [0.2852, 0.2144]],
         [[0.2503, 0.2501],
          [0.2499, 0.2497]],
         [[0.2503, 0.2501],
          [0.2499, 0.2497]],
         [[0.2502, 0.2502],
          [0.2498, 0.2498]]],
        [[[0.2497, 0.2500],
          [0.2500, 0.2503]],
         [[0.2500, 0.2497],
          [0.2503, 0.2500]],
         [[0.2500, 0.2496],
          [0.2504, 0.2500]],
         [[0.2496, 0.2500],
          [0.2500, 0.2504]],
         [[0.2852, 0.2144],
          [0.2856, 0.2148]],
         [[0.2499, 0.2497],
          [0.2503, 0.2501]],
         [[0.2499, 0.2497],
          [0.2503, 0.2501]],
         [[0.2498, 0.2498],
          [0.2502, 0.2502]]],
        [[[0.2852, 0.2856],
          [0.2145, 0.2147]],
         [[0.2856, 0.2852],
          [0.2147, 0.2145]],
         [[0.2856, 0.2852],
          [0.2148, 0.2144]],
         [[0.2852, 0.2856],
          [0.2144, 0.2148]],
         [[0.3258, 0.2450],
          [0.2450, 0.1842]],
         [[0.2855, 0.2853],
          [0.2147, 0.2145]],
         [[0.2855, 0.2853],
          [0.2147, 0.2145]],
         [[0.2854, 0.2854],
          [0.2146, 0.2146]]],
        [[[0.2500, 0.2503],
          [0.2497, 0.2500]],
         [[0.2503, 0.2500],
          [0.2500, 0.2497]],
         [[0.2503, 0.2499],
          [0.2501, 0.2497]],
         [[0.2499, 0.2503],
          [0.2497, 0.2501]],
         [[0.2855, 0.2147],
          [0.2853, 0.2145]],
         [[0.2502, 0.2500],
          [0.2500, 0.2498]],
         [[0.2502, 0.2500],
          [0.2500, 0.2498]],
         [[0.2501, 0.2501],
          [0.2499, 0.2499]]],
        [[[0.2499, 0.2502],
          [0.2498, 0.2501]],
         [[0.2502, 0.2499],
          [0.2501, 0.2498]],
         [[0.2503, 0.2499],
          [0.2501, 0.2497]],
         [[0.2499, 0.2503],
          [0.2497, 0.2501]],
         [[0.2855, 0.2147],
          [0.2853, 0.2145]],
         [[0.2502, 0.2500],
          [0.2500, 0.2498]],
         [[0.2502, 0.2500],
          [0.2500, 0.2498]],
         [[0.2501, 0.2501],
          [0.2499, 0.2499]]],
        [[[0.2498, 0.2501],
          [0.2499, 0.2502]],
         [[0.2501, 0.2498],
          [0.2502, 0.2499]],
         [[0.2502, 0.2498],
          [0.2502, 0.2498]],
         [[0.2498, 0.2502],
          [0.2498, 0.2502]],
         [[0.2854, 0.2146],
          [0.2854, 0.2146]],
         [[0.2501, 0.2499],
          [0.2501, 0.2499]],
         [[0.2501, 0.2499],
          [0.2501, 0.2499]],
         [[0.2500, 0.2500],
          [0.2500, 0.2500]]]], device='cuda:0')
[1m([22mPdb[1m)
torch.Size([8, 8, 2, 2])
[1m([22mPdb[1m)
> /local/scratch/a/ko120/DM-Benchmark/src/metrics/train_metrics.py(84)__call__()
-> k_yj = k_yy[:,y].T
[1m([22mPdb[1m)
tensor([[[[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2852, 0.0000],
          [0.0000, 0.2147]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2856, 0.0000],
          [0.0000, 0.2145]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2504, 0.0000],
          [0.0000, 0.2496]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2856, 0.0000],
          [0.0000, 0.2144]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]]],
        [[[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2496, 0.0000],
          [0.0000, 0.2504]],
         [[0.2852, 0.0000],
          [0.0000, 0.2148]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]]],
        [[[0.2852, 0.0000],
          [0.0000, 0.2147]],
         [[0.2856, 0.0000],
          [0.0000, 0.2145]],
         [[0.2856, 0.0000],
          [0.0000, 0.2144]],
         [[0.2852, 0.0000],
          [0.0000, 0.2148]],
         [[0.3258, 0.0000],
          [0.0000, 0.1842]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2854, 0.0000],
          [0.0000, 0.2146]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2498, 0.0000],
          [0.0000, 0.2502]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]],
         [[0.2854, 0.0000],
          [0.0000, 0.2146]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]]]], device='cuda:0')
[1m([22mPdb[1m)
tensor([0, 0, 0, 1, 0, 0, 0, 0], device='cuda:0')
[1m([22mPdb[1m)
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
[1m([22mPdb[1m)
tensor([[0.4997, 0.5003],
        [0.5003, 0.4997],
        [0.5004, 0.4996],
        [0.4996, 0.5004],
        [0.5708, 0.4292],
        [0.5002, 0.4998],
        [0.5002, 0.4998],
        [0.5000, 0.5000]], device='cuda:0')
[1m([22mPdb[1m)
tensor([[[1., 0.],
         [0., 1.]]], device='cuda:0')
[1m([22mPdb[1m)
tensor([[[[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2852, 0.0000],
          [0.0000, 0.2147]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2856, 0.0000],
          [0.0000, 0.2145]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2504, 0.0000],
          [0.0000, 0.2496]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2856, 0.0000],
          [0.0000, 0.2144]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]]],
        [[[0.2497, 0.0000],
          [0.0000, 0.2503]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2496, 0.0000],
          [0.0000, 0.2504]],
         [[0.2852, 0.0000],
          [0.0000, 0.2148]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]]],
        [[[0.2852, 0.0000],
          [0.0000, 0.2147]],
         [[0.2856, 0.0000],
          [0.0000, 0.2145]],
         [[0.2856, 0.0000],
          [0.0000, 0.2144]],
         [[0.2852, 0.0000],
          [0.0000, 0.2148]],
         [[0.3258, 0.0000],
          [0.0000, 0.1842]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2854, 0.0000],
          [0.0000, 0.2146]]],
        [[[0.2500, 0.0000],
          [0.0000, 0.2500]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2503, 0.0000],
          [0.0000, 0.2497]],
         [[0.2499, 0.0000],
          [0.0000, 0.2501]],
         [[0.2855, 0.0000],
          [0.0000, 0.2145]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]]],
        [[[0.2498, 0.0000],
          [0.0000, 0.2502]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2502, 0.0000],
          [0.0000, 0.2498]],
         [[0.2498, 0.0000],
          [0.0000, 0.2502]],
         [[0.2854, 0.0000],
          [0.0000, 0.2146]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2501, 0.0000],
          [0.0000, 0.2499]],
         [[0.2500, 0.0000],
          [0.0000, 0.2500]]]], device='cuda:0')
