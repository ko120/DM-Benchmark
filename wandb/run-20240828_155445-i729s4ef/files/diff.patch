diff --git a/configs/debug/default.yaml b/configs/debug/default.yaml
index 103180f..b8ed628 100644
--- a/configs/debug/default.yaml
+++ b/configs/debug/default.yaml
@@ -8,7 +8,7 @@ defaults:
 
 trainer:
   max_epochs: 1
-  accelerator: 'cpu' # debuggers don't like gpus
+  accelerator: 'gpu' # debuggers don't like gpus
   # detect_anomaly: true # raise exception if NaN or +/-inf is detected in any tensor
 
 datamodule:
diff --git a/configs/experiment/classification_mixed.yaml b/configs/experiment/classification_mixed.yaml
index c13342e..8f3e55b 100644
--- a/configs/experiment/classification_mixed.yaml
+++ b/configs/experiment/classification_mixed.yaml
@@ -22,15 +22,11 @@ seed: 12345
 
 trainer:
   min_epochs: 10
-  max_epochs: 200
+  max_epochs: 30
   gradient_clip_val: 0.5
 
 model:
   lr: 0.001
-  net:
-    lin1_size: 256
-    lin2_size: 256
-    lin3_size: 256
   criterion:
     _target_: src.metrics.train_metrics.ClassificationMixedLoss
     loss_scalers:
diff --git a/configs/model/classification.yaml b/configs/model/classification.yaml
index 3265244..9f45047 100644
--- a/configs/model/classification.yaml
+++ b/configs/model/classification.yaml
@@ -3,13 +3,12 @@ lr: 0.001
 weight_decay: 0.0005
 
 net:
-  _target_: src.models.components.models.SimpleDenseNet
+  _target_: src.models.components.models.MLP
   input_size: 1
-  lin1_size: 256
-  lin2_size: 256
-  lin3_size: 256
+  hwdith: 256
+  hdepth: 2
   output_size: 1
-  use_batchnorm: True
+
 
 criterion:
   _target_: torch.nn.CrossEntropyLoss
diff --git a/src/__pycache__/training_pipeline.cpython-310.pyc b/src/__pycache__/training_pipeline.cpython-310.pyc
index 07e3556..4317d72 100644
Binary files a/src/__pycache__/training_pipeline.cpython-310.pyc and b/src/__pycache__/training_pipeline.cpython-310.pyc differ
diff --git a/src/calibration/__pycache__/calibration.cpython-310.pyc b/src/calibration/__pycache__/calibration.cpython-310.pyc
index 1e91872..3821cb7 100644
Binary files a/src/calibration/__pycache__/calibration.cpython-310.pyc and b/src/calibration/__pycache__/calibration.cpython-310.pyc differ
diff --git a/src/datamodules/__pycache__/classification_datamodule.cpython-310.pyc b/src/datamodules/__pycache__/classification_datamodule.cpython-310.pyc
index 0b4cd4f..f6e9907 100644
Binary files a/src/datamodules/__pycache__/classification_datamodule.cpython-310.pyc and b/src/datamodules/__pycache__/classification_datamodule.cpython-310.pyc differ
diff --git a/src/datamodules/classification_datamodule.py b/src/datamodules/classification_datamodule.py
index 91a75b3..50aabbe 100644
--- a/src/datamodules/classification_datamodule.py
+++ b/src/datamodules/classification_datamodule.py
@@ -9,7 +9,7 @@ from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split, T
 from torchvision.datasets import MNIST
 from torchvision.transforms import transforms
 from functools import partial
-
+import pdb
 class ClassificationDataModule(LightningDataModule):
     """Datamodule for classification datasets.
 
@@ -78,7 +78,13 @@ class ClassificationDataModule(LightningDataModule):
         # load datasets only if they're not loaded already
         if not self.data_train and not self.data_val and not self.data_test:
             loader_fun = classification_load_funs[self.hparams.dataset_name]
-            X, y = loader_fun(self.hparams.data_dir)
+            if self.hparams.dataset_name == "adult_fair":
+                X, y, A = loader_fun(self.hparams.data_dir)
+                if A.ndim == 1:
+                    A = A.reshape(-1, 1)
+            else:
+                X, y = loader_fun(self.hparams.data_dir)
+
             if self.hparams.normalize:
                 std = X.std(axis=0)
                 zeros = np.isclose(std, 0.)
@@ -86,10 +92,17 @@ class ClassificationDataModule(LightningDataModule):
                 X[:, zeros] = 0.
             if y.ndim == 1:
                 y = y.reshape(-1, 1)
+
             # Split based on initialized ratio
-            dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y).long())
-            lengths = [int(len(X) * p) for p in self.hparams.train_val_test_split]
-            lengths[-1] += len(X) - sum(lengths)  # fix any rounding errors
+            if self.hparams.dataset_name == "adult_fair":
+                dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y).long(), torch.Tensor(A).long())
+                lengths = [int(len(X) * p) for p in self.hparams.train_val_test_split]
+                lengths[-1] += len(X) - sum(lengths)  # fix any rounding errors
+            else:
+                dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y).long())
+                lengths = [int(len(X) * p) for p in self.hparams.train_val_test_split]
+                lengths[-1] += len(X) - sum(lengths)  # fix any rounding errors
+
             self.data_train, self.data_val, self.data_test = random_split(
                 dataset=dataset,
                 lengths=lengths,
@@ -126,6 +139,43 @@ class ClassificationDataModule(LightningDataModule):
             drop_last=True
         )
 
+def _load_adult_fair(data_dir):
+    """
+    Attribute Information:
+    The dataset contains 16 columns
+    Target filed: Income
+    -- The income is divide into two classes: <=50K and >50K
+    Number of attributes: 14
+    -- These are the demographics and other features to describe a person
+    """
+    data_file = os.path.join(data_dir, 'classification/adult/adult.data')
+    colnames = ["age","workclass","fnlwgt","education","educational-num","marital-status","occupation","relationship","race","gender","capital-gain","capital-loss","hours-per-week","native-country","income"]
+    data = pd.read_csv(data_file, header=None, names=colnames, skipinitialspace=True)
+    data = data.replace("?", np.nan).dropna()
+    category_col =['workclass', 'education','marital-status', 'occupation',
+                  'relationship', 'race','native-country']
+    b, c = np.unique(data['income'], return_inverse=True)
+    d, e = np.unique(data['gender'], return_inverse=True)
+    data['income'] = c # turn into binary [0,1]
+    data['gender'] = e # turn into binary [0,1]
+
+
+    def encode_and_bind(original_dataframe, feature_to_encode):
+      dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])
+      res = pd.concat([original_dataframe, dummies], axis=1)
+      res = res.drop([feature_to_encode], axis=1)
+      return res
+
+    for feature in category_col:
+        data = encode_and_bind(data, feature)
+
+    y = data['income'].to_numpy()
+    A = data['gender'].to_numpy()
+    data = data.drop('income', axis=1)
+    data = data.drop('gender', axis=1)
+    X = data.to_numpy().astype(float)
+    return X, y, A
+
 def _load_adult(data_dir):
     """
     Attribute Information:
@@ -140,7 +190,7 @@ def _load_adult(data_dir):
     data = pd.read_csv(data_file, header=None, names=colnames, skipinitialspace=True)
     data = data.replace("?", np.nan).dropna()
     category_col =['workclass', 'education','marital-status', 'occupation',
-                  'relationship', 'race', 'gender', 'native-country']
+                  'relationship', 'race', 'gender','native-country']
     b, c = np.unique(data['income'], return_inverse=True)
     data['income'] = c # turn into binary [0,1]
 
@@ -156,15 +206,17 @@ def _load_adult(data_dir):
     y = data['income'].to_numpy()
     data = data.drop('income', axis=1)
     X = data.to_numpy().astype(float)
-    return X, y
 
+    return X, y
 
 classification_load_funs = {
-    "adult": _load_adult}
+    "adult": _load_adult,
+    "adult_fair":_load_adult_fair}
 
 classification_shapes = {
     "wdbc": (30, 2),
     "adult": (104, 2),
+    "adult_fair":(102,2), # feature decreased by 2 since we droped gender_Male and gender_Female
     "heart-disease": (23, 5),
     "online-shoppers": (28, 2),
     "dry-bean": (16, 7)
diff --git a/src/metrics/__pycache__/__init__.cpython-310.pyc b/src/metrics/__pycache__/__init__.cpython-310.pyc
index 63d638d..dc66c13 100644
Binary files a/src/metrics/__pycache__/__init__.cpython-310.pyc and b/src/metrics/__pycache__/__init__.cpython-310.pyc differ
diff --git a/src/metrics/__pycache__/evaluation_metrics.cpython-310.pyc b/src/metrics/__pycache__/evaluation_metrics.cpython-310.pyc
index deba34a..853138d 100644
Binary files a/src/metrics/__pycache__/evaluation_metrics.cpython-310.pyc and b/src/metrics/__pycache__/evaluation_metrics.cpython-310.pyc differ
diff --git a/src/metrics/__pycache__/train_metrics.cpython-310.pyc b/src/metrics/__pycache__/train_metrics.cpython-310.pyc
index 503a205..39a2596 100644
Binary files a/src/metrics/__pycache__/train_metrics.cpython-310.pyc and b/src/metrics/__pycache__/train_metrics.cpython-310.pyc differ
diff --git a/src/metrics/train_metrics.py b/src/metrics/train_metrics.py
index 1e02670..7cc8545 100644
--- a/src/metrics/train_metrics.py
+++ b/src/metrics/train_metrics.py
@@ -5,7 +5,7 @@ from typing import Optional, Dict, List
 from scipy.special import erf
 import numpy as np
 import time
-
+import pdb
 # Kernels Utils
 
 def rbf_kernel(u: torch.Tensor, v: torch.Tensor, bandwidth=1):
@@ -42,6 +42,7 @@ class ClassificationKernelLoss:
         MMD loss function for classification tasks.
         Allows for distribution matching by specifying operands and kernel functions.
         `scalers` and `bandwidths` are the parameters of the kernel functions.
+        It requires output dim=2 for binary case
     """
     def __init__(self,
                  operands: Dict[str, str] = {'x': "rbf", 'y': "rbf"},
@@ -63,39 +64,40 @@ class ClassificationKernelLoss:
     def __call__(self, x, y, logits, verbose=False):
         kernel_out = None
         loss_mats = [None for i in range(3)]
-
         for op in self.operands:
             scaler = self.scalers[op]
             bandwidth = self.bandwidths[op]
             if op == 'x':
                 # This is only true for tabular data. For example, multi-channel images will have 4D batches for x.
                 assert x.dim() == 2
+                # Computing k(x,x)
                 loss_mat = loss_mat2 = loss_mat3 = scaler * self.kernel_fun[op](x, x, bandwidth)
             elif op == 'y':
                 # Computes MMD loss for classification (See Section 4.1 of paper)
-                num_classes = logits.shape[-1]
+                # Computing Q
+                num_classes = logits.shape[-1] # we consider num_classes=2 for binary
                 y_all = torch.eye(num_classes).to(logits.device)
                 k_yy = self.kernel_fun[op](y_all, y_all, bandwidth)
                 q_y = F.softmax(logits, dim=-1)
                 q_yy = torch.einsum('ic,jd->ijcd', q_y, q_y)
                 total_yy = q_yy * k_yy.unsqueeze(0)
-
+                # Computing PQ
                 k_yj = k_yy[:,y].T
                 total_yj = torch.einsum('ic,jc->ijc', q_y, k_yj)
                 y_one_hot = F.one_hot(y, num_classes=num_classes).float()
-
+                # Computing P
                 loss_mat = scaler * total_yy.sum(dim=(2,3))
                 loss_mat2 = scaler * total_yj.sum(-1)
                 loss_mat3 = scaler * self.kernel_fun[op](y_one_hot, y_one_hot, bandwidth)
             else:
                 assert False, f"When running classification, operands must be x and y. Got operand {op} instead."
-
+            # Computing Expectations
             for i, value in enumerate([loss_mat, loss_mat2, loss_mat3]):
                 if loss_mats[i] is None:
                     loss_mats[i] = value
                 else:
                     loss_mats[i] =  loss_mats[i] * value
-
+        # MMD = E_Q[k(x,x)] -2E_P[E_Q[k(x,x)]] + E_P[k(x,x)], we are ignoring diagonal since it is kernel distance by itself
         kernel_out = mean_no_diag(loss_mats[0]) - 2 * mean_no_diag(loss_mats[1]) + mean_no_diag(loss_mats[2])
 
         return kernel_out
diff --git a/src/models/__pycache__/lightening_module.cpython-310.pyc b/src/models/__pycache__/lightening_module.cpython-310.pyc
index fd253ab..2736328 100644
Binary files a/src/models/__pycache__/lightening_module.cpython-310.pyc and b/src/models/__pycache__/lightening_module.cpython-310.pyc differ
diff --git a/src/models/components/__pycache__/models.cpython-310.pyc b/src/models/components/__pycache__/models.cpython-310.pyc
index 95e0492..d9b7338 100644
Binary files a/src/models/components/__pycache__/models.cpython-310.pyc and b/src/models/components/__pycache__/models.cpython-310.pyc differ
diff --git a/src/models/components/models.py b/src/models/components/models.py
index b980312..6754b94 100644
--- a/src/models/components/models.py
+++ b/src/models/components/models.py
@@ -1,4 +1,6 @@
 from torch import nn
+import torch.nn.functional as F
+import torch
 
 
 class SimpleDenseNet(nn.Module):
@@ -43,4 +45,82 @@ class SimpleDenseNet(nn.Module):
         self.output_size = output_size
 
     def forward(self, x):
-        return self.model(x)
\ No newline at end of file
+        return self.model(x)
+
+        
+
+
+class MLP(nn.Module):
+    """
+    MLP layer with declaring number of neurons as list ex. [input_size] + hdepth*[hwdith] +[output_size]
+    """
+    def __init__(self, input_size, hwdith, hdepth, output_size, activ="leakyrelu"):
+        """Initializes MLP unit"""
+        super(MLP, self).__init__()
+        self.input_size = input_size
+        self.output_size = output_size
+        self.layers = [input_size] + hdepth*[hwdith] +[output_size] # output size becomes 1 for binary
+        self.num_layers = len(self.layers) - 1
+        self.hiddens = nn.ModuleList(
+            [
+                nn.Linear(self.layers[i], self.layers[i + 1])
+                for i in range(self.num_layers)
+            ]
+        )
+        for hidden in self.hiddens:
+            torch.nn.init.xavier_uniform_(hidden.weight)
+        self.activ = activ
+
+    def forward(self, inputs):
+        """Computes forward pass through the model"""
+        L = inputs
+        for hidden in self.hiddens:
+            L = hidden(L)
+            if self.activ == "softplus":
+                L = F.softplus(L)
+            elif self.activ == "sigmoid":
+                L = F.sigmoid(L)
+            elif self.activ == "relu":
+                L = F.relu(L)
+            elif self.activ == "leakyrelu":
+                L = F.leaky_relu(L)
+            elif self.activ == "None":
+                pass
+            else:
+                raise Exception("bad activation function")
+        return L
+
+    # def freeze(self):
+    #     """Stops gradient computation through MLP parameters"""
+    #     for para in self.parameters():
+    #         para.requires_grad = False
+
+    # def activate(self):
+    #     """Activates gradient computation through MLP parameters"""
+    #     for para in self.parameters():
+    #         para.requires_grad = True
+
+
+class LaftrNet(nn.Module):
+    def __init__(self, input_size ,output_size ,zdim , edepth, ewidths, cdepth, cwidths, adepth, awidths, num_groups):
+        super(LaftrNet,self).__init__()
+        # for the adversary network, we make output class=1
+        # declare models
+        self.encoder = MLP(input_size = input_size, hdepth = edepth,hwdith = ewidths, output_size = zdim)
+        self.classifier = MLP(input_size = zdim, hdepth = cdepth, hwdith = cwidths, output_size = output_size)
+        self.discriminator = MLP(input_size = zdim, hdepth = adepth, hwdith = awidths, output_size = num_groups-1)
+        self.output_size = output_size
+
+
+    def forward(self,x):
+        return self.encoder(x)
+        
+    def predict_params(self):
+        """Returns encoder and classifier parameters"""
+        return list(self.classifier.parameters()) + list(self.encoder.parameters())
+
+    def audit_params(self):
+        """Returns discriminator parameters"""
+        return self.discriminator.parameters()
+
+        
diff --git a/src/models/lightening_module.py b/src/models/lightening_module.py
index 5ee7679..948f9dd 100644
--- a/src/models/lightening_module.py
+++ b/src/models/lightening_module.py
@@ -9,6 +9,7 @@ from torchmetrics.classification.calibration_error import CalibrationError
 import pdb
 from typing import Any, List, Literal, Optional, Dict, Callable
 from src.metrics import ShannonEntropyError, ClassificationKernelCalibrationError
+
 # Core NN Module
 class ClassificationLitModule(LightningModule):
     """ LightningModule for Classification tasks.
@@ -56,7 +57,7 @@ class ClassificationLitModule(LightningModule):
         self.train_acc = Accuracy(task= task)
         self.val_acc = Accuracy(task= task)
         self.test_acc = Accuracy(task= task)
-    
+
         # Initialize metrics
         assert net.output_size >= 2, f"Must have >=2 classes for classification task. Model only has {net.output_size} classes."
         ece_kwargs = {"task": 'multiclass', "n_bins": 20, "norm": 'l1', "num_classes": net.output_size} # We are always using Multiclass ECE since we are considering binary case as multiclass by 0 as class1 and 1 as class 2
@@ -191,8 +192,8 @@ class ClassificationLitModule(LightningModule):
             logits = self.forward(val_x)
             val_pred = F.softmax(logits, dim=-1)
 
-        with torch.enable_grad():
-            self.calibrator.train(val_pred, val_y)
+        torch.set_grad_enabled(True) # need to set it to True since Lightning by default, it doesn't require grad for val/test
+        self.calibrator.train(val_pred, val_y)
 
     def test_step(self, batch: Any, batch_idx: int):
         loss, preds, logits, targets= self.step(batch)
diff --git a/src/utils/__pycache__/__init__.cpython-310.pyc b/src/utils/__pycache__/__init__.cpython-310.pyc
index fbfeb67..50cd3e8 100644
Binary files a/src/utils/__pycache__/__init__.cpython-310.pyc and b/src/utils/__pycache__/__init__.cpython-310.pyc differ
diff --git a/train.py b/train.py
index 4087054..66342fc 100644
--- a/train.py
+++ b/train.py
@@ -30,13 +30,13 @@ def main(config: DictConfig):
    
     
     # Modifying the configuration
-    config.datamodule._target_ = 'src.datamodules.classification_datamodule.ClassificationDataModule'
-    config.datamodule.dataset_name = 'adult'
-    config.datamodule.data_dir = 'data'
-    config.model._target_ = 'src.models.lightening_module.ClassificationLitModule'
-    config.model.net._target_ = 'src.models.components.models.SimpleDenseNet'
-    config.model.criterion._target_ = 'src.metrics.train_metrics.ClassificationMixedLoss'
-    config.model.calibrator._target_ = 'src.calibration.calibration.TemperatureScaling'
+    # config.datamodule._target_ = 'src.datamodules.classification_datamodule.ClassificationDataModule'
+    # config.datamodule.dataset_name = 'adult'
+    # config.datamodule.data_dir = 'data'
+    # config.model._target_ = 'src.models.lightening_module.ClassificationLitModule'
+    # config.model.net._target_ = 'src.models.components.models.MLP'
+    # config.model.criterion._target_ = 'src.metrics.train_metrics.ClassificationMixedLoss'
+    # config.model.calibrator._target_ = 'src.calibration.calibration.TemperatureScaling'
     # config.logger.wandb.project = 'DM_Benchmark'
     # config.trainer.accelerator = 'gpu'
     # config.trainer.devices = '1'
