[[36m2024-08-12 15:39:06,852[39m][[34msrc.training_pipeline[39m][[32mINFO[39m] - Starting training!
[[36m2024-08-12 15:39:06,918[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[[36m2024-08-12 15:39:06,920[39m][[34mlightning_fabric.utilities.distributed[39m][[32mINFO[39m] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[[36m2024-08-12 15:39:11,236[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------
[[36m2024-08-12 15:39:11,666[39m][[34mpytorch_lightning.accelerators.cuda[39m][[32mINFO[39m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓
┃[1m    [22m┃[1m Name                    [22m┃[1m Type                                 [22m┃[1m Params [22m┃[1m Mode  [22m┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩
│ 0  │ net                     │ SimpleDenseNet                       │  160 K │ train │
│ 1  │ net.model               │ Sequential                           │  160 K │ train │
│ 2  │ net.model.0             │ Linear                               │ 26.9 K │ train │
│ 3  │ net.model.1             │ BatchNorm1d                          │    512 │ train │
│ 4  │ net.model.2             │ ReLU                                 │      0 │ train │
│ 5  │ net.model.3             │ Linear                               │ 65.8 K │ train │
│ 6  │ net.model.4             │ BatchNorm1d                          │    512 │ train │
│ 7  │ net.model.5             │ ReLU                                 │      0 │ train │
│ 8  │ net.model.6             │ Linear                               │ 65.8 K │ train │
│ 9  │ net.model.7             │ BatchNorm1d                          │    512 │ train │
│ 10 │ net.model.8             │ ReLU                                 │      0 │ train │
│ 11 │ net.model.9             │ Linear                               │    514 │ train │
│ 12 │ train_acc               │ BinaryAccuracy                       │      0 │ train │
│ 13 │ val_acc                 │ BinaryAccuracy                       │      0 │ train │
│ 14 │ test_acc                │ BinaryAccuracy                       │      0 │ train │
│ 15 │ train_ece               │ MulticlassCalibrationError           │      0 │ train │
│ 16 │ val_ece                 │ MulticlassCalibrationError           │      0 │ train │
│ 17 │ test_ece                │ MulticlassCalibrationError           │      0 │ train │
│ 18 │ train_entropy           │ ShannonEntropyError                  │      0 │ train │
│ 19 │ val_entropy             │ ShannonEntropyError                  │      0 │ train │
│ 20 │ test_entropy            │ ShannonEntropyError                  │      0 │ train │
│ 21 │ test_kcal               │ ClassificationKernelCalibrationError │      0 │ train │
│ 22 │ val_acc_best            │ MaxMetric                            │      0 │ train │
│ 23 │ val_ece_best            │ MinMetric                            │      0 │ train │
│ 24 │ val_entropy_best        │ MinMetric                            │      0 │ train │
│ 25 │ test_calibrated_acc     │ BinaryAccuracy                       │      0 │ train │
│ 26 │ test_calibrated_ece     │ MulticlassCalibrationError           │      0 │ train │
│ 27 │ test_calibrated_entropy │ ShannonEntropyError                  │      0 │ train │
│ 28 │ test_calibrated_kcal    │ ClassificationKernelCalibrationError │      0 │ train │
└────┴─────────────────────────┴──────────────────────────────────────┴────────┴───────┘
[1mTrainable params[22m: 160 K
[1mNon-trainable params[22m: 0
[1mTotal params[22m: 160 K
[1mTotal estimated model params size (MB)[22m: 0
[1mModules in train mode[22m: 29
[1mModules in eval mode[22m: 0
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider
increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/acc', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/ece', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/entropy', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider
increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
[37mEpoch 0/199[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m164/164[39m [38m0:00:03 • 0:00:00[39m [38m54.02it/s [39m [37mv_num: q0qc
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/acc', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/ece', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/entropy', ..., sync_dist=True)`
when logging on epoch level in distributed setting to accumulate the metric across devices.
Epoch 1/199 [38m━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m21/164[39m [38m0:00:00 • 0:00:04[39m [38m46.18it/s[39m [37mv_num: q0qc val/loss: 0.334 val/acc: 0.835 val/ece: 0.192 val/entropy: 0.323 val/acc_best: 0.828 val/ece_best: 0.022                 

                                                                                         [37mval/entropy_best: 0.332 train/loss: 0.367 train/acc: 0.825 train/ece: 0.107 train/entropy: 0.351                                    [39m[[36m2024-08-12 15:39:21,801[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] -
Epoch 1/199 [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺[39m [37m161/164[39m [38m0:00:03 • 0:00:01[39m [38m42.62it/s[39m [37mv_num: q0qc val/loss: 0.334 val/acc: 0.835 val/ece: 0.192 val/entropy: 0.323 val/acc_best: 0.828 val/ece_best: 0.022                
                                                                                         [37mval/entropy_best: 0.332 train/loss: 0.367 train/acc: 0.825 train/ece: 0.107 train/entropy: 0.351                                    
[?25h