[[36m2024-08-12 15:39:06,852[39m][[34msrc.training_pipeline[39m][[32mINFO[39m] - Starting training!
[[36m2024-08-12 15:39:06,918[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[[36m2024-08-12 15:39:06,920[39m][[34mlightning_fabric.utilities.distributed[39m][[32mINFO[39m] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[[36m2024-08-12 15:39:11,236[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------
[[36m2024-08-12 15:39:11,666[39m][[34mpytorch_lightning.accelerators.cuda[39m][[32mINFO[39m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ[1m    [22mâ”ƒ[1m Name                    [22mâ”ƒ[1m Type                                 [22mâ”ƒ[1m Params [22mâ”ƒ[1m Mode  [22mâ”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net                     â”‚ SimpleDenseNet                       â”‚  160 K â”‚ train â”‚
â”‚ 1  â”‚ net.model               â”‚ Sequential                           â”‚  160 K â”‚ train â”‚
â”‚ 2  â”‚ net.model.0             â”‚ Linear                               â”‚ 26.9 K â”‚ train â”‚
â”‚ 3  â”‚ net.model.1             â”‚ BatchNorm1d                          â”‚    512 â”‚ train â”‚
â”‚ 4  â”‚ net.model.2             â”‚ ReLU                                 â”‚      0 â”‚ train â”‚
â”‚ 5  â”‚ net.model.3             â”‚ Linear                               â”‚ 65.8 K â”‚ train â”‚
â”‚ 6  â”‚ net.model.4             â”‚ BatchNorm1d                          â”‚    512 â”‚ train â”‚
â”‚ 7  â”‚ net.model.5             â”‚ ReLU                                 â”‚      0 â”‚ train â”‚
â”‚ 8  â”‚ net.model.6             â”‚ Linear                               â”‚ 65.8 K â”‚ train â”‚
â”‚ 9  â”‚ net.model.7             â”‚ BatchNorm1d                          â”‚    512 â”‚ train â”‚
â”‚ 10 â”‚ net.model.8             â”‚ ReLU                                 â”‚      0 â”‚ train â”‚
â”‚ 11 â”‚ net.model.9             â”‚ Linear                               â”‚    514 â”‚ train â”‚
â”‚ 12 â”‚ train_acc               â”‚ BinaryAccuracy                       â”‚      0 â”‚ train â”‚
â”‚ 13 â”‚ val_acc                 â”‚ BinaryAccuracy                       â”‚      0 â”‚ train â”‚
â”‚ 14 â”‚ test_acc                â”‚ BinaryAccuracy                       â”‚      0 â”‚ train â”‚
â”‚ 15 â”‚ train_ece               â”‚ MulticlassCalibrationError           â”‚      0 â”‚ train â”‚
â”‚ 16 â”‚ val_ece                 â”‚ MulticlassCalibrationError           â”‚      0 â”‚ train â”‚
â”‚ 17 â”‚ test_ece                â”‚ MulticlassCalibrationError           â”‚      0 â”‚ train â”‚
â”‚ 18 â”‚ train_entropy           â”‚ ShannonEntropyError                  â”‚      0 â”‚ train â”‚
â”‚ 19 â”‚ val_entropy             â”‚ ShannonEntropyError                  â”‚      0 â”‚ train â”‚
â”‚ 20 â”‚ test_entropy            â”‚ ShannonEntropyError                  â”‚      0 â”‚ train â”‚
â”‚ 21 â”‚ test_kcal               â”‚ ClassificationKernelCalibrationError â”‚      0 â”‚ train â”‚
â”‚ 22 â”‚ val_acc_best            â”‚ MaxMetric                            â”‚      0 â”‚ train â”‚
â”‚ 23 â”‚ val_ece_best            â”‚ MinMetric                            â”‚      0 â”‚ train â”‚
â”‚ 24 â”‚ val_entropy_best        â”‚ MinMetric                            â”‚      0 â”‚ train â”‚
â”‚ 25 â”‚ test_calibrated_acc     â”‚ BinaryAccuracy                       â”‚      0 â”‚ train â”‚
â”‚ 26 â”‚ test_calibrated_ece     â”‚ MulticlassCalibrationError           â”‚      0 â”‚ train â”‚
â”‚ 27 â”‚ test_calibrated_entropy â”‚ ShannonEntropyError                  â”‚      0 â”‚ train â”‚
â”‚ 28 â”‚ test_calibrated_kcal    â”‚ ClassificationKernelCalibrationError â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 160 K
[1mNon-trainable params[22m: 0
[1mTotal params[22m: 160 K
[1mTotal estimated model params size (MB)[22m: 0
[1mModules in train mode[22m: 29
[1mModules in eval mode[22m: 0
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider
increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/acc', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/ece', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/entropy', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider
increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
[37mEpoch 0/199[39m [38mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m164/164[39m [38m0:00:03 â€¢ 0:00:00[39m [38m54.02it/s [39m [37mv_num: q0qc
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/acc', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/ece', ..., sync_dist=True)` when
logging on epoch level in distributed setting to accumulate the metric across devices.
/local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/entropy', ..., sync_dist=True)`
when logging on epoch level in distributed setting to accumulate the metric across devices.
Epoch 1/199 [38mâ”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m21/164[39m [38m0:00:00 â€¢ 0:00:04[39m [38m46.18it/s[39m [37mv_num: q0qc val/loss: 0.334 val/acc: 0.835 val/ece: 0.192 val/entropy: 0.323 val/acc_best: 0.828 val/ece_best: 0.022                 

                                                                                         [37mval/entropy_best: 0.332 train/loss: 0.367 train/acc: 0.825 train/ece: 0.107 train/entropy: 0.351                                    [39m[[36m2024-08-12 15:39:21,801[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] -
Epoch 1/199 [38mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•º[39m [37m161/164[39m [38m0:00:03 â€¢ 0:00:01[39m [38m42.62it/s[39m [37mv_num: q0qc val/loss: 0.334 val/acc: 0.835 val/ece: 0.192 val/entropy: 0.323 val/acc_best: 0.828 val/ece_best: 0.022                
                                                                                         [37mval/entropy_best: 0.332 train/loss: 0.367 train/acc: 0.825 train/ece: 0.107 train/entropy: 0.351                                    
[?25h