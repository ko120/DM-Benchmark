diff --git a/.env.example b/.env.example
deleted file mode 100644
index a790e32..0000000
--- a/.env.example
+++ /dev/null
@@ -1,6 +0,0 @@
-# example of file for storing private and user specific environment variables, like keys or system paths
-# rename it to ".env" (excluded from version control by default)
-# .env is loaded by train.py automatically
-# hydra allows you to reference variables in .yaml configs with special syntax: ${oc.env:MY_VAR}
-
-MY_VAR="/home/user/my/system/path"
diff --git a/.github/PULL_REQUEST_TEMPLATE.md b/.github/PULL_REQUEST_TEMPLATE.md
deleted file mode 100644
index 410bcd8..0000000
--- a/.github/PULL_REQUEST_TEMPLATE.md
+++ /dev/null
@@ -1,22 +0,0 @@
-## What does this PR do?
-
-<!--
-Please include a summary of the change and which issue is fixed.
-Please also include relevant motivation and context.
-List any dependencies that are required for this change.
-List all the breaking changes introduced by this pull request.
--->
-
-Fixes #\<issue_number>
-
-## Before submitting
-
-- [ ] Did you make sure **title is self-explanatory** and **the description concisely explains the PR**?
-- [ ] Did you make sure your **PR does only one thing**, instead of bundling different changes together?
-- [ ] Did you list all the **breaking changes** introduced by this pull request?
-- [ ] Did you **test your PR locally** with `pytest` command?
-- [ ] Did you **run pre-commit hooks** with `pre-commit run -a` command?
-
-## Did you have fun?
-
-Make sure you had fun coding üôÉ
diff --git a/.github/codecov.yml b/.github/codecov.yml
deleted file mode 100644
index c66853c..0000000
--- a/.github/codecov.yml
+++ /dev/null
@@ -1,15 +0,0 @@
-coverage:
-  status:
-    # measures overall project coverage
-    project:
-      default:
-        threshold: 100% # how much decrease in coverage is needed to not consider success
-
-    # measures PR or single commit coverage
-    patch:
-      default:
-        threshold: 100% # how much decrease in coverage is needed to not consider success
-
-
-    # project: off
-    # patch: off
diff --git a/.github/dependabot.yml b/.github/dependabot.yml
deleted file mode 100644
index 5a861fd..0000000
--- a/.github/dependabot.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-# To get started with Dependabot version updates, you'll need to specify which
-# package ecosystems to update and where the package manifests are located.
-# Please see the documentation for all configuration options:
-# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates
-
-version: 2
-updates:
-  - package-ecosystem: "pip" # See documentation for possible values
-    directory: "/" # Location of package manifests
-    schedule:
-      interval: "daily"
-    ignore:
-      - dependency-name: "pytorch-lightning"
-        update-types: ["version-update:semver-patch"]
-      - dependency-name: "torchmetrics"
-        update-types: ["version-update:semver-patch"]
diff --git a/.github/release-drafter.yml b/.github/release-drafter.yml
deleted file mode 100644
index 59af159..0000000
--- a/.github/release-drafter.yml
+++ /dev/null
@@ -1,44 +0,0 @@
-name-template: "v$RESOLVED_VERSION"
-tag-template: "v$RESOLVED_VERSION"
-
-categories:
-  - title: "üöÄ Features"
-    labels:
-      - "feature"
-      - "enhancement"
-  - title: "üêõ Bug Fixes"
-    labels:
-      - "fix"
-      - "bugfix"
-      - "bug"
-  - title: "üßπ Maintenance"
-    labels:
-      - "maintenance"
-      - "dependencies"
-      - "refactoring"
-      - "cosmetic"
-      - "chore"
-  - title: "üìùÔ∏è Documentation"
-    labels:
-      - "documentation"
-      - "docs"
-
-change-template: "- $TITLE @$AUTHOR (#$NUMBER)"
-change-title-escapes: '\<*_&' # You can add # and @ to disable mentions
-
-version-resolver:
-  major:
-    labels:
-      - "major"
-  minor:
-    labels:
-      - "minor"
-  patch:
-    labels:
-      - "patch"
-  default: patch
-
-template: |
-  ## Changes
-
-  $CHANGES
diff --git a/.github/workflows/code-quality-main.yaml b/.github/workflows/code-quality-main.yaml
deleted file mode 100644
index 88b7220..0000000
--- a/.github/workflows/code-quality-main.yaml
+++ /dev/null
@@ -1,22 +0,0 @@
-# Same as `code-quality-pr.yaml` but triggered on commit to main branch
-# and runs on all files (instead of only the changed ones)
-
-name: Code Quality Main
-
-on:
-  push:
-    branches: [main]
-
-jobs:
-  code-quality:
-    runs-on: ubuntu-latest
-
-    steps:
-      - name: Checkout
-        uses: actions/checkout@v2
-
-      - name: Set up Python
-        uses: actions/setup-python@v2
-
-      - name: Run pre-commits
-        uses: pre-commit/action@v2.0.3
diff --git a/.github/workflows/code-quality-pr.yaml b/.github/workflows/code-quality-pr.yaml
deleted file mode 100644
index e58df42..0000000
--- a/.github/workflows/code-quality-pr.yaml
+++ /dev/null
@@ -1,36 +0,0 @@
-# This workflow finds which files were changed, prints them,
-# and runs `pre-commit` on those files.
-
-# Inspired by the sktime library:
-# https://github.com/alan-turing-institute/sktime/blob/main/.github/workflows/test.yml
-
-name: Code Quality PR
-
-on:
-  pull_request:
-    branches: [main, "release/*", "dev"]
-
-jobs:
-  code-quality:
-    runs-on: ubuntu-latest
-
-    steps:
-      - name: Checkout
-        uses: actions/checkout@v2
-
-      - name: Set up Python
-        uses: actions/setup-python@v2
-
-      - name: Find modified files
-        id: file_changes
-        uses: trilom/file-changes-action@v1.2.4
-        with:
-          output: " "
-
-      - name: List modified files
-        run: echo '${{ steps.file_changes.outputs.files}}'
-
-      - name: Run pre-commits
-        uses: pre-commit/action@v2.0.3
-        with:
-          extra_args: --files ${{ steps.file_changes.outputs.files}}
diff --git a/.github/workflows/release-drafter.yml b/.github/workflows/release-drafter.yml
deleted file mode 100644
index 6a45e15..0000000
--- a/.github/workflows/release-drafter.yml
+++ /dev/null
@@ -1,27 +0,0 @@
-name: Release Drafter
-
-on:
-  push:
-    # branches to consider in the event; optional, defaults to all
-    branches:
-      - main
-
-permissions:
-  contents: read
-
-jobs:
-  update_release_draft:
-    permissions:
-      # write permission is required to create a github release
-      contents: write
-      # write permission is required for autolabeler
-      # otherwise, read permission is required at least
-      pull-requests: write
-
-    runs-on: ubuntu-latest
-
-    steps:
-      # Drafts your next Release notes as Pull Requests are merged into "master"
-      - uses: release-drafter/release-drafter@v5
-        env:
-          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
diff --git a/.github/workflows/test.yml b/.github/workflows/test.yml
deleted file mode 100644
index e205ee5..0000000
--- a/.github/workflows/test.yml
+++ /dev/null
@@ -1,139 +0,0 @@
-name: Tests
-
-on:
-  push:
-    branches: [main]
-  pull_request:
-    branches: [main, "release/*", "dev"]
-
-jobs:
-  run_tests_ubuntu:
-    runs-on: ${{ matrix.os }}
-
-    strategy:
-      fail-fast: false
-      matrix:
-        os: ["ubuntu-latest"]
-        python-version: ["3.8", "3.9", "3.10"]
-
-    timeout-minutes: 20
-
-    steps:
-      - name: Checkout
-        uses: actions/checkout@v3
-
-      - name: Set up Python ${{ matrix.python-version }}
-        uses: actions/setup-python@v3
-        with:
-          python-version: ${{ matrix.python-version }}
-
-      - name: Install dependencies
-        run: |
-          python -m pip install --upgrade pip
-          pip install -r requirements.txt
-          pip install pytest
-          pip install sh
-
-      - name: List dependencies
-        run: |
-          python -m pip list
-
-      - name: Run pytest
-        run: |
-          pytest -v
-
-  run_tests_macos:
-    runs-on: ${{ matrix.os }}
-
-    strategy:
-      fail-fast: false
-      matrix:
-        os: ["macos-latest"]
-        python-version: ["3.8", "3.9", "3.10"]
-
-    timeout-minutes: 20
-
-    steps:
-      - name: Checkout
-        uses: actions/checkout@v3
-
-      - name: Set up Python ${{ matrix.python-version }}
-        uses: actions/setup-python@v3
-        with:
-          python-version: ${{ matrix.python-version }}
-
-      - name: Install dependencies
-        run: |
-          python -m pip install --upgrade pip
-          pip install -r requirements.txt
-          pip install pytest
-          pip install sh
-
-      - name: List dependencies
-        run: |
-          python -m pip list
-
-      - name: Run pytest
-        run: |
-          pytest -v
-
-  run_tests_windows:
-    runs-on: ${{ matrix.os }}
-
-    strategy:
-      fail-fast: false
-      matrix:
-        os: ["windows-latest"]
-        python-version: ["3.8", "3.9", "3.10"]
-
-    timeout-minutes: 20
-
-    steps:
-      - name: Checkout
-        uses: actions/checkout@v3
-
-      - name: Set up Python ${{ matrix.python-version }}
-        uses: actions/setup-python@v3
-        with:
-          python-version: ${{ matrix.python-version }}
-
-      - name: Install dependencies
-        run: |
-          python -m pip install --upgrade pip
-          pip install -r requirements.txt
-          pip install pytest
-
-      - name: List dependencies
-        run: |
-          python -m pip list
-
-      - name: Run pytest
-        run: |
-          pytest -v
-
-  # upload code coverage report
-  code-coverage:
-    runs-on: ubuntu-latest
-
-    steps:
-      - name: Checkout
-        uses: actions/checkout@v2
-
-      - name: Set up Python 3.10
-        uses: actions/setup-python@v2
-        with:
-          python-version: "3.10"
-
-      - name: Install dependencies
-        run: |
-          python -m pip install --upgrade pip
-          pip install -r requirements.txt
-          pip install pytest
-          pip install pytest-cov[toml]
-          pip install sh
-
-      - name: Run tests and collect coverage
-        run: pytest --cov src # NEEDS TO BE UPDATED WHEN CHANGING THE NAME OF "src" FOLDER
-
-      - name: Upload coverage to Codecov
-        uses: codecov/codecov-action@v3
diff --git a/.gitignore b/.gitignore
deleted file mode 100644
index 04a0648..0000000
--- a/.gitignore
+++ /dev/null
@@ -1,154 +0,0 @@
-# Byte-compiled / optimized / DLL files
-__pycache__/
-*.py[cod]
-*$py.class
-
-# C extensions
-*.so
-
-# Distribution / packaging
-.Python
-build/
-develop-eggs/
-dist/
-downloads/
-eggs/
-.eggs/
-lib/
-lib64/
-parts/
-sdist/
-var/
-wheels/
-pip-wheel-metadata/
-share/python-wheels/
-*.egg-info/
-.installed.cfg
-*.egg
-MANIFEST
-
-# PyInstaller
-#  Usually these files are written by a python script from a template
-#  before PyInstaller builds the exe, so as to inject date/other infos into it.
-*.manifest
-*.spec
-
-# Installer logs
-pip-log.txt
-pip-delete-this-directory.txt
-
-# Unit test / coverage reports
-htmlcov/
-.tox/
-.nox/
-.coverage
-.coverage.*
-.cache
-nosetests.xml
-coverage.xml
-*.cover
-*.py,cover
-.hypothesis/
-.pytest_cache/
-
-# Translations
-*.mo
-*.pot
-
-# Django stuff:
-*.log
-local_settings.py
-db.sqlite3
-db.sqlite3-journal
-
-# Flask stuff:
-instance/
-.webassets-cache
-
-# Scrapy stuff:
-.scrapy
-
-# Sphinx documentation
-docs/_build/
-
-# PyBuilder
-target/
-
-# Jupyter Notebook
-.ipynb_checkpoints
-
-# IPython
-profile_default/
-ipython_config.py
-
-# pyenv
-.python-version
-
-# pipenv
-#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
-#   However, in case of collaboration, if having platform-specific dependencies or dependencies
-#   having no cross-platform support, pipenv may install dependencies that don't work, or not
-#   install all needed dependencies.
-#Pipfile.lock
-
-# PEP 582; used by e.g. github.com/David-OConnor/pyflow
-__pypackages__/
-
-# Celery stuff
-celerybeat-schedule
-celerybeat.pid
-
-# SageMath parsed files
-*.sage.py
-
-# Environments
-.venv
-env/
-venv/
-ENV/
-env.bak/
-venv.bak/
-
-# Spyder project settings
-.spyderproject
-.spyproject
-
-# Rope project settings
-.ropeproject
-
-# mkdocs documentation
-/site
-
-# mypy
-.mypy_cache/
-.dmypy.json
-dmypy.json
-
-# Pyre type checker
-.pyre/
-
-### VisualStudioCode
-.vscode/*
-!.vscode/settings.json
-!.vscode/tasks.json
-!.vscode/launch.json
-!.vscode/extensions.json
-*.code-workspace
-**/.vscode
-
-# JetBrains
-.idea/
-
-# Data & Models
-*.h5
-*.tar
-*.tar.gz
-
-# Lightning-Hydra-Template
-configs/local/default.yaml
-/data/
-/logs/
-.env
-
-# Aim logging
-.aim
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
deleted file mode 100644
index ee45ce1..0000000
--- a/.pre-commit-config.yaml
+++ /dev/null
@@ -1,147 +0,0 @@
-default_language_version:
-  python: python3
-
-repos:
-  - repo: https://github.com/pre-commit/pre-commit-hooks
-    rev: v4.4.0
-    hooks:
-      # list of supported hooks: https://pre-commit.com/hooks.html
-      - id: trailing-whitespace
-      - id: end-of-file-fixer
-      - id: check-docstring-first
-      - id: check-yaml
-      - id: debug-statements
-      - id: detect-private-key
-      - id: check-executables-have-shebangs
-      - id: check-toml
-      - id: check-case-conflict
-      - id: check-added-large-files
-
-  # python code formatting
-  - repo: https://github.com/psf/black
-    rev: 23.1.0
-    hooks:
-      - id: black
-        args: [--line-length, "99"]
-
-  # python import sorting
-  - repo: https://github.com/PyCQA/isort
-    rev: 5.12.0
-    hooks:
-      - id: isort
-        args: ["--profile", "black", "--filter-files"]
-
-  # python upgrading syntax to newer version
-  - repo: https://github.com/asottile/pyupgrade
-    rev: v3.3.1
-    hooks:
-      - id: pyupgrade
-        args: [--py38-plus]
-
-  # python docstring formatting
-  - repo: https://github.com/myint/docformatter
-    rev: v1.7.4
-    hooks:
-      - id: docformatter
-        args:
-          [
-            --in-place,
-            --wrap-summaries=99,
-            --wrap-descriptions=99,
-            --style=sphinx,
-            --black,
-          ]
-
-  # python docstring coverage checking
-  - repo: https://github.com/econchick/interrogate
-    rev: 1.5.0 # or master if you're bold
-    hooks:
-      - id: interrogate
-        args:
-          [
-            --verbose,
-            --fail-under=80,
-            --ignore-init-module,
-            --ignore-init-method,
-            --ignore-module,
-            --ignore-nested-functions,
-            -vv,
-          ]
-
-  # python check (PEP8), programming errors and code complexity
-  - repo: https://github.com/PyCQA/flake8
-    rev: 6.0.0
-    hooks:
-      - id: flake8
-        args:
-          [
-            "--extend-ignore",
-            "E203,E402,E501,F401,F841,RST2,RST301",
-            "--exclude",
-            "logs/*,data/*",
-          ]
-        additional_dependencies: [flake8-rst-docstrings==0.3.0]
-
-  # python security linter
-  - repo: https://github.com/PyCQA/bandit
-    rev: "1.7.5"
-    hooks:
-      - id: bandit
-        args: ["-s", "B101"]
-
-  # yaml formatting
-  - repo: https://github.com/pre-commit/mirrors-prettier
-    rev: v3.0.0-alpha.6
-    hooks:
-      - id: prettier
-        types: [yaml]
-        exclude: "environment.yaml"
-
-  # shell scripts linter
-  - repo: https://github.com/shellcheck-py/shellcheck-py
-    rev: v0.9.0.2
-    hooks:
-      - id: shellcheck
-
-  # md formatting
-  - repo: https://github.com/executablebooks/mdformat
-    rev: 0.7.16
-    hooks:
-      - id: mdformat
-        args: ["--number"]
-        additional_dependencies:
-          - mdformat-gfm
-          - mdformat-tables
-          - mdformat_frontmatter
-          # - mdformat-toc
-          # - mdformat-black
-
-  # word spelling linter
-  - repo: https://github.com/codespell-project/codespell
-    rev: v2.2.4
-    hooks:
-      - id: codespell
-        args:
-          - --skip=logs/**,data/**,*.ipynb
-          # - --ignore-words-list=abc,def
-
-  # jupyter notebook cell output clearing
-  - repo: https://github.com/kynan/nbstripout
-    rev: 0.6.1
-    hooks:
-      - id: nbstripout
-
-  # jupyter notebook linting
-  - repo: https://github.com/nbQA-dev/nbQA
-    rev: 1.6.3
-    hooks:
-      - id: nbqa-black
-        args: ["--line-length=99"]
-      - id: nbqa-isort
-        args: ["--profile=black"]
-      - id: nbqa-flake8
-        args:
-          [
-            "--extend-ignore=E203,E402,E501,F401,F841",
-            "--exclude=logs/*,data/*",
-          ]
diff --git a/.project-root b/.project-root
deleted file mode 100644
index 63eab77..0000000
--- a/.project-root
+++ /dev/null
@@ -1,2 +0,0 @@
-# this file is required for inferring the project root directory
-# do not delete
diff --git a/Makefile b/Makefile
deleted file mode 100644
index 38184df..0000000
--- a/Makefile
+++ /dev/null
@@ -1,30 +0,0 @@
-
-help:  ## Show help
-	@grep -E '^[.a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-30s\033[0m %s\n", $$1, $$2}'
-
-clean: ## Clean autogenerated files
-	rm -rf dist
-	find . -type f -name "*.DS_Store" -ls -delete
-	find . | grep -E "(__pycache__|\.pyc|\.pyo)" | xargs rm -rf
-	find . | grep -E ".pytest_cache" | xargs rm -rf
-	find . | grep -E ".ipynb_checkpoints" | xargs rm -rf
-	rm -f .coverage
-
-clean-logs: ## Clean logs
-	rm -rf logs/**
-
-format: ## Run pre-commit hooks
-	pre-commit run -a
-
-sync: ## Merge changes from main branch to your current branch
-	git pull
-	git pull origin main
-
-test: ## Run not slow tests
-	pytest -k "not slow"
-
-test-full: ## Run all tests
-	pytest
-
-train: ## Train the model
-	python src/train.py
diff --git a/README.md b/README.md
deleted file mode 100644
index 4c847e5..0000000
--- a/README.md
+++ /dev/null
@@ -1,75 +0,0 @@
-
-<div align="center">
-
-# Distribution Matching Benchmark
-
-<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
-<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
-<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
-<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a><br>
-[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
-[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/paper/2020)
-
-</div>
-
-## Description
-
-What it does
-
-## Installation
-
-#### Pip
-
-```bash
-# clone project
-git clone https://github.com/YourGithubName/your-repo-name
-cd your-repo-name
-
-# [OPTIONAL] create conda environment
-conda create -n myenv python=3.9
-conda activate myenv
-
-# install pytorch according to instructions
-# https://pytorch.org/get-started/
-
-# install requirements
-pip install -r requirements.txt
-```
-
-#### Conda
-
-```bash
-# clone project
-git clone https://github.com/YourGithubName/your-repo-name
-cd your-repo-name
-
-# create conda environment and install dependencies
-conda env create -f environment.yaml -n myenv
-
-# activate conda environment
-conda activate myenv
-```
-
-## How to run
-
-Train model with default configuration
-
-```bash
-# train on CPU
-python src/train.py trainer=cpu
-
-# train on GPU
-python src/train.py trainer=gpu
-```
-
-Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/)
-
-```bash
-python src/train.py experiment=experiment_name.yaml
-```
-
-You can override any parameter from command line like this
-
-```bash
-python src/train.py trainer.max_epochs=20 data.batch_size=64
-```
diff --git a/configs/__init__.py b/configs/__init__.py
deleted file mode 100644
index 56bf7f4..0000000
--- a/configs/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-# this file is needed here to include configs when building project as a package
diff --git a/configs/callbacks/default.yaml b/configs/callbacks/default.yaml
index c9bf2fb..2bf3ce4 100644
--- a/configs/callbacks/default.yaml
+++ b/configs/callbacks/default.yaml
@@ -1,22 +1,24 @@
-defaults:
-  - model_checkpoint
-  - early_stopping
-  - model_summary
-  - rich_progress_bar
-  - _self_
-
 model_checkpoint:
-  dirpath: ${paths.output_dir}/checkpoints
+  _target_: pytorch_lightning.callbacks.ModelCheckpoint
+  monitor: "val/loss" # name of the logged metric which determines when model is improving
+  mode: "min" # "max" means higher metric value is better, can be also "min"
+  save_top_k: 1 # save k best models (determined by above metric)
+  save_last: True # additionaly always save model from last epoch
+  verbose: False
+  dirpath: "checkpoints/"
   filename: "epoch_{epoch:03d}"
-  monitor: "val/acc"
-  mode: "max"
-  save_last: True
   auto_insert_metric_name: False
 
 early_stopping:
-  monitor: "val/acc"
-  patience: 100
-  mode: "max"
+  _target_: pytorch_lightning.callbacks.EarlyStopping
+  monitor: "val/loss" # name of the logged metric which determines when model is improving
+  mode: "min" # "max" means higher metric value is better, can be also "min"
+  patience: 30 # how many validation epochs of not improving until training stops
+  min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement
 
 model_summary:
+  _target_: pytorch_lightning.callbacks.RichModelSummary
   max_depth: -1
+
+rich_progress_bar:
+  _target_: pytorch_lightning.callbacks.RichProgressBar
diff --git a/configs/callbacks/early_stopping.yaml b/configs/callbacks/early_stopping.yaml
deleted file mode 100644
index c826c8d..0000000
--- a/configs/callbacks/early_stopping.yaml
+++ /dev/null
@@ -1,15 +0,0 @@
-# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html
-
-early_stopping:
-  _target_: lightning.pytorch.callbacks.EarlyStopping
-  monitor: ??? # quantity to be monitored, must be specified !!!
-  min_delta: 0. # minimum change in the monitored quantity to qualify as an improvement
-  patience: 3 # number of checks with no improvement after which training will be stopped
-  verbose: False # verbosity mode
-  mode: "min" # "max" means higher metric value is better, can be also "min"
-  strict: True # whether to crash the training if monitor is not found in the validation metrics
-  check_finite: True # when set True, stops training when the monitor becomes NaN or infinite
-  stopping_threshold: null # stop training immediately once the monitored quantity reaches this threshold
-  divergence_threshold: null # stop training as soon as the monitored quantity becomes worse than this threshold
-  check_on_train_epoch_end: null # whether to run early stopping at the end of the training epoch
-  # log_rank_zero_only: False  # this keyword argument isn't available in stable version
diff --git a/configs/callbacks/model_checkpoint.yaml b/configs/callbacks/model_checkpoint.yaml
deleted file mode 100644
index bf946e8..0000000
--- a/configs/callbacks/model_checkpoint.yaml
+++ /dev/null
@@ -1,17 +0,0 @@
-# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html
-
-model_checkpoint:
-  _target_: lightning.pytorch.callbacks.ModelCheckpoint
-  dirpath: null # directory to save the model file
-  filename: null # checkpoint filename
-  monitor: null # name of the logged metric which determines when model is improving
-  verbose: False # verbosity mode
-  save_last: null # additionally always save an exact copy of the last checkpoint to a file last.ckpt
-  save_top_k: 1 # save k best models (determined by above metric)
-  mode: "min" # "max" means higher metric value is better, can be also "min"
-  auto_insert_metric_name: True # when True, the checkpoints filenames will contain the metric name
-  save_weights_only: False # if True, then only the model‚Äôs weights will be saved
-  every_n_train_steps: null # number of training steps between checkpoints
-  train_time_interval: null # checkpoints are monitored at the specified time interval
-  every_n_epochs: null # number of epochs between checkpoints
-  save_on_train_epoch_end: null # whether to run checkpointing at the end of the training epoch or the end of validation
diff --git a/configs/callbacks/model_summary.yaml b/configs/callbacks/model_summary.yaml
deleted file mode 100644
index b75981d..0000000
--- a/configs/callbacks/model_summary.yaml
+++ /dev/null
@@ -1,5 +0,0 @@
-# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.RichModelSummary.html
-
-model_summary:
-  _target_: lightning.pytorch.callbacks.RichModelSummary
-  max_depth: 1 # the maximum depth of layer nesting that the summary will include
diff --git a/configs/callbacks/rich_progress_bar.yaml b/configs/callbacks/rich_progress_bar.yaml
deleted file mode 100644
index de6f1cc..0000000
--- a/configs/callbacks/rich_progress_bar.yaml
+++ /dev/null
@@ -1,4 +0,0 @@
-# https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.RichProgressBar.html
-
-rich_progress_bar:
-  _target_: lightning.pytorch.callbacks.RichProgressBar
diff --git a/configs/data/mnist.yaml b/configs/data/mnist.yaml
deleted file mode 100644
index 51bfaff..0000000
--- a/configs/data/mnist.yaml
+++ /dev/null
@@ -1,6 +0,0 @@
-_target_: src.data.mnist_datamodule.MNISTDataModule
-data_dir: ${paths.data_dir}
-batch_size: 128 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
-train_val_test_split: [55_000, 5_000, 10_000]
-num_workers: 0
-pin_memory: False
diff --git a/configs/debug/default.yaml b/configs/debug/default.yaml
index 1886902..8dfb104 100644
--- a/configs/debug/default.yaml
+++ b/configs/debug/default.yaml
@@ -3,33 +3,26 @@
 # default debugging setup, runs 1 full epoch
 # other debugging configs can inherit from this one
 
-# overwrite task name so debugging logs are stored in separate folder
-task_name: "debug"
+defaults:
+  - override /log_dir: debug.yaml
 
-# disable callbacks and loggers during debugging
-callbacks: null
-logger: null
+trainer:
+  max_epochs: 1
+  gpus: 0 # debuggers don't like gpus
+  detect_anomaly: true # raise exception if NaN or +/-inf is detected in any tensor
+  track_grad_norm: 2 # track gradient norm with loggers
 
-extras:
-  ignore_warnings: False
-  enforce_tags: False
+datamodule:
+  num_workers: 0 # debuggers don't like multiprocessing
+  pin_memory: False # disable gpu memory pin
 
 # sets level of all command line loggers to 'DEBUG'
 # https://hydra.cc/docs/tutorials/basic/running_your_app/logging/
 hydra:
-  job_logging:
-    root:
-      level: DEBUG
+  verbose: True
 
-  # use this to also set hydra loggers to 'DEBUG'
-  # verbose: True
+  # use this to set level of only chosen command line loggers to 'DEBUG':
+  # verbose: [src.train, src.utils]
 
-trainer:
-  max_epochs: 1
-  accelerator: cpu # debuggers don't like gpus
-  devices: 1 # debuggers don't like multiprocessing
-  detect_anomaly: true # raise exception if NaN or +/-inf is detected in any tensor
-
-data:
-  num_workers: 0 # debuggers don't like multiprocessing
-  pin_memory: False # disable gpu memory pin
+# config is already printed by hydra when `hydra/verbose: True`
+print_config: False
diff --git a/configs/debug/fdr.yaml b/configs/debug/fdr.yaml
deleted file mode 100644
index 7f2d34f..0000000
--- a/configs/debug/fdr.yaml
+++ /dev/null
@@ -1,9 +0,0 @@
-# @package _global_
-
-# runs 1 train, 1 validation and 1 test step
-
-defaults:
-  - default
-
-trainer:
-  fast_dev_run: true
diff --git a/configs/debug/limit.yaml b/configs/debug/limit.yaml
deleted file mode 100644
index 514d77f..0000000
--- a/configs/debug/limit.yaml
+++ /dev/null
@@ -1,12 +0,0 @@
-# @package _global_
-
-# uses only 1% of the training data and 5% of validation/test data
-
-defaults:
-  - default
-
-trainer:
-  max_epochs: 3
-  limit_train_batches: 0.01
-  limit_val_batches: 0.05
-  limit_test_batches: 0.05
diff --git a/configs/debug/overfit.yaml b/configs/debug/overfit.yaml
index 9906586..2ce654b 100644
--- a/configs/debug/overfit.yaml
+++ b/configs/debug/overfit.yaml
@@ -3,11 +3,8 @@
 # overfits to 3 batches
 
 defaults:
-  - default
+  - default.yaml
 
 trainer:
   max_epochs: 20
   overfit_batches: 3
-
-# model ckpt and early stopping need to be disabled during overfitting
-callbacks: null
diff --git a/configs/debug/profiler.yaml b/configs/debug/profiler.yaml
index 2bd7da8..e18df1c 100644
--- a/configs/debug/profiler.yaml
+++ b/configs/debug/profiler.yaml
@@ -3,7 +3,7 @@
 # runs with execution time profiling
 
 defaults:
-  - default
+  - default.yaml
 
 trainer:
   max_epochs: 1
diff --git a/configs/eval.yaml b/configs/eval.yaml
deleted file mode 100644
index be31299..0000000
--- a/configs/eval.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
-# @package _global_
-
-defaults:
-  - _self_
-  - data: mnist # choose datamodule with `test_dataloader()` for evaluation
-  - model: mnist
-  - logger: null
-  - trainer: default
-  - paths: default
-  - extras: default
-  - hydra: default
-
-task_name: "eval"
-
-tags: ["dev"]
-
-# passing checkpoint path is necessary for evaluation
-ckpt_path: ???
diff --git a/configs/experiment/example.yaml b/configs/experiment/example.yaml
deleted file mode 100644
index 9a93b54..0000000
--- a/configs/experiment/example.yaml
+++ /dev/null
@@ -1,41 +0,0 @@
-# @package _global_
-
-# to execute this experiment run:
-# python train.py experiment=example
-
-defaults:
-  - override /data: mnist
-  - override /model: mnist
-  - override /callbacks: default
-  - override /trainer: default
-
-# all parameters below will be merged with parameters from default configurations set above
-# this allows you to overwrite only specified parameters
-
-tags: ["mnist", "simple_dense_net"]
-
-seed: 12345
-
-trainer:
-  min_epochs: 10
-  max_epochs: 10
-  gradient_clip_val: 0.5
-
-model:
-  optimizer:
-    lr: 0.002
-  net:
-    lin1_size: 128
-    lin2_size: 256
-    lin3_size: 64
-  compile: false
-
-data:
-  batch_size: 64
-
-logger:
-  wandb:
-    tags: ${tags}
-    group: "mnist"
-  aim:
-    experiment: "mnist"
diff --git a/configs/extras/default.yaml b/configs/extras/default.yaml
index b9c6b62..de8f061 100644
--- a/configs/extras/default.yaml
+++ b/configs/extras/default.yaml
@@ -5,4 +5,4 @@ ignore_warnings: False
 enforce_tags: True
 
 # pretty print config tree at the start of the run using Rich library
-print_config: True
+print_config: True
\ No newline at end of file
diff --git a/configs/hparams_search/mnist_optuna.yaml b/configs/hparams_search/mnist_optuna.yaml
deleted file mode 100644
index 1391183..0000000
--- a/configs/hparams_search/mnist_optuna.yaml
+++ /dev/null
@@ -1,52 +0,0 @@
-# @package _global_
-
-# example hyperparameter optimization of some experiment with Optuna:
-# python train.py -m hparams_search=mnist_optuna experiment=example
-
-defaults:
-  - override /hydra/sweeper: optuna
-
-# choose metric which will be optimized by Optuna
-# make sure this is the correct name of some metric logged in lightning module!
-optimized_metric: "val/acc_best"
-
-# here we define Optuna hyperparameter search
-# it optimizes for value returned from function with @hydra.main decorator
-# docs: https://hydra.cc/docs/next/plugins/optuna_sweeper
-hydra:
-  mode: "MULTIRUN" # set hydra to multirun by default if this config is attached
-
-  sweeper:
-    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
-
-    # storage URL to persist optimization results
-    # for example, you can use SQLite if you set 'sqlite:///example.db'
-    storage: null
-
-    # name of the study to persist optimization results
-    study_name: null
-
-    # number of parallel workers
-    n_jobs: 1
-
-    # 'minimize' or 'maximize' the objective
-    direction: maximize
-
-    # total number of runs that will be executed
-    n_trials: 20
-
-    # choose Optuna hyperparameter sampler
-    # you can choose bayesian sampler (tpe), random search (without optimization), grid sampler, and others
-    # docs: https://optuna.readthedocs.io/en/stable/reference/samplers.html
-    sampler:
-      _target_: optuna.samplers.TPESampler
-      seed: 1234
-      n_startup_trials: 10 # number of random sampling runs before optimization starts
-
-    # define hyperparameter search space
-    params:
-      model.optimizer.lr: interval(0.0001, 0.1)
-      data.batch_size: choice(32, 64, 128, 256)
-      model.net.lin1_size: choice(64, 128, 256)
-      model.net.lin2_size: choice(64, 128, 256)
-      model.net.lin3_size: choice(32, 64, 128, 256)
diff --git a/configs/hydra/default.yaml b/configs/hydra/default.yaml
index a61e9b3..51b93a9 100644
--- a/configs/hydra/default.yaml
+++ b/configs/hydra/default.yaml
@@ -10,10 +10,4 @@ run:
   dir: ${paths.log_dir}/${task_name}/runs/${now:%Y-%m-%d}_${now:%H-%M-%S}
 sweep:
   dir: ${paths.log_dir}/${task_name}/multiruns/${now:%Y-%m-%d}_${now:%H-%M-%S}
-  subdir: ${hydra.job.num}
-
-job_logging:
-  handlers:
-    file:
-      # Incorporates fix from https://github.com/facebookresearch/hydra/pull/2242
-      filename: ${hydra.runtime.output_dir}/${task_name}.log
+  subdir: ${hydra.job.num}
\ No newline at end of file
diff --git a/configs/logger/aim.yaml b/configs/logger/aim.yaml
deleted file mode 100644
index 8f9f6ad..0000000
--- a/configs/logger/aim.yaml
+++ /dev/null
@@ -1,28 +0,0 @@
-# https://aimstack.io/
-
-# example usage in lightning module:
-# https://github.com/aimhubio/aim/blob/main/examples/pytorch_lightning_track.py
-
-# open the Aim UI with the following command (run in the folder containing the `.aim` folder):
-# `aim up`
-
-aim:
-  _target_: aim.pytorch_lightning.AimLogger
-  repo: ${paths.root_dir} # .aim folder will be created here
-  # repo: "aim://ip_address:port" # can instead provide IP address pointing to Aim remote tracking server which manages the repo, see https://aimstack.readthedocs.io/en/latest/using/remote_tracking.html#
-
-  # aim allows to group runs under experiment name
-  experiment: null # any string, set to "default" if not specified
-
-  train_metric_prefix: "train/"
-  val_metric_prefix: "val/"
-  test_metric_prefix: "test/"
-
-  # sets the tracking interval in seconds for system usage metrics (CPU, GPU, memory, etc.)
-  system_tracking_interval: 10 # set to null to disable system metrics tracking
-
-  # enable/disable logging of system params such as installed packages, git info, env vars, etc.
-  log_system_params: true
-
-  # enable/disable tracking console logs (default value is true)
-  capture_terminal_logs: false # set to false to avoid infinite console log loop issue https://github.com/aimhubio/aim/issues/2550
diff --git a/configs/logger/comet.yaml b/configs/logger/comet.yaml
index e078927..6ac99f4 100644
--- a/configs/logger/comet.yaml
+++ b/configs/logger/comet.yaml
@@ -1,12 +1,7 @@
 # https://www.comet.ml
 
 comet:
-  _target_: lightning.pytorch.loggers.comet.CometLogger
+  _target_: pytorch_lightning.loggers.comet.CometLogger
   api_key: ${oc.env:COMET_API_TOKEN} # api key is loaded from environment variable
-  save_dir: "${paths.output_dir}"
-  project_name: "lightning-hydra-template"
-  rest_api_key: null
-  # experiment_name: ""
-  experiment_key: null # set to resume experiment
-  offline: False
-  prefix: ""
+  project_name: "template-tests"
+  experiment_name: ${name}
diff --git a/configs/logger/csv.yaml b/configs/logger/csv.yaml
index fa028e9..aaec6d7 100644
--- a/configs/logger/csv.yaml
+++ b/configs/logger/csv.yaml
@@ -1,7 +1,7 @@
 # csv logger built in lightning
 
 csv:
-  _target_: lightning.pytorch.loggers.csv_logs.CSVLogger
-  save_dir: "${paths.output_dir}"
+  _target_: pytorch_lightning.loggers.csv_logs.CSVLogger
+  save_dir: "."
   name: "csv/"
   prefix: ""
diff --git a/configs/logger/many_loggers.yaml b/configs/logger/many_loggers.yaml
index dd58680..801444d 100644
--- a/configs/logger/many_loggers.yaml
+++ b/configs/logger/many_loggers.yaml
@@ -1,9 +1,9 @@
 # train with many loggers at once
 
 defaults:
-  # - comet
-  - csv
-  # - mlflow
-  # - neptune
-  - tensorboard
-  - wandb
+  # - comet.yaml
+  - csv.yaml
+  # - mlflow.yaml
+  # - neptune.yaml
+  - tensorboard.yaml
+  - wandb.yaml
diff --git a/configs/logger/mlflow.yaml b/configs/logger/mlflow.yaml
index f8fb7e6..130d3de 100644
--- a/configs/logger/mlflow.yaml
+++ b/configs/logger/mlflow.yaml
@@ -1,12 +1,9 @@
 # https://mlflow.org
 
 mlflow:
-  _target_: lightning.pytorch.loggers.mlflow.MLFlowLogger
-  # experiment_name: ""
-  # run_name: ""
-  tracking_uri: ${paths.log_dir}/mlflow/mlruns # run `mlflow ui` command inside the `logs/mlflow/` dir to open the UI
+  _target_: pytorch_lightning.loggers.mlflow.MLFlowLogger
+  experiment_name: ${name}
+  tracking_uri: ${original_work_dir}/logs/mlflow/mlruns # run `mlflow ui` command inside the `logs/mlflow/` dir to open the UI
   tags: null
-  # save_dir: "./mlruns"
   prefix: ""
   artifact_location: null
-  # run_id: ""
diff --git a/configs/logger/neptune.yaml b/configs/logger/neptune.yaml
index 8233c14..d80f9de 100644
--- a/configs/logger/neptune.yaml
+++ b/configs/logger/neptune.yaml
@@ -1,9 +1,11 @@
 # https://neptune.ai
 
 neptune:
-  _target_: lightning.pytorch.loggers.neptune.NeptuneLogger
+  _target_: pytorch_lightning.loggers.neptune.NeptuneLogger
   api_key: ${oc.env:NEPTUNE_API_TOKEN} # api key is loaded from environment variable
-  project: username/lightning-hydra-template
-  # name: ""
-  log_model_checkpoints: True
+  project_name: charliemarx/test
+  close_after_fit: True
+  offline_mode: False
+  experiment_name: ${name}
+  experiment_id: null
   prefix: ""
diff --git a/configs/logger/tensorboard.yaml b/configs/logger/tensorboard.yaml
index 2bd31f6..b0e0531 100644
--- a/configs/logger/tensorboard.yaml
+++ b/configs/logger/tensorboard.yaml
@@ -1,10 +1,10 @@
 # https://www.tensorflow.org/tensorboard/
 
 tensorboard:
-  _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
-  save_dir: "${paths.output_dir}/tensorboard/"
+  _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
+  save_dir: "tensorboard/"
   name: null
+  version: ${name}
   log_graph: False
   default_hp_metric: True
   prefix: ""
-  # version: ""
diff --git a/configs/logger/wandb.yaml b/configs/logger/wandb.yaml
index ece1658..f0b84b4 100644
--- a/configs/logger/wandb.yaml
+++ b/configs/logger/wandb.yaml
@@ -1,16 +1,20 @@
 # https://wandb.ai
 
 wandb:
-  _target_: lightning.pytorch.loggers.wandb.WandbLogger
-  # name: "" # name of the run (normally generated by wandb)
-  save_dir: "${paths.output_dir}"
-  offline: False
+  _target_: pytorch_lightning.loggers.wandb.WandbLogger
+  project: "DM_project"
+  name: ${name}
+  save_dir: "."
+  offline: False # set True to store all logs only locally
   id: null # pass correct id to resume experiment!
-  anonymous: null # enable anonymous logging
-  project: "lightning-hydra-template"
-  log_model: False # upload lightning ckpts
-  prefix: "" # a string to put at the beginning of metric keys
-  # entity: "" # set to name of your wandb team
+
+  # settings:
+  #   _target_: wandb.Settings
+  #   start_method: "thread"
+
+  # entity: ""  # set to name of your wandb team
+  log_model: False
+  prefix: ""
+  job_type: "train"
   group: ""
   tags: []
-  job_type: ""
diff --git a/configs/model/mnist.yaml b/configs/model/mnist.yaml
index 6f9c2fa..7799b31 100644
--- a/configs/model/mnist.yaml
+++ b/configs/model/mnist.yaml
@@ -1,25 +1,11 @@
 _target_: src.models.mnist_module.MNISTLitModule
-
-optimizer:
-  _target_: torch.optim.Adam
-  _partial_: true
-  lr: 0.001
-  weight_decay: 0.0
-
-scheduler:
-  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
-  _partial_: true
-  mode: min
-  factor: 0.1
-  patience: 10
+lr: 0.001
+weight_decay: 0.0005
 
 net:
   _target_: src.models.components.simple_dense_net.SimpleDenseNet
   input_size: 784
-  lin1_size: 64
-  lin2_size: 128
-  lin3_size: 64
+  lin1_size: 256
+  lin2_size: 256
+  lin3_size: 256
   output_size: 10
-
-# compile model for faster training with pytorch 2.0
-compile: false
diff --git a/configs/paths/default.yaml b/configs/paths/default.yaml
index ec81db2..5f8c553 100644
--- a/configs/paths/default.yaml
+++ b/configs/paths/default.yaml
@@ -1,6 +1,6 @@
 # path to root directory
 # this requires PROJECT_ROOT environment variable to exist
-# you can replace it with "." if you want the root to be the current working directory
+# PROJECT_ROOT is inferred and set by pyrootutils package in `train.py` and `eval.py`
 root_dir: ${oc.env:PROJECT_ROOT}
 
 # path to data directory
@@ -15,4 +15,4 @@ log_dir: ${paths.root_dir}/logs/
 output_dir: ${hydra:runtime.output_dir}
 
 # path to working directory
-work_dir: ${hydra:runtime.cwd}
+work_dir: ${hydra:runtime.cwd}
\ No newline at end of file
diff --git a/configs/train.yaml b/configs/train.yaml
index ef7bdab..a7f5d49 100644
--- a/configs/train.yaml
+++ b/configs/train.yaml
@@ -1,31 +1,30 @@
 # @package _global_
 
-# specify here default configuration
-# order of defaults determines the order in which configs override each other
+# specify here default training configuration
 defaults:
   - _self_
-  - data: mnist
-  - model: mnist
-  - callbacks: default
+  - datamodule: classification.yaml
+  - model: classification.yaml
+  - callbacks: default.yaml
   - logger: null # set logger here or use command line (e.g. `python train.py logger=tensorboard`)
-  - trainer: default
-  - paths: default
-  - extras: default
-  - hydra: default
+  - trainer: default.yaml
+  - paths: default.yaml
+  - extras: default.yaml
+  - hydra: default.yaml
 
-  # experiment configs allow for version control of specific hyperparameters
-  # e.g. best hyperparameters for given model and datamodule
+  # experiment configs allow for version control of specific configurations
+  # e.g. best hyperparameters for each combination of model and datamodule
   - experiment: null
 
+  # debugging config (enable through command line, e.g. `python train.py debug=default)
+  - debug: null
+
   # config for hyperparameter optimization
   - hparams_search: null
 
   # optional local config for machine/user specific settings
   # it's optional since it doesn't need to exist and is excluded from version control
-  - optional local: default
-
-  # debugging config (enable through command line, e.g. `python train.py debug=default)
-  - debug: null
+  - optional local: default.yaml
 
 # task name, determines output directory path
 task_name: "train"
@@ -33,6 +32,8 @@ task_name: "train"
 # tags to help you identify your experiments
 # you can overwrite this in experiment configs
 # overwrite from command line with `python train.py tags="[first_tag, second_tag]"`
+# appending lists from command line is currently not supported :(
+# https://github.com/facebookresearch/hydra/issues/1547
 tags: ["dev"]
 
 # set False to skip model training
diff --git a/configs/trainer/cpu.yaml b/configs/trainer/cpu.yaml
deleted file mode 100644
index b7d6767..0000000
--- a/configs/trainer/cpu.yaml
+++ /dev/null
@@ -1,5 +0,0 @@
-defaults:
-  - default
-
-accelerator: cpu
-devices: 1
diff --git a/configs/trainer/ddp.yaml b/configs/trainer/ddp.yaml
index ab8f890..a317b86 100644
--- a/configs/trainer/ddp.yaml
+++ b/configs/trainer/ddp.yaml
@@ -1,9 +1,5 @@
 defaults:
-  - default
+  - default.yaml
 
 strategy: ddp
-
-accelerator: gpu
-devices: 4
-num_nodes: 1
 sync_batchnorm: True
diff --git a/configs/trainer/ddp_sim.yaml b/configs/trainer/ddp_sim.yaml
deleted file mode 100644
index 8404419..0000000
--- a/configs/trainer/ddp_sim.yaml
+++ /dev/null
@@ -1,7 +0,0 @@
-defaults:
-  - default
-
-# simulate DDP on CPU, useful for debugging
-accelerator: cpu
-devices: 2
-strategy: ddp_spawn
diff --git a/configs/trainer/default.yaml b/configs/trainer/default.yaml
index 50905e7..23cf33f 100644
--- a/configs/trainer/default.yaml
+++ b/configs/trainer/default.yaml
@@ -1,19 +1,10 @@
-_target_: lightning.pytorch.trainer.Trainer
+_target_: pytorch_lightning.Trainer
 
-default_root_dir: ${paths.output_dir}
 
-min_epochs: 1 # prevents early stopping
-max_epochs: 10
+min_epochs: 1
+max_epochs: 200
 
-accelerator: cpu
-devices: 1
+# number of validation steps to execute at the beginning of the training
+# num_sanity_val_steps: 0
 
-# mixed precision for extra speed-up
-# precision: 16
 
-# perform a validation loop every N training epochs
-check_val_every_n_epoch: 1
-
-# set True to to ensure deterministic results
-# makes training slower but gives more reproducibility than just setting seeds
-deterministic: False
diff --git a/configs/trainer/gpu.yaml b/configs/trainer/gpu.yaml
deleted file mode 100644
index b238951..0000000
--- a/configs/trainer/gpu.yaml
+++ /dev/null
@@ -1,5 +0,0 @@
-defaults:
-  - default
-
-accelerator: gpu
-devices: 1
diff --git a/configs/trainer/mps.yaml b/configs/trainer/mps.yaml
deleted file mode 100644
index 1ecf6d5..0000000
--- a/configs/trainer/mps.yaml
+++ /dev/null
@@ -1,5 +0,0 @@
-defaults:
-  - default
-
-accelerator: mps
-devices: 1
diff --git a/data/.gitkeep b/data/.gitkeep
deleted file mode 100644
index e69de29..0000000
diff --git a/environment.yaml b/environment.yaml
deleted file mode 100644
index f74ee8c..0000000
--- a/environment.yaml
+++ /dev/null
@@ -1,45 +0,0 @@
-# reasons you might want to use `environment.yaml` instead of `requirements.txt`:
-# - pip installs packages in a loop, without ensuring dependencies across all packages
-#   are fulfilled simultaneously, but conda achieves proper dependency control across
-#   all packages
-# - conda allows for installing packages without requiring certain compilers or
-#   libraries to be available in the system, since it installs precompiled binaries
-
-name: myenv
-
-channels:
-  - pytorch
-  - conda-forge
-  - defaults
-
-# it is strongly recommended to specify versions of packages installed through conda
-# to avoid situation when version-unspecified packages install their latest major
-# versions which can sometimes break things
-
-# current approach below keeps the dependencies in the same major versions across all
-# users, but allows for different minor and patch versions of packages where backwards
-# compatibility is usually guaranteed
-
-dependencies:
-  - python=3.10
-  - pytorch=2.*
-  - torchvision=0.*
-  - lightning=2.*
-  - torchmetrics=0.*
-  - hydra-core=1.*
-  - rich=13.*
-  - pre-commit=3.*
-  - pytest=7.*
-
-  # --------- loggers --------- #
-  # - wandb
-  # - neptune-client
-  # - mlflow
-  # - comet-ml
-  # - aim>=3.16.2 # no lower than 3.16.2, see https://github.com/aimhubio/aim/issues/2550
-
-  - pip>=23
-  - pip:
-      - hydra-optuna-sweeper
-      - hydra-colorlog
-      - rootutils
diff --git a/logs/.gitkeep b/logs/.gitkeep
deleted file mode 100644
index e69de29..0000000
diff --git a/notebooks/.gitkeep b/notebooks/.gitkeep
deleted file mode 100644
index e69de29..0000000
diff --git a/pyproject.toml b/pyproject.toml
deleted file mode 100644
index 300ebf0..0000000
--- a/pyproject.toml
+++ /dev/null
@@ -1,25 +0,0 @@
-[tool.pytest.ini_options]
-addopts = [
-  "--color=yes",
-  "--durations=0",
-  "--strict-markers",
-  "--doctest-modules",
-]
-filterwarnings = [
-  "ignore::DeprecationWarning",
-  "ignore::UserWarning",
-]
-log_cli = "True"
-markers = [
-  "slow: slow tests",
-]
-minversion = "6.0"
-testpaths = "tests/"
-
-[tool.coverage.report]
-exclude_lines = [
-    "pragma: nocover",
-    "raise NotImplementedError",
-    "raise NotImplementedError()",
-    "if __name__ == .__main__.:",
-]
diff --git a/requirements.txt b/requirements.txt
index d837268..e4ccd50 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,24 +1,15 @@
-# --------- pytorch --------- #
-torch>=2.0.0
-torchvision>=0.15.0
-lightning>=2.0.0
-torchmetrics>=0.11.4
+torch<=2.4.0
+torchvision
+torchmetrics>=0.7.0
+pytorch_lightning==2.4
+pyrootutils==1.0.4
+python-dotenv==1.0.1
 
-# --------- hydra --------- #
-hydra-core==1.3.2
+hydra-core==1.2.0
 hydra-colorlog==1.2.0
 hydra-optuna-sweeper==1.2.0
 
-# --------- loggers --------- #
-# wandb
-# neptune-client
-# mlflow
-# comet-ml
-# aim>=3.16.2  # no lower than 3.16.2, see https://github.com/aimhubio/aim/issues/2550
-
-# --------- others --------- #
-rootutils       # standardizing the project root setup
-pre-commit      # hooks for applying linters on commit
-rich            # beautiful text formatting in terminal
-pytest          # tests
-# sh            # for running bash commands in some tests (linux/macos only)
+pandas
+matplotlib
+rich
+wandb
\ No newline at end of file
diff --git a/scripts/schedule.sh b/scripts/schedule.sh
deleted file mode 100644
index 44b3da1..0000000
--- a/scripts/schedule.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/bin/bash
-# Schedule execution of many runs
-# Run from root folder with: bash scripts/schedule.sh
-
-python src/train.py trainer.max_epochs=5 logger=csv
-
-python src/train.py trainer.max_epochs=10 logger=csv
diff --git a/setup.py b/setup.py
deleted file mode 100644
index f1826cc..0000000
--- a/setup.py
+++ /dev/null
@@ -1,21 +0,0 @@
-#!/usr/bin/env python
-
-from setuptools import find_packages, setup
-
-setup(
-    name="src",
-    version="0.0.1",
-    description="Describe Your Cool Project",
-    author="",
-    author_email="",
-    url="https://github.com/user/project",
-    install_requires=["lightning", "hydra-core"],
-    packages=find_packages(),
-    # use this to customize global commands available in the terminal after installing the package
-    entry_points={
-        "console_scripts": [
-            "train_command = src.train:main",
-            "eval_command = src.eval:main",
-        ]
-    },
-)
diff --git a/src/__init__.py b/src/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/data/__init__.py b/src/data/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/data/components/__init__.py b/src/data/components/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/data/mnist_datamodule.py b/src/data/mnist_datamodule.py
deleted file mode 100644
index 88879ca..0000000
--- a/src/data/mnist_datamodule.py
+++ /dev/null
@@ -1,201 +0,0 @@
-from typing import Any, Dict, Optional, Tuple
-
-import torch
-from lightning import LightningDataModule
-from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split
-from torchvision.datasets import MNIST
-from torchvision.transforms import transforms
-
-
-class MNISTDataModule(LightningDataModule):
-    """`LightningDataModule` for the MNIST dataset.
-
-    The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples.
-    It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a
-    fixed-size image. The original black and white images from NIST were size normalized to fit in a 20x20 pixel box
-    while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing
-    technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of
-    mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.
-
-    A `LightningDataModule` implements 7 key methods:
-
-    ```python
-        def prepare_data(self):
-        # Things to do on 1 GPU/TPU (not on every GPU/TPU in DDP).
-        # Download data, pre-process, split, save to disk, etc...
-
-        def setup(self, stage):
-        # Things to do on every process in DDP.
-        # Load data, set variables, etc...
-
-        def train_dataloader(self):
-        # return train dataloader
-
-        def val_dataloader(self):
-        # return validation dataloader
-
-        def test_dataloader(self):
-        # return test dataloader
-
-        def predict_dataloader(self):
-        # return predict dataloader
-
-        def teardown(self, stage):
-        # Called on every process in DDP.
-        # Clean up after fit or test.
-    ```
-
-    This allows you to share a full dataset without explaining how to download,
-    split, transform and process the data.
-
-    Read the docs:
-        https://lightning.ai/docs/pytorch/latest/data/datamodule.html
-    """
-
-    def __init__(
-        self,
-        data_dir: str = "data/",
-        train_val_test_split: Tuple[int, int, int] = (55_000, 5_000, 10_000),
-        batch_size: int = 64,
-        num_workers: int = 0,
-        pin_memory: bool = False,
-    ) -> None:
-        """Initialize a `MNISTDataModule`.
-
-        :param data_dir: The data directory. Defaults to `"data/"`.
-        :param train_val_test_split: The train, validation and test split. Defaults to `(55_000, 5_000, 10_000)`.
-        :param batch_size: The batch size. Defaults to `64`.
-        :param num_workers: The number of workers. Defaults to `0`.
-        :param pin_memory: Whether to pin memory. Defaults to `False`.
-        """
-        super().__init__()
-
-        # this line allows to access init params with 'self.hparams' attribute
-        # also ensures init params will be stored in ckpt
-        self.save_hyperparameters(logger=False)
-
-        # data transformations
-        self.transforms = transforms.Compose(
-            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
-        )
-
-        self.data_train: Optional[Dataset] = None
-        self.data_val: Optional[Dataset] = None
-        self.data_test: Optional[Dataset] = None
-
-        self.batch_size_per_device = batch_size
-
-    @property
-    def num_classes(self) -> int:
-        """Get the number of classes.
-
-        :return: The number of MNIST classes (10).
-        """
-        return 10
-
-    def prepare_data(self) -> None:
-        """Download data if needed. Lightning ensures that `self.prepare_data()` is called only
-        within a single process on CPU, so you can safely add your downloading logic within. In
-        case of multi-node training, the execution of this hook depends upon
-        `self.prepare_data_per_node()`.
-
-        Do not use it to assign state (self.x = y).
-        """
-        MNIST(self.hparams.data_dir, train=True, download=True)
-        MNIST(self.hparams.data_dir, train=False, download=True)
-
-    def setup(self, stage: Optional[str] = None) -> None:
-        """Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.
-
-        This method is called by Lightning before `trainer.fit()`, `trainer.validate()`, `trainer.test()`, and
-        `trainer.predict()`, so be careful not to execute things like random split twice! Also, it is called after
-        `self.prepare_data()` and there is a barrier in between which ensures that all the processes proceed to
-        `self.setup()` once the data is prepared and available for use.
-
-        :param stage: The stage to setup. Either `"fit"`, `"validate"`, `"test"`, or `"predict"`. Defaults to ``None``.
-        """
-        # Divide batch size by the number of devices.
-        if self.trainer is not None:
-            if self.hparams.batch_size % self.trainer.world_size != 0:
-                raise RuntimeError(
-                    f"Batch size ({self.hparams.batch_size}) is not divisible by the number of devices ({self.trainer.world_size})."
-                )
-            self.batch_size_per_device = self.hparams.batch_size // self.trainer.world_size
-
-        # load and split datasets only if not loaded already
-        if not self.data_train and not self.data_val and not self.data_test:
-            trainset = MNIST(self.hparams.data_dir, train=True, transform=self.transforms)
-            testset = MNIST(self.hparams.data_dir, train=False, transform=self.transforms)
-            dataset = ConcatDataset(datasets=[trainset, testset])
-            self.data_train, self.data_val, self.data_test = random_split(
-                dataset=dataset,
-                lengths=self.hparams.train_val_test_split,
-                generator=torch.Generator().manual_seed(42),
-            )
-
-    def train_dataloader(self) -> DataLoader[Any]:
-        """Create and return the train dataloader.
-
-        :return: The train dataloader.
-        """
-        return DataLoader(
-            dataset=self.data_train,
-            batch_size=self.batch_size_per_device,
-            num_workers=self.hparams.num_workers,
-            pin_memory=self.hparams.pin_memory,
-            shuffle=True,
-        )
-
-    def val_dataloader(self) -> DataLoader[Any]:
-        """Create and return the validation dataloader.
-
-        :return: The validation dataloader.
-        """
-        return DataLoader(
-            dataset=self.data_val,
-            batch_size=self.batch_size_per_device,
-            num_workers=self.hparams.num_workers,
-            pin_memory=self.hparams.pin_memory,
-            shuffle=False,
-        )
-
-    def test_dataloader(self) -> DataLoader[Any]:
-        """Create and return the test dataloader.
-
-        :return: The test dataloader.
-        """
-        return DataLoader(
-            dataset=self.data_test,
-            batch_size=self.batch_size_per_device,
-            num_workers=self.hparams.num_workers,
-            pin_memory=self.hparams.pin_memory,
-            shuffle=False,
-        )
-
-    def teardown(self, stage: Optional[str] = None) -> None:
-        """Lightning hook for cleaning up after `trainer.fit()`, `trainer.validate()`,
-        `trainer.test()`, and `trainer.predict()`.
-
-        :param stage: The stage being torn down. Either `"fit"`, `"validate"`, `"test"`, or `"predict"`.
-            Defaults to ``None``.
-        """
-        pass
-
-    def state_dict(self) -> Dict[Any, Any]:
-        """Called when saving a checkpoint. Implement to generate and save the datamodule state.
-
-        :return: A dictionary containing the datamodule state that you want to save.
-        """
-        return {}
-
-    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
-        """Called when loading a checkpoint. Implement to reload datamodule state given datamodule
-        `state_dict()`.
-
-        :param state_dict: The datamodule state returned by `self.state_dict()`.
-        """
-        pass
-
-
-if __name__ == "__main__":
-    _ = MNISTDataModule()
diff --git a/src/eval.py b/src/eval.py
deleted file mode 100644
index b70faae..0000000
--- a/src/eval.py
+++ /dev/null
@@ -1,99 +0,0 @@
-from typing import Any, Dict, List, Tuple
-
-import hydra
-import rootutils
-from lightning import LightningDataModule, LightningModule, Trainer
-from lightning.pytorch.loggers import Logger
-from omegaconf import DictConfig
-
-rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)
-# ------------------------------------------------------------------------------------ #
-# the setup_root above is equivalent to:
-# - adding project root dir to PYTHONPATH
-#       (so you don't need to force user to install project as a package)
-#       (necessary before importing any local modules e.g. `from src import utils`)
-# - setting up PROJECT_ROOT environment variable
-#       (which is used as a base for paths in "configs/paths/default.yaml")
-#       (this way all filepaths are the same no matter where you run the code)
-# - loading environment variables from ".env" in root dir
-#
-# you can remove it if you:
-# 1. either install project as a package or move entry files to project root dir
-# 2. set `root_dir` to "." in "configs/paths/default.yaml"
-#
-# more info: https://github.com/ashleve/rootutils
-# ------------------------------------------------------------------------------------ #
-
-from src.utils import (
-    RankedLogger,
-    extras,
-    instantiate_loggers,
-    log_hyperparameters,
-    task_wrapper,
-)
-
-log = RankedLogger(__name__, rank_zero_only=True)
-
-
-@task_wrapper
-def evaluate(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
-    """Evaluates given checkpoint on a datamodule testset.
-
-    This method is wrapped in optional @task_wrapper decorator, that controls the behavior during
-    failure. Useful for multiruns, saving info about the crash, etc.
-
-    :param cfg: DictConfig configuration composed by Hydra.
-    :return: Tuple[dict, dict] with metrics and dict with all instantiated objects.
-    """
-    assert cfg.ckpt_path
-
-    log.info(f"Instantiating datamodule <{cfg.data._target_}>")
-    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)
-
-    log.info(f"Instantiating model <{cfg.model._target_}>")
-    model: LightningModule = hydra.utils.instantiate(cfg.model)
-
-    log.info("Instantiating loggers...")
-    logger: List[Logger] = instantiate_loggers(cfg.get("logger"))
-
-    log.info(f"Instantiating trainer <{cfg.trainer._target_}>")
-    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)
-
-    object_dict = {
-        "cfg": cfg,
-        "datamodule": datamodule,
-        "model": model,
-        "logger": logger,
-        "trainer": trainer,
-    }
-
-    if logger:
-        log.info("Logging hyperparameters!")
-        log_hyperparameters(object_dict)
-
-    log.info("Starting testing!")
-    trainer.test(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)
-
-    # for predictions use trainer.predict(...)
-    # predictions = trainer.predict(model=model, dataloaders=dataloaders, ckpt_path=cfg.ckpt_path)
-
-    metric_dict = trainer.callback_metrics
-
-    return metric_dict, object_dict
-
-
-@hydra.main(version_base="1.3", config_path="../configs", config_name="eval.yaml")
-def main(cfg: DictConfig) -> None:
-    """Main entry point for evaluation.
-
-    :param cfg: DictConfig configuration composed by Hydra.
-    """
-    # apply extra utilities
-    # (e.g. ask for tags if none are provided in cfg, print cfg tree, etc.)
-    extras(cfg)
-
-    evaluate(cfg)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/src/models/__init__.py b/src/models/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/models/components/__init__.py b/src/models/components/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/src/models/components/simple_dense_net.py b/src/models/components/simple_dense_net.py
deleted file mode 100644
index ae817c9..0000000
--- a/src/models/components/simple_dense_net.py
+++ /dev/null
@@ -1,54 +0,0 @@
-import torch
-from torch import nn
-
-
-class SimpleDenseNet(nn.Module):
-    """A simple fully-connected neural net for computing predictions."""
-
-    def __init__(
-        self,
-        input_size: int = 784,
-        lin1_size: int = 256,
-        lin2_size: int = 256,
-        lin3_size: int = 256,
-        output_size: int = 10,
-    ) -> None:
-        """Initialize a `SimpleDenseNet` module.
-
-        :param input_size: The number of input features.
-        :param lin1_size: The number of output features of the first linear layer.
-        :param lin2_size: The number of output features of the second linear layer.
-        :param lin3_size: The number of output features of the third linear layer.
-        :param output_size: The number of output features of the final linear layer.
-        """
-        super().__init__()
-
-        self.model = nn.Sequential(
-            nn.Linear(input_size, lin1_size),
-            nn.BatchNorm1d(lin1_size),
-            nn.ReLU(),
-            nn.Linear(lin1_size, lin2_size),
-            nn.BatchNorm1d(lin2_size),
-            nn.ReLU(),
-            nn.Linear(lin2_size, lin3_size),
-            nn.BatchNorm1d(lin3_size),
-            nn.ReLU(),
-            nn.Linear(lin3_size, output_size),
-        )
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        """Perform a single forward pass through the network.
-
-        :param x: The input tensor.
-        :return: A tensor of predictions.
-        """
-        batch_size, channels, width, height = x.size()
-
-        # (batch, 1, width, height) -> (batch, 1*width*height)
-        x = x.view(batch_size, -1)
-
-        return self.model(x)
-
-
-if __name__ == "__main__":
-    _ = SimpleDenseNet()
diff --git a/src/models/mnist_module.py b/src/models/mnist_module.py
deleted file mode 100644
index 5d303ac..0000000
--- a/src/models/mnist_module.py
+++ /dev/null
@@ -1,217 +0,0 @@
-from typing import Any, Dict, Tuple
-
-import torch
-from lightning import LightningModule
-from torchmetrics import MaxMetric, MeanMetric
-from torchmetrics.classification.accuracy import Accuracy
-
-
-class MNISTLitModule(LightningModule):
-    """Example of a `LightningModule` for MNIST classification.
-
-    A `LightningModule` implements 8 key methods:
-
-    ```python
-    def __init__(self):
-    # Define initialization code here.
-
-    def setup(self, stage):
-    # Things to setup before each stage, 'fit', 'validate', 'test', 'predict'.
-    # This hook is called on every process when using DDP.
-
-    def training_step(self, batch, batch_idx):
-    # The complete training step.
-
-    def validation_step(self, batch, batch_idx):
-    # The complete validation step.
-
-    def test_step(self, batch, batch_idx):
-    # The complete test step.
-
-    def predict_step(self, batch, batch_idx):
-    # The complete predict step.
-
-    def configure_optimizers(self):
-    # Define and configure optimizers and LR schedulers.
-    ```
-
-    Docs:
-        https://lightning.ai/docs/pytorch/latest/common/lightning_module.html
-    """
-
-    def __init__(
-        self,
-        net: torch.nn.Module,
-        optimizer: torch.optim.Optimizer,
-        scheduler: torch.optim.lr_scheduler,
-        compile: bool,
-    ) -> None:
-        """Initialize a `MNISTLitModule`.
-
-        :param net: The model to train.
-        :param optimizer: The optimizer to use for training.
-        :param scheduler: The learning rate scheduler to use for training.
-        """
-        super().__init__()
-
-        # this line allows to access init params with 'self.hparams' attribute
-        # also ensures init params will be stored in ckpt
-        self.save_hyperparameters(logger=False)
-
-        self.net = net
-
-        # loss function
-        self.criterion = torch.nn.CrossEntropyLoss()
-
-        # metric objects for calculating and averaging accuracy across batches
-        self.train_acc = Accuracy(task="multiclass", num_classes=10)
-        self.val_acc = Accuracy(task="multiclass", num_classes=10)
-        self.test_acc = Accuracy(task="multiclass", num_classes=10)
-
-        # for averaging loss across batches
-        self.train_loss = MeanMetric()
-        self.val_loss = MeanMetric()
-        self.test_loss = MeanMetric()
-
-        # for tracking best so far validation accuracy
-        self.val_acc_best = MaxMetric()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        """Perform a forward pass through the model `self.net`.
-
-        :param x: A tensor of images.
-        :return: A tensor of logits.
-        """
-        return self.net(x)
-
-    def on_train_start(self) -> None:
-        """Lightning hook that is called when training begins."""
-        # by default lightning executes validation step sanity checks before training starts,
-        # so it's worth to make sure validation metrics don't store results from these checks
-        self.val_loss.reset()
-        self.val_acc.reset()
-        self.val_acc_best.reset()
-
-    def model_step(
-        self, batch: Tuple[torch.Tensor, torch.Tensor]
-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-        """Perform a single model step on a batch of data.
-
-        :param batch: A batch of data (a tuple) containing the input tensor of images and target labels.
-
-        :return: A tuple containing (in order):
-            - A tensor of losses.
-            - A tensor of predictions.
-            - A tensor of target labels.
-        """
-        x, y = batch
-        logits = self.forward(x)
-        loss = self.criterion(logits, y)
-        preds = torch.argmax(logits, dim=1)
-        return loss, preds, y
-
-    def training_step(
-        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int
-    ) -> torch.Tensor:
-        """Perform a single training step on a batch of data from the training set.
-
-        :param batch: A batch of data (a tuple) containing the input tensor of images and target
-            labels.
-        :param batch_idx: The index of the current batch.
-        :return: A tensor of losses between model predictions and targets.
-        """
-        loss, preds, targets = self.model_step(batch)
-
-        # update and log metrics
-        self.train_loss(loss)
-        self.train_acc(preds, targets)
-        self.log("train/loss", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)
-        self.log("train/acc", self.train_acc, on_step=False, on_epoch=True, prog_bar=True)
-
-        # return loss or backpropagation will fail
-        return loss
-
-    def on_train_epoch_end(self) -> None:
-        "Lightning hook that is called when a training epoch ends."
-        pass
-
-    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:
-        """Perform a single validation step on a batch of data from the validation set.
-
-        :param batch: A batch of data (a tuple) containing the input tensor of images and target
-            labels.
-        :param batch_idx: The index of the current batch.
-        """
-        loss, preds, targets = self.model_step(batch)
-
-        # update and log metrics
-        self.val_loss(loss)
-        self.val_acc(preds, targets)
-        self.log("val/loss", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)
-        self.log("val/acc", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)
-
-    def on_validation_epoch_end(self) -> None:
-        "Lightning hook that is called when a validation epoch ends."
-        acc = self.val_acc.compute()  # get current val acc
-        self.val_acc_best(acc)  # update best so far val acc
-        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object
-        # otherwise metric would be reset by lightning after each epoch
-        self.log("val/acc_best", self.val_acc_best.compute(), sync_dist=True, prog_bar=True)
-
-    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:
-        """Perform a single test step on a batch of data from the test set.
-
-        :param batch: A batch of data (a tuple) containing the input tensor of images and target
-            labels.
-        :param batch_idx: The index of the current batch.
-        """
-        loss, preds, targets = self.model_step(batch)
-
-        # update and log metrics
-        self.test_loss(loss)
-        self.test_acc(preds, targets)
-        self.log("test/loss", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)
-        self.log("test/acc", self.test_acc, on_step=False, on_epoch=True, prog_bar=True)
-
-    def on_test_epoch_end(self) -> None:
-        """Lightning hook that is called when a test epoch ends."""
-        pass
-
-    def setup(self, stage: str) -> None:
-        """Lightning hook that is called at the beginning of fit (train + validate), validate,
-        test, or predict.
-
-        This is a good hook when you need to build models dynamically or adjust something about
-        them. This hook is called on every process when using DDP.
-
-        :param stage: Either `"fit"`, `"validate"`, `"test"`, or `"predict"`.
-        """
-        if self.hparams.compile and stage == "fit":
-            self.net = torch.compile(self.net)
-
-    def configure_optimizers(self) -> Dict[str, Any]:
-        """Choose what optimizers and learning-rate schedulers to use in your optimization.
-        Normally you'd need one. But in the case of GANs or similar you might have multiple.
-
-        Examples:
-            https://lightning.ai/docs/pytorch/latest/common/lightning_module.html#configure-optimizers
-
-        :return: A dict containing the configured optimizers and learning-rate schedulers to be used for training.
-        """
-        optimizer = self.hparams.optimizer(params=self.trainer.model.parameters())
-        if self.hparams.scheduler is not None:
-            scheduler = self.hparams.scheduler(optimizer=optimizer)
-            return {
-                "optimizer": optimizer,
-                "lr_scheduler": {
-                    "scheduler": scheduler,
-                    "monitor": "val/loss",
-                    "interval": "epoch",
-                    "frequency": 1,
-                },
-            }
-        return {"optimizer": optimizer}
-
-
-if __name__ == "__main__":
-    _ = MNISTLitModule(None, None, None, None)
diff --git a/src/train.py b/src/train.py
deleted file mode 100644
index 4adbcf4..0000000
--- a/src/train.py
+++ /dev/null
@@ -1,132 +0,0 @@
-from typing import Any, Dict, List, Optional, Tuple
-
-import hydra
-import lightning as L
-import rootutils
-import torch
-from lightning import Callback, LightningDataModule, LightningModule, Trainer
-from lightning.pytorch.loggers import Logger
-from omegaconf import DictConfig
-
-rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)
-# ------------------------------------------------------------------------------------ #
-# the setup_root above is equivalent to:
-# - adding project root dir to PYTHONPATH
-#       (so you don't need to force user to install project as a package)
-#       (necessary before importing any local modules e.g. `from src import utils`)
-# - setting up PROJECT_ROOT environment variable
-#       (which is used as a base for paths in "configs/paths/default.yaml")
-#       (this way all filepaths are the same no matter where you run the code)
-# - loading environment variables from ".env" in root dir
-#
-# you can remove it if you:
-# 1. either install project as a package or move entry files to project root dir
-# 2. set `root_dir` to "." in "configs/paths/default.yaml"
-#
-# more info: https://github.com/ashleve/rootutils
-# ------------------------------------------------------------------------------------ #
-
-from src.utils import (
-    RankedLogger,
-    extras,
-    get_metric_value,
-    instantiate_callbacks,
-    instantiate_loggers,
-    log_hyperparameters,
-    task_wrapper,
-)
-
-log = RankedLogger(__name__, rank_zero_only=True)
-
-
-@task_wrapper
-def train(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
-    """Trains the model. Can additionally evaluate on a testset, using best weights obtained during
-    training.
-
-    This method is wrapped in optional @task_wrapper decorator, that controls the behavior during
-    failure. Useful for multiruns, saving info about the crash, etc.
-
-    :param cfg: A DictConfig configuration composed by Hydra.
-    :return: A tuple with metrics and dict with all instantiated objects.
-    """
-    # set seed for random number generators in pytorch, numpy and python.random
-    if cfg.get("seed"):
-        L.seed_everything(cfg.seed, workers=True)
-
-    log.info(f"Instantiating datamodule <{cfg.data._target_}>")
-    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)
-
-    log.info(f"Instantiating model <{cfg.model._target_}>")
-    model: LightningModule = hydra.utils.instantiate(cfg.model)
-
-    log.info("Instantiating callbacks...")
-    callbacks: List[Callback] = instantiate_callbacks(cfg.get("callbacks"))
-
-    log.info("Instantiating loggers...")
-    logger: List[Logger] = instantiate_loggers(cfg.get("logger"))
-
-    log.info(f"Instantiating trainer <{cfg.trainer._target_}>")
-    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=logger)
-
-    object_dict = {
-        "cfg": cfg,
-        "datamodule": datamodule,
-        "model": model,
-        "callbacks": callbacks,
-        "logger": logger,
-        "trainer": trainer,
-    }
-
-    if logger:
-        log.info("Logging hyperparameters!")
-        log_hyperparameters(object_dict)
-
-    if cfg.get("train"):
-        log.info("Starting training!")
-        trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
-
-    train_metrics = trainer.callback_metrics
-
-    if cfg.get("test"):
-        log.info("Starting testing!")
-        ckpt_path = trainer.checkpoint_callback.best_model_path
-        if ckpt_path == "":
-            log.warning("Best ckpt not found! Using current weights for testing...")
-            ckpt_path = None
-        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
-        log.info(f"Best ckpt path: {ckpt_path}")
-
-    test_metrics = trainer.callback_metrics
-
-    # merge train and test metrics
-    metric_dict = {**train_metrics, **test_metrics}
-
-    return metric_dict, object_dict
-
-
-@hydra.main(version_base="1.3", config_path="../configs", config_name="train.yaml")
-def main(cfg: DictConfig) -> Optional[float]:
-    """Main entry point for training.
-
-    :param cfg: DictConfig configuration composed by Hydra.
-    :return: Optional[float] with optimized metric value.
-    """
-    # apply extra utilities
-    # (e.g. ask for tags if none are provided in cfg, print cfg tree, etc.)
-    extras(cfg)
-
-    # train the model
-    metric_dict, _ = train(cfg)
-
-    # safely retrieve metric value for hydra-based hyperparameter optimization
-    metric_value = get_metric_value(
-        metric_dict=metric_dict, metric_name=cfg.get("optimized_metric")
-    )
-
-    # return optimized metric
-    return metric_value
-
-
-if __name__ == "__main__":
-    main()
diff --git a/src/utils/__init__.py b/src/utils/__init__.py
index 5b0707c..1fb172f 100644
--- a/src/utils/__init__.py
+++ b/src/utils/__init__.py
@@ -1,5 +1,164 @@
-from src.utils.instantiators import instantiate_callbacks, instantiate_loggers
-from src.utils.logging_utils import log_hyperparameters
-from src.utils.pylogger import RankedLogger
-from src.utils.rich_utils import enforce_tags, print_config_tree
-from src.utils.utils import extras, get_metric_value, task_wrapper
+import logging
+import warnings
+from typing import List, Sequence
+
+import pytorch_lightning as pl
+import rich.syntax
+import rich.tree
+from omegaconf import DictConfig, OmegaConf
+from pytorch_lightning.utilities import rank_zero_only
+import wandb
+from pytorch_lightning.loggers.logger import Logger ## Used for storing multiple loggers
+
+
+def get_logger(name=__name__) -> logging.Logger:
+    """Initializes multi-GPU-friendly python command line logger."""
+
+    logger = logging.getLogger(name)
+
+    # this ensures all logging levels get marked with the rank zero decorator
+    # otherwise logs would get multiplied for each GPU process in multi-GPU setup
+    for level in (
+        "debug",
+        "info",
+        "warning",
+        "error",
+        "exception",
+        "fatal",
+        "critical",
+    ):
+        setattr(logger, level, rank_zero_only(getattr(logger, level)))
+
+    return logger
+
+
+log = get_logger(__name__)
+
+
+def extras(config: DictConfig) -> None:
+    """Applies optional utilities, controlled by config flags.
+
+    Utilities:
+    - Ignoring python warnings
+    - Rich config printing
+    """
+
+    # disable python warnings if <config.ignore_warnings=True>
+    if config.get("ignore_warnings"):
+        log.info("Disabling python warnings! <config.ignore_warnings=True>")
+        warnings.filterwarnings("ignore")
+
+    # pretty print config tree using Rich library if <config.print_config=True>
+    if config.get("print_config"):
+        log.info("Printing config tree with Rich! <config.print_config=True>")
+        print_config(config, resolve=True)
+
+
+@rank_zero_only
+def print_config(
+    config: DictConfig,
+    print_order: Sequence[str] = (
+        "datamodule",
+        "model",
+        "callbacks",
+        "logger",
+        "trainer",
+    ),
+    resolve: bool = True,
+) -> None:
+    """Prints content of DictConfig using Rich library and its tree structure.
+
+    Args:
+        config (DictConfig): Configuration composed by Hydra.
+        print_order (Sequence[str], optional): Determines in what order config components are printed.
+        resolve (bool, optional): Whether to resolve reference fields of DictConfig.
+    """
+
+    style = "dim"
+    tree = rich.tree.Tree("CONFIG", style=style, guide_style=style)
+
+    quee = []
+
+    for field in print_order:
+        quee.append(field) if field in config else log.info(f"Field '{field}' not found in config")
+
+    for field in config:
+        if field not in quee:
+            quee.append(field)
+
+    for field in quee:
+        branch = tree.add(field, style=style, guide_style=style)
+
+        config_group = config[field]
+        if isinstance(config_group, DictConfig):
+            branch_content = OmegaConf.to_yaml(config_group, resolve=resolve)
+        else:
+            branch_content = str(config_group)
+
+        branch.add(rich.syntax.Syntax(branch_content, "yaml"))
+
+    rich.print(tree)
+
+    with open("config_tree.log", "w") as file:
+        rich.print(tree, file=file)
+
+
+@rank_zero_only
+def log_hyperparameters(
+    config: DictConfig,
+    model: pl.LightningModule,
+    datamodule: pl.LightningDataModule,
+    trainer: pl.Trainer,
+    callbacks: List[pl.Callback],
+    logger: List[Logger],
+) -> None:
+    """Controls which config parts are saved by Lightning loggers.
+
+    Additionaly saves:
+    - number of model parameters
+    """
+
+    if not trainer.logger:
+        return
+
+    hparams = {}
+
+    # choose which parts of hydra config will be saved to loggers
+    hparams["model"] = config["model"]
+
+    # save number of model parameters
+    hparams["model/params/total"] = sum(p.numel() for p in model.parameters())
+    hparams["model/params/trainable"] = sum(
+        p.numel() for p in model.parameters() if p.requires_grad
+    )
+    hparams["model/params/non_trainable"] = sum(
+        p.numel() for p in model.parameters() if not p.requires_grad
+    )
+
+    hparams["datamodule"] = config["datamodule"]
+    hparams["trainer"] = config["trainer"]
+
+    if "seed" in config:
+        hparams["seed"] = config["seed"]
+    if "callbacks" in config:
+        hparams["callbacks"] = config["callbacks"]
+
+    # send hparams to all loggers
+    trainer.logger.log_hyperparams(hparams)
+
+
+def finish(
+    config: DictConfig,
+    model: pl.LightningModule,
+    datamodule: pl.LightningDataModule,
+    trainer: pl.Trainer,
+    callbacks: List[pl.Callback],
+    logger: List[Logger],
+) -> None:
+    """Makes sure everything closed properly."""
+
+    # without this sweeps with wandb logger might crash!
+    for lg in logger:
+        if isinstance(lg, pl.loggers.wandb.WandbLogger):
+            import wandb
+            wandb.finish()
diff --git a/src/utils/instantiators.py b/src/utils/instantiators.py
deleted file mode 100644
index 82b9278..0000000
--- a/src/utils/instantiators.py
+++ /dev/null
@@ -1,56 +0,0 @@
-from typing import List
-
-import hydra
-from lightning import Callback
-from lightning.pytorch.loggers import Logger
-from omegaconf import DictConfig
-
-from src.utils import pylogger
-
-log = pylogger.RankedLogger(__name__, rank_zero_only=True)
-
-
-def instantiate_callbacks(callbacks_cfg: DictConfig) -> List[Callback]:
-    """Instantiates callbacks from config.
-
-    :param callbacks_cfg: A DictConfig object containing callback configurations.
-    :return: A list of instantiated callbacks.
-    """
-    callbacks: List[Callback] = []
-
-    if not callbacks_cfg:
-        log.warning("No callback configs found! Skipping..")
-        return callbacks
-
-    if not isinstance(callbacks_cfg, DictConfig):
-        raise TypeError("Callbacks config must be a DictConfig!")
-
-    for _, cb_conf in callbacks_cfg.items():
-        if isinstance(cb_conf, DictConfig) and "_target_" in cb_conf:
-            log.info(f"Instantiating callback <{cb_conf._target_}>")
-            callbacks.append(hydra.utils.instantiate(cb_conf))
-
-    return callbacks
-
-
-def instantiate_loggers(logger_cfg: DictConfig) -> List[Logger]:
-    """Instantiates loggers from config.
-
-    :param logger_cfg: A DictConfig object containing logger configurations.
-    :return: A list of instantiated loggers.
-    """
-    logger: List[Logger] = []
-
-    if not logger_cfg:
-        log.warning("No logger configs found! Skipping...")
-        return logger
-
-    if not isinstance(logger_cfg, DictConfig):
-        raise TypeError("Logger config must be a DictConfig!")
-
-    for _, lg_conf in logger_cfg.items():
-        if isinstance(lg_conf, DictConfig) and "_target_" in lg_conf:
-            log.info(f"Instantiating logger <{lg_conf._target_}>")
-            logger.append(hydra.utils.instantiate(lg_conf))
-
-    return logger
diff --git a/src/utils/logging_utils.py b/src/utils/logging_utils.py
deleted file mode 100644
index 360abcd..0000000
--- a/src/utils/logging_utils.py
+++ /dev/null
@@ -1,57 +0,0 @@
-from typing import Any, Dict
-
-from lightning_utilities.core.rank_zero import rank_zero_only
-from omegaconf import OmegaConf
-
-from src.utils import pylogger
-
-log = pylogger.RankedLogger(__name__, rank_zero_only=True)
-
-
-@rank_zero_only
-def log_hyperparameters(object_dict: Dict[str, Any]) -> None:
-    """Controls which config parts are saved by Lightning loggers.
-
-    Additionally saves:
-        - Number of model parameters
-
-    :param object_dict: A dictionary containing the following objects:
-        - `"cfg"`: A DictConfig object containing the main config.
-        - `"model"`: The Lightning model.
-        - `"trainer"`: The Lightning trainer.
-    """
-    hparams = {}
-
-    cfg = OmegaConf.to_container(object_dict["cfg"])
-    model = object_dict["model"]
-    trainer = object_dict["trainer"]
-
-    if not trainer.logger:
-        log.warning("Logger not found! Skipping hyperparameter logging...")
-        return
-
-    hparams["model"] = cfg["model"]
-
-    # save number of model parameters
-    hparams["model/params/total"] = sum(p.numel() for p in model.parameters())
-    hparams["model/params/trainable"] = sum(
-        p.numel() for p in model.parameters() if p.requires_grad
-    )
-    hparams["model/params/non_trainable"] = sum(
-        p.numel() for p in model.parameters() if not p.requires_grad
-    )
-
-    hparams["data"] = cfg["data"]
-    hparams["trainer"] = cfg["trainer"]
-
-    hparams["callbacks"] = cfg.get("callbacks")
-    hparams["extras"] = cfg.get("extras")
-
-    hparams["task_name"] = cfg.get("task_name")
-    hparams["tags"] = cfg.get("tags")
-    hparams["ckpt_path"] = cfg.get("ckpt_path")
-    hparams["seed"] = cfg.get("seed")
-
-    # send hparams to all loggers
-    for logger in trainer.loggers:
-        logger.log_hyperparams(hparams)
diff --git a/src/utils/pylogger.py b/src/utils/pylogger.py
deleted file mode 100644
index c4ee867..0000000
--- a/src/utils/pylogger.py
+++ /dev/null
@@ -1,51 +0,0 @@
-import logging
-from typing import Mapping, Optional
-
-from lightning_utilities.core.rank_zero import rank_prefixed_message, rank_zero_only
-
-
-class RankedLogger(logging.LoggerAdapter):
-    """A multi-GPU-friendly python command line logger."""
-
-    def __init__(
-        self,
-        name: str = __name__,
-        rank_zero_only: bool = False,
-        extra: Optional[Mapping[str, object]] = None,
-    ) -> None:
-        """Initializes a multi-GPU-friendly python command line logger that logs on all processes
-        with their rank prefixed in the log message.
-
-        :param name: The name of the logger. Default is ``__name__``.
-        :param rank_zero_only: Whether to force all logs to only occur on the rank zero process. Default is `False`.
-        :param extra: (Optional) A dict-like object which provides contextual information. See `logging.LoggerAdapter`.
-        """
-        logger = logging.getLogger(name)
-        super().__init__(logger=logger, extra=extra)
-        self.rank_zero_only = rank_zero_only
-
-    def log(self, level: int, msg: str, rank: Optional[int] = None, *args, **kwargs) -> None:
-        """Delegate a log call to the underlying logger, after prefixing its message with the rank
-        of the process it's being logged from. If `'rank'` is provided, then the log will only
-        occur on that rank/process.
-
-        :param level: The level to log at. Look at `logging.__init__.py` for more information.
-        :param msg: The message to log.
-        :param rank: The rank to log at.
-        :param args: Additional args to pass to the underlying logging function.
-        :param kwargs: Any additional keyword args to pass to the underlying logging function.
-        """
-        if self.isEnabledFor(level):
-            msg, kwargs = self.process(msg, kwargs)
-            current_rank = getattr(rank_zero_only, "rank", None)
-            if current_rank is None:
-                raise RuntimeError("The `rank_zero_only.rank` needs to be set before use")
-            msg = rank_prefixed_message(msg, current_rank)
-            if self.rank_zero_only:
-                if current_rank == 0:
-                    self.logger.log(level, msg, *args, **kwargs)
-            else:
-                if rank is None:
-                    self.logger.log(level, msg, *args, **kwargs)
-                elif current_rank == rank:
-                    self.logger.log(level, msg, *args, **kwargs)
diff --git a/src/utils/rich_utils.py b/src/utils/rich_utils.py
deleted file mode 100644
index aeec680..0000000
--- a/src/utils/rich_utils.py
+++ /dev/null
@@ -1,99 +0,0 @@
-from pathlib import Path
-from typing import Sequence
-
-import rich
-import rich.syntax
-import rich.tree
-from hydra.core.hydra_config import HydraConfig
-from lightning_utilities.core.rank_zero import rank_zero_only
-from omegaconf import DictConfig, OmegaConf, open_dict
-from rich.prompt import Prompt
-
-from src.utils import pylogger
-
-log = pylogger.RankedLogger(__name__, rank_zero_only=True)
-
-
-@rank_zero_only
-def print_config_tree(
-    cfg: DictConfig,
-    print_order: Sequence[str] = (
-        "data",
-        "model",
-        "callbacks",
-        "logger",
-        "trainer",
-        "paths",
-        "extras",
-    ),
-    resolve: bool = False,
-    save_to_file: bool = False,
-) -> None:
-    """Prints the contents of a DictConfig as a tree structure using the Rich library.
-
-    :param cfg: A DictConfig composed by Hydra.
-    :param print_order: Determines in what order config components are printed. Default is ``("data", "model",
-    "callbacks", "logger", "trainer", "paths", "extras")``.
-    :param resolve: Whether to resolve reference fields of DictConfig. Default is ``False``.
-    :param save_to_file: Whether to export config to the hydra output folder. Default is ``False``.
-    """
-    style = "dim"
-    tree = rich.tree.Tree("CONFIG", style=style, guide_style=style)
-
-    queue = []
-
-    # add fields from `print_order` to queue
-    for field in print_order:
-        queue.append(field) if field in cfg else log.warning(
-            f"Field '{field}' not found in config. Skipping '{field}' config printing..."
-        )
-
-    # add all the other fields to queue (not specified in `print_order`)
-    for field in cfg:
-        if field not in queue:
-            queue.append(field)
-
-    # generate config tree from queue
-    for field in queue:
-        branch = tree.add(field, style=style, guide_style=style)
-
-        config_group = cfg[field]
-        if isinstance(config_group, DictConfig):
-            branch_content = OmegaConf.to_yaml(config_group, resolve=resolve)
-        else:
-            branch_content = str(config_group)
-
-        branch.add(rich.syntax.Syntax(branch_content, "yaml"))
-
-    # print config tree
-    rich.print(tree)
-
-    # save config tree to file
-    if save_to_file:
-        with open(Path(cfg.paths.output_dir, "config_tree.log"), "w") as file:
-            rich.print(tree, file=file)
-
-
-@rank_zero_only
-def enforce_tags(cfg: DictConfig, save_to_file: bool = False) -> None:
-    """Prompts user to input tags from command line if no tags are provided in config.
-
-    :param cfg: A DictConfig composed by Hydra.
-    :param save_to_file: Whether to export tags to the hydra output folder. Default is ``False``.
-    """
-    if not cfg.get("tags"):
-        if "id" in HydraConfig().cfg.hydra.job:
-            raise ValueError("Specify tags before launching a multirun!")
-
-        log.warning("No tags provided in config. Prompting user to input tags...")
-        tags = Prompt.ask("Enter a list of comma separated tags", default="dev")
-        tags = [t.strip() for t in tags.split(",") if t != ""]
-
-        with open_dict(cfg):
-            cfg.tags = tags
-
-        log.info(f"Tags: {cfg.tags}")
-
-    if save_to_file:
-        with open(Path(cfg.paths.output_dir, "tags.log"), "w") as file:
-            rich.print(cfg.tags, file=file)
diff --git a/src/utils/utils.py b/src/utils/utils.py
deleted file mode 100644
index 02b5576..0000000
--- a/src/utils/utils.py
+++ /dev/null
@@ -1,119 +0,0 @@
-import warnings
-from importlib.util import find_spec
-from typing import Any, Callable, Dict, Optional, Tuple
-
-from omegaconf import DictConfig
-
-from src.utils import pylogger, rich_utils
-
-log = pylogger.RankedLogger(__name__, rank_zero_only=True)
-
-
-def extras(cfg: DictConfig) -> None:
-    """Applies optional utilities before the task is started.
-
-    Utilities:
-        - Ignoring python warnings
-        - Setting tags from command line
-        - Rich config printing
-
-    :param cfg: A DictConfig object containing the config tree.
-    """
-    # return if no `extras` config
-    if not cfg.get("extras"):
-        log.warning("Extras config not found! <cfg.extras=null>")
-        return
-
-    # disable python warnings
-    if cfg.extras.get("ignore_warnings"):
-        log.info("Disabling python warnings! <cfg.extras.ignore_warnings=True>")
-        warnings.filterwarnings("ignore")
-
-    # prompt user to input tags from command line if none are provided in the config
-    if cfg.extras.get("enforce_tags"):
-        log.info("Enforcing tags! <cfg.extras.enforce_tags=True>")
-        rich_utils.enforce_tags(cfg, save_to_file=True)
-
-    # pretty print config tree using Rich library
-    if cfg.extras.get("print_config"):
-        log.info("Printing config tree with Rich! <cfg.extras.print_config=True>")
-        rich_utils.print_config_tree(cfg, resolve=True, save_to_file=True)
-
-
-def task_wrapper(task_func: Callable) -> Callable:
-    """Optional decorator that controls the failure behavior when executing the task function.
-
-    This wrapper can be used to:
-        - make sure loggers are closed even if the task function raises an exception (prevents multirun failure)
-        - save the exception to a `.log` file
-        - mark the run as failed with a dedicated file in the `logs/` folder (so we can find and rerun it later)
-        - etc. (adjust depending on your needs)
-
-    Example:
-    ```
-    @utils.task_wrapper
-    def train(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
-        ...
-        return metric_dict, object_dict
-    ```
-
-    :param task_func: The task function to be wrapped.
-
-    :return: The wrapped task function.
-    """
-
-    def wrap(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
-        # execute the task
-        try:
-            metric_dict, object_dict = task_func(cfg=cfg)
-
-        # things to do if exception occurs
-        except Exception as ex:
-            # save exception to `.log` file
-            log.exception("")
-
-            # some hyperparameter combinations might be invalid or cause out-of-memory errors
-            # so when using hparam search plugins like Optuna, you might want to disable
-            # raising the below exception to avoid multirun failure
-            raise ex
-
-        # things to always do after either success or exception
-        finally:
-            # display output dir path in terminal
-            log.info(f"Output dir: {cfg.paths.output_dir}")
-
-            # always close wandb run (even if exception occurs so multirun won't fail)
-            if find_spec("wandb"):  # check if wandb is installed
-                import wandb
-
-                if wandb.run:
-                    log.info("Closing wandb!")
-                    wandb.finish()
-
-        return metric_dict, object_dict
-
-    return wrap
-
-
-def get_metric_value(metric_dict: Dict[str, Any], metric_name: Optional[str]) -> Optional[float]:
-    """Safely retrieves value of the metric logged in LightningModule.
-
-    :param metric_dict: A dict containing metric values.
-    :param metric_name: If provided, the name of the metric to retrieve.
-    :return: If a metric name was provided, the value of the metric.
-    """
-    if not metric_name:
-        log.info("Metric name is None! Skipping metric value retrieval...")
-        return None
-
-    if metric_name not in metric_dict:
-        raise Exception(
-            f"Metric value not found! <metric_name={metric_name}>\n"
-            "Make sure metric name logged in LightningModule is correct!\n"
-            "Make sure `optimized_metric` name in `hparams_search` config is correct!"
-        )
-
-    metric_value = metric_dict[metric_name].item()
-    log.info(f"Retrieved metric value! <{metric_name}={metric_value}>")
-
-    return metric_value
diff --git a/tests/__init__.py b/tests/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/tests/conftest.py b/tests/conftest.py
deleted file mode 100644
index b5dea33..0000000
--- a/tests/conftest.py
+++ /dev/null
@@ -1,107 +0,0 @@
-"""This file prepares config fixtures for other tests."""
-
-from pathlib import Path
-
-import pytest
-import rootutils
-from hydra import compose, initialize
-from hydra.core.global_hydra import GlobalHydra
-from omegaconf import DictConfig, open_dict
-
-
-@pytest.fixture(scope="package")
-def cfg_train_global() -> DictConfig:
-    """A pytest fixture for setting up a default Hydra DictConfig for training.
-
-    :return: A DictConfig object containing a default Hydra configuration for training.
-    """
-    with initialize(version_base="1.3", config_path="../configs"):
-        cfg = compose(config_name="train.yaml", return_hydra_config=True, overrides=[])
-
-        # set defaults for all tests
-        with open_dict(cfg):
-            cfg.paths.root_dir = str(rootutils.find_root(indicator=".project-root"))
-            cfg.trainer.max_epochs = 1
-            cfg.trainer.limit_train_batches = 0.01
-            cfg.trainer.limit_val_batches = 0.1
-            cfg.trainer.limit_test_batches = 0.1
-            cfg.trainer.accelerator = "cpu"
-            cfg.trainer.devices = 1
-            cfg.data.num_workers = 0
-            cfg.data.pin_memory = False
-            cfg.extras.print_config = False
-            cfg.extras.enforce_tags = False
-            cfg.logger = None
-
-    return cfg
-
-
-@pytest.fixture(scope="package")
-def cfg_eval_global() -> DictConfig:
-    """A pytest fixture for setting up a default Hydra DictConfig for evaluation.
-
-    :return: A DictConfig containing a default Hydra configuration for evaluation.
-    """
-    with initialize(version_base="1.3", config_path="../configs"):
-        cfg = compose(config_name="eval.yaml", return_hydra_config=True, overrides=["ckpt_path=."])
-
-        # set defaults for all tests
-        with open_dict(cfg):
-            cfg.paths.root_dir = str(rootutils.find_root(indicator=".project-root"))
-            cfg.trainer.max_epochs = 1
-            cfg.trainer.limit_test_batches = 0.1
-            cfg.trainer.accelerator = "cpu"
-            cfg.trainer.devices = 1
-            cfg.data.num_workers = 0
-            cfg.data.pin_memory = False
-            cfg.extras.print_config = False
-            cfg.extras.enforce_tags = False
-            cfg.logger = None
-
-    return cfg
-
-
-@pytest.fixture(scope="function")
-def cfg_train(cfg_train_global: DictConfig, tmp_path: Path) -> DictConfig:
-    """A pytest fixture built on top of the `cfg_train_global()` fixture, which accepts a temporary
-    logging path `tmp_path` for generating a temporary logging path.
-
-    This is called by each test which uses the `cfg_train` arg. Each test generates its own temporary logging path.
-
-    :param cfg_train_global: The input DictConfig object to be modified.
-    :param tmp_path: The temporary logging path.
-
-    :return: A DictConfig with updated output and log directories corresponding to `tmp_path`.
-    """
-    cfg = cfg_train_global.copy()
-
-    with open_dict(cfg):
-        cfg.paths.output_dir = str(tmp_path)
-        cfg.paths.log_dir = str(tmp_path)
-
-    yield cfg
-
-    GlobalHydra.instance().clear()
-
-
-@pytest.fixture(scope="function")
-def cfg_eval(cfg_eval_global: DictConfig, tmp_path: Path) -> DictConfig:
-    """A pytest fixture built on top of the `cfg_eval_global()` fixture, which accepts a temporary
-    logging path `tmp_path` for generating a temporary logging path.
-
-    This is called by each test which uses the `cfg_eval` arg. Each test generates its own temporary logging path.
-
-    :param cfg_train_global: The input DictConfig object to be modified.
-    :param tmp_path: The temporary logging path.
-
-    :return: A DictConfig with updated output and log directories corresponding to `tmp_path`.
-    """
-    cfg = cfg_eval_global.copy()
-
-    with open_dict(cfg):
-        cfg.paths.output_dir = str(tmp_path)
-        cfg.paths.log_dir = str(tmp_path)
-
-    yield cfg
-
-    GlobalHydra.instance().clear()
diff --git a/tests/helpers/__init__.py b/tests/helpers/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/tests/helpers/package_available.py b/tests/helpers/package_available.py
deleted file mode 100644
index 0afdba8..0000000
--- a/tests/helpers/package_available.py
+++ /dev/null
@@ -1,32 +0,0 @@
-import platform
-
-import pkg_resources
-from lightning.fabric.accelerators import TPUAccelerator
-
-
-def _package_available(package_name: str) -> bool:
-    """Check if a package is available in your environment.
-
-    :param package_name: The name of the package to be checked.
-
-    :return: `True` if the package is available. `False` otherwise.
-    """
-    try:
-        return pkg_resources.require(package_name) is not None
-    except pkg_resources.DistributionNotFound:
-        return False
-
-
-_TPU_AVAILABLE = TPUAccelerator.is_available()
-
-_IS_WINDOWS = platform.system() == "Windows"
-
-_SH_AVAILABLE = not _IS_WINDOWS and _package_available("sh")
-
-_DEEPSPEED_AVAILABLE = not _IS_WINDOWS and _package_available("deepspeed")
-_FAIRSCALE_AVAILABLE = not _IS_WINDOWS and _package_available("fairscale")
-
-_WANDB_AVAILABLE = _package_available("wandb")
-_NEPTUNE_AVAILABLE = _package_available("neptune")
-_COMET_AVAILABLE = _package_available("comet_ml")
-_MLFLOW_AVAILABLE = _package_available("mlflow")
diff --git a/tests/helpers/run_if.py b/tests/helpers/run_if.py
deleted file mode 100644
index 9703af4..0000000
--- a/tests/helpers/run_if.py
+++ /dev/null
@@ -1,142 +0,0 @@
-"""Adapted from:
-
-https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/helpers/runif.py
-"""
-
-import sys
-from typing import Any, Dict, Optional
-
-import pytest
-import torch
-from packaging.version import Version
-from pkg_resources import get_distribution
-from pytest import MarkDecorator
-
-from tests.helpers.package_available import (
-    _COMET_AVAILABLE,
-    _DEEPSPEED_AVAILABLE,
-    _FAIRSCALE_AVAILABLE,
-    _IS_WINDOWS,
-    _MLFLOW_AVAILABLE,
-    _NEPTUNE_AVAILABLE,
-    _SH_AVAILABLE,
-    _TPU_AVAILABLE,
-    _WANDB_AVAILABLE,
-)
-
-
-class RunIf:
-    """RunIf wrapper for conditional skipping of tests.
-
-    Fully compatible with `@pytest.mark`.
-
-    Example:
-
-    ```python
-        @RunIf(min_torch="1.8")
-        @pytest.mark.parametrize("arg1", [1.0, 2.0])
-        def test_wrapper(arg1):
-            assert arg1 > 0
-    ```
-    """
-
-    def __new__(
-        cls,
-        min_gpus: int = 0,
-        min_torch: Optional[str] = None,
-        max_torch: Optional[str] = None,
-        min_python: Optional[str] = None,
-        skip_windows: bool = False,
-        sh: bool = False,
-        tpu: bool = False,
-        fairscale: bool = False,
-        deepspeed: bool = False,
-        wandb: bool = False,
-        neptune: bool = False,
-        comet: bool = False,
-        mlflow: bool = False,
-        **kwargs: Dict[Any, Any],
-    ) -> MarkDecorator:
-        """Creates a new `@RunIf` `MarkDecorator` decorator.
-
-        :param min_gpus: Min number of GPUs required to run test.
-        :param min_torch: Minimum pytorch version to run test.
-        :param max_torch: Maximum pytorch version to run test.
-        :param min_python: Minimum python version required to run test.
-        :param skip_windows: Skip test for Windows platform.
-        :param tpu: If TPU is available.
-        :param sh: If `sh` module is required to run the test.
-        :param fairscale: If `fairscale` module is required to run the test.
-        :param deepspeed: If `deepspeed` module is required to run the test.
-        :param wandb: If `wandb` module is required to run the test.
-        :param neptune: If `neptune` module is required to run the test.
-        :param comet: If `comet` module is required to run the test.
-        :param mlflow: If `mlflow` module is required to run the test.
-        :param kwargs: Native `pytest.mark.skipif` keyword arguments.
-        """
-        conditions = []
-        reasons = []
-
-        if min_gpus:
-            conditions.append(torch.cuda.device_count() < min_gpus)
-            reasons.append(f"GPUs>={min_gpus}")
-
-        if min_torch:
-            torch_version = get_distribution("torch").version
-            conditions.append(Version(torch_version) < Version(min_torch))
-            reasons.append(f"torch>={min_torch}")
-
-        if max_torch:
-            torch_version = get_distribution("torch").version
-            conditions.append(Version(torch_version) >= Version(max_torch))
-            reasons.append(f"torch<{max_torch}")
-
-        if min_python:
-            py_version = (
-                f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
-            )
-            conditions.append(Version(py_version) < Version(min_python))
-            reasons.append(f"python>={min_python}")
-
-        if skip_windows:
-            conditions.append(_IS_WINDOWS)
-            reasons.append("does not run on Windows")
-
-        if tpu:
-            conditions.append(not _TPU_AVAILABLE)
-            reasons.append("TPU")
-
-        if sh:
-            conditions.append(not _SH_AVAILABLE)
-            reasons.append("sh")
-
-        if fairscale:
-            conditions.append(not _FAIRSCALE_AVAILABLE)
-            reasons.append("fairscale")
-
-        if deepspeed:
-            conditions.append(not _DEEPSPEED_AVAILABLE)
-            reasons.append("deepspeed")
-
-        if wandb:
-            conditions.append(not _WANDB_AVAILABLE)
-            reasons.append("wandb")
-
-        if neptune:
-            conditions.append(not _NEPTUNE_AVAILABLE)
-            reasons.append("neptune")
-
-        if comet:
-            conditions.append(not _COMET_AVAILABLE)
-            reasons.append("comet")
-
-        if mlflow:
-            conditions.append(not _MLFLOW_AVAILABLE)
-            reasons.append("mlflow")
-
-        reasons = [rs for cond, rs in zip(conditions, reasons) if cond]
-        return pytest.mark.skipif(
-            condition=any(conditions),
-            reason=f"Requires: [{' + '.join(reasons)}]",
-            **kwargs,
-        )
diff --git a/tests/helpers/run_sh_command.py b/tests/helpers/run_sh_command.py
deleted file mode 100644
index fdd2ed6..0000000
--- a/tests/helpers/run_sh_command.py
+++ /dev/null
@@ -1,22 +0,0 @@
-from typing import List
-
-import pytest
-
-from tests.helpers.package_available import _SH_AVAILABLE
-
-if _SH_AVAILABLE:
-    import sh
-
-
-def run_sh_command(command: List[str]) -> None:
-    """Default method for executing shell commands with `pytest` and `sh` package.
-
-    :param command: A list of shell commands as strings.
-    """
-    msg = None
-    try:
-        sh.python(command)
-    except sh.ErrorReturnCode as e:
-        msg = e.stderr.decode()
-    if msg:
-        pytest.fail(msg=msg)
diff --git a/tests/test_configs.py b/tests/test_configs.py
deleted file mode 100644
index d7041dc..0000000
--- a/tests/test_configs.py
+++ /dev/null
@@ -1,37 +0,0 @@
-import hydra
-from hydra.core.hydra_config import HydraConfig
-from omegaconf import DictConfig
-
-
-def test_train_config(cfg_train: DictConfig) -> None:
-    """Tests the training configuration provided by the `cfg_train` pytest fixture.
-
-    :param cfg_train: A DictConfig containing a valid training configuration.
-    """
-    assert cfg_train
-    assert cfg_train.data
-    assert cfg_train.model
-    assert cfg_train.trainer
-
-    HydraConfig().set_config(cfg_train)
-
-    hydra.utils.instantiate(cfg_train.data)
-    hydra.utils.instantiate(cfg_train.model)
-    hydra.utils.instantiate(cfg_train.trainer)
-
-
-def test_eval_config(cfg_eval: DictConfig) -> None:
-    """Tests the evaluation configuration provided by the `cfg_eval` pytest fixture.
-
-    :param cfg_train: A DictConfig containing a valid evaluation configuration.
-    """
-    assert cfg_eval
-    assert cfg_eval.data
-    assert cfg_eval.model
-    assert cfg_eval.trainer
-
-    HydraConfig().set_config(cfg_eval)
-
-    hydra.utils.instantiate(cfg_eval.data)
-    hydra.utils.instantiate(cfg_eval.model)
-    hydra.utils.instantiate(cfg_eval.trainer)
diff --git a/tests/test_datamodules.py b/tests/test_datamodules.py
deleted file mode 100644
index 901f3d6..0000000
--- a/tests/test_datamodules.py
+++ /dev/null
@@ -1,38 +0,0 @@
-from pathlib import Path
-
-import pytest
-import torch
-
-from src.data.mnist_datamodule import MNISTDataModule
-
-
-@pytest.mark.parametrize("batch_size", [32, 128])
-def test_mnist_datamodule(batch_size: int) -> None:
-    """Tests `MNISTDataModule` to verify that it can be downloaded correctly, that the necessary
-    attributes were created (e.g., the dataloader objects), and that dtypes and batch sizes
-    correctly match.
-
-    :param batch_size: Batch size of the data to be loaded by the dataloader.
-    """
-    data_dir = "data/"
-
-    dm = MNISTDataModule(data_dir=data_dir, batch_size=batch_size)
-    dm.prepare_data()
-
-    assert not dm.data_train and not dm.data_val and not dm.data_test
-    assert Path(data_dir, "MNIST").exists()
-    assert Path(data_dir, "MNIST", "raw").exists()
-
-    dm.setup()
-    assert dm.data_train and dm.data_val and dm.data_test
-    assert dm.train_dataloader() and dm.val_dataloader() and dm.test_dataloader()
-
-    num_datapoints = len(dm.data_train) + len(dm.data_val) + len(dm.data_test)
-    assert num_datapoints == 70_000
-
-    batch = next(iter(dm.train_dataloader()))
-    x, y = batch
-    assert len(x) == batch_size
-    assert len(y) == batch_size
-    assert x.dtype == torch.float32
-    assert y.dtype == torch.int64
diff --git a/tests/test_eval.py b/tests/test_eval.py
deleted file mode 100644
index 423c9d2..0000000
--- a/tests/test_eval.py
+++ /dev/null
@@ -1,39 +0,0 @@
-import os
-from pathlib import Path
-
-import pytest
-from hydra.core.hydra_config import HydraConfig
-from omegaconf import DictConfig, open_dict
-
-from src.eval import evaluate
-from src.train import train
-
-
-@pytest.mark.slow
-def test_train_eval(tmp_path: Path, cfg_train: DictConfig, cfg_eval: DictConfig) -> None:
-    """Tests training and evaluation by training for 1 epoch with `train.py` then evaluating with
-    `eval.py`.
-
-    :param tmp_path: The temporary logging path.
-    :param cfg_train: A DictConfig containing a valid training configuration.
-    :param cfg_eval: A DictConfig containing a valid evaluation configuration.
-    """
-    assert str(tmp_path) == cfg_train.paths.output_dir == cfg_eval.paths.output_dir
-
-    with open_dict(cfg_train):
-        cfg_train.trainer.max_epochs = 1
-        cfg_train.test = True
-
-    HydraConfig().set_config(cfg_train)
-    train_metric_dict, _ = train(cfg_train)
-
-    assert "last.ckpt" in os.listdir(tmp_path / "checkpoints")
-
-    with open_dict(cfg_eval):
-        cfg_eval.ckpt_path = str(tmp_path / "checkpoints" / "last.ckpt")
-
-    HydraConfig().set_config(cfg_eval)
-    test_metric_dict, _ = evaluate(cfg_eval)
-
-    assert test_metric_dict["test/acc"] > 0.0
-    assert abs(train_metric_dict["test/acc"].item() - test_metric_dict["test/acc"].item()) < 0.001
diff --git a/tests/test_sweeps.py b/tests/test_sweeps.py
deleted file mode 100644
index 7856b15..0000000
--- a/tests/test_sweeps.py
+++ /dev/null
@@ -1,107 +0,0 @@
-from pathlib import Path
-
-import pytest
-
-from tests.helpers.run_if import RunIf
-from tests.helpers.run_sh_command import run_sh_command
-
-startfile = "src/train.py"
-overrides = ["logger=[]"]
-
-
-@RunIf(sh=True)
-@pytest.mark.slow
-def test_experiments(tmp_path: Path) -> None:
-    """Test running all available experiment configs with `fast_dev_run=True.`
-
-    :param tmp_path: The temporary logging path.
-    """
-    command = [
-        startfile,
-        "-m",
-        "experiment=glob(*)",
-        "hydra.sweep.dir=" + str(tmp_path),
-        "++trainer.fast_dev_run=true",
-    ] + overrides
-    run_sh_command(command)
-
-
-@RunIf(sh=True)
-@pytest.mark.slow
-def test_hydra_sweep(tmp_path: Path) -> None:
-    """Test default hydra sweep.
-
-    :param tmp_path: The temporary logging path.
-    """
-    command = [
-        startfile,
-        "-m",
-        "hydra.sweep.dir=" + str(tmp_path),
-        "model.optimizer.lr=0.005,0.01",
-        "++trainer.fast_dev_run=true",
-    ] + overrides
-
-    run_sh_command(command)
-
-
-@RunIf(sh=True)
-@pytest.mark.slow
-def test_hydra_sweep_ddp_sim(tmp_path: Path) -> None:
-    """Test default hydra sweep with ddp sim.
-
-    :param tmp_path: The temporary logging path.
-    """
-    command = [
-        startfile,
-        "-m",
-        "hydra.sweep.dir=" + str(tmp_path),
-        "trainer=ddp_sim",
-        "trainer.max_epochs=3",
-        "+trainer.limit_train_batches=0.01",
-        "+trainer.limit_val_batches=0.1",
-        "+trainer.limit_test_batches=0.1",
-        "model.optimizer.lr=0.005,0.01,0.02",
-    ] + overrides
-    run_sh_command(command)
-
-
-@RunIf(sh=True)
-@pytest.mark.slow
-def test_optuna_sweep(tmp_path: Path) -> None:
-    """Test Optuna hyperparam sweeping.
-
-    :param tmp_path: The temporary logging path.
-    """
-    command = [
-        startfile,
-        "-m",
-        "hparams_search=mnist_optuna",
-        "hydra.sweep.dir=" + str(tmp_path),
-        "hydra.sweeper.n_trials=10",
-        "hydra.sweeper.sampler.n_startup_trials=5",
-        "++trainer.fast_dev_run=true",
-    ] + overrides
-    run_sh_command(command)
-
-
-@RunIf(wandb=True, sh=True)
-@pytest.mark.slow
-def test_optuna_sweep_ddp_sim_wandb(tmp_path: Path) -> None:
-    """Test Optuna sweep with wandb logging and ddp sim.
-
-    :param tmp_path: The temporary logging path.
-    """
-    command = [
-        startfile,
-        "-m",
-        "hparams_search=mnist_optuna",
-        "hydra.sweep.dir=" + str(tmp_path),
-        "hydra.sweeper.n_trials=5",
-        "trainer=ddp_sim",
-        "trainer.max_epochs=3",
-        "+trainer.limit_train_batches=0.01",
-        "+trainer.limit_val_batches=0.1",
-        "+trainer.limit_test_batches=0.1",
-        "logger=wandb",
-    ]
-    run_sh_command(command)
diff --git a/tests/test_train.py b/tests/test_train.py
deleted file mode 100644
index c13ae02..0000000
--- a/tests/test_train.py
+++ /dev/null
@@ -1,108 +0,0 @@
-import os
-from pathlib import Path
-
-import pytest
-from hydra.core.hydra_config import HydraConfig
-from omegaconf import DictConfig, open_dict
-
-from src.train import train
-from tests.helpers.run_if import RunIf
-
-
-def test_train_fast_dev_run(cfg_train: DictConfig) -> None:
-    """Run for 1 train, val and test step.
-
-    :param cfg_train: A DictConfig containing a valid training configuration.
-    """
-    HydraConfig().set_config(cfg_train)
-    with open_dict(cfg_train):
-        cfg_train.trainer.fast_dev_run = True
-        cfg_train.trainer.accelerator = "cpu"
-    train(cfg_train)
-
-
-@RunIf(min_gpus=1)
-def test_train_fast_dev_run_gpu(cfg_train: DictConfig) -> None:
-    """Run for 1 train, val and test step on GPU.
-
-    :param cfg_train: A DictConfig containing a valid training configuration.
-    """
-    HydraConfig().set_config(cfg_train)
-    with open_dict(cfg_train):
-        cfg_train.trainer.fast_dev_run = True
-        cfg_train.trainer.accelerator = "gpu"
-    train(cfg_train)
-
-
-@RunIf(min_gpus=1)
-@pytest.mark.slow
-def test_train_epoch_gpu_amp(cfg_train: DictConfig) -> None:
-    """Train 1 epoch on GPU with mixed-precision.
-
-    :param cfg_train: A DictConfig containing a valid training configuration.
-    """
-    HydraConfig().set_config(cfg_train)
-    with open_dict(cfg_train):
-        cfg_train.trainer.max_epochs = 1
-        cfg_train.trainer.accelerator = "gpu"
-        cfg_train.trainer.precision = 16
-    train(cfg_train)
-
-
-@pytest.mark.slow
-def test_train_epoch_double_val_loop(cfg_train: DictConfig) -> None:
-    """Train 1 epoch with validation loop twice per epoch.
-
-    :param cfg_train: A DictConfig containing a valid training configuration.
-    """
-    HydraConfig().set_config(cfg_train)
-    with open_dict(cfg_train):
-        cfg_train.trainer.max_epochs = 1
-        cfg_train.trainer.val_check_interval = 0.5
-    train(cfg_train)
-
-
-@pytest.mark.slow
-def test_train_ddp_sim(cfg_train: DictConfig) -> None:
-    """Simulate DDP (Distributed Data Parallel) on 2 CPU processes.
-
-    :param cfg_train: A DictConfig containing a valid training configuration.
-    """
-    HydraConfig().set_config(cfg_train)
-    with open_dict(cfg_train):
-        cfg_train.trainer.max_epochs = 2
-        cfg_train.trainer.accelerator = "cpu"
-        cfg_train.trainer.devices = 2
-        cfg_train.trainer.strategy = "ddp_spawn"
-    train(cfg_train)
-
-
-@pytest.mark.slow
-def test_train_resume(tmp_path: Path, cfg_train: DictConfig) -> None:
-    """Run 1 epoch, finish, and resume for another epoch.
-
-    :param tmp_path: The temporary logging path.
-    :param cfg_train: A DictConfig containing a valid training configuration.
-    """
-    with open_dict(cfg_train):
-        cfg_train.trainer.max_epochs = 1
-
-    HydraConfig().set_config(cfg_train)
-    metric_dict_1, _ = train(cfg_train)
-
-    files = os.listdir(tmp_path / "checkpoints")
-    assert "last.ckpt" in files
-    assert "epoch_000.ckpt" in files
-
-    with open_dict(cfg_train):
-        cfg_train.ckpt_path = str(tmp_path / "checkpoints" / "last.ckpt")
-        cfg_train.trainer.max_epochs = 2
-
-    metric_dict_2, _ = train(cfg_train)
-
-    files = os.listdir(tmp_path / "checkpoints")
-    assert "epoch_001.ckpt" in files
-    assert "epoch_002.ckpt" not in files
-
-    assert metric_dict_1["train/acc"] < metric_dict_2["train/acc"]
-    assert metric_dict_1["val/acc"] < metric_dict_2["val/acc"]
