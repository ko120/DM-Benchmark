[2024-09-03 13:24:17,672][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-03 13:24:17,673][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-03 13:24:18,228][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-03 13:24:18,314][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-03 13:24:18,316][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-03 13:24:18,317][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-03 13:24:18,318][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-03 13:24:18,319][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-03 13:24:18,324][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-03 13:24:18,367][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-03 13:24:18,377][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-03 13:24:18,378][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-03 13:24:18,378][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-03 13:24:18,379][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-03 13:24:21,050][src.training_pipeline][INFO] - Starting training!
[2024-09-03 13:24:21,119][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-03 13:24:21,122][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 13:27:38,852][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-03 13:27:38,866][src.training_pipeline][INFO] - Starting testing!
[2024-09-03 13:27:38,867][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004.ckpt
[2024-09-03 13:27:38,873][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 13:27:38,877][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004.ckpt
[2024-09-03 13:27:42,322][src.training_pipeline][INFO] - Finalizing!
[2024-09-03 13:27:48,537][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004.ckpt
[2024-09-03 13:32:48,977][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-03 13:32:48,980][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-03 13:32:48,981][matplotlib][DEBUG] - interactive is False
[2024-09-03 13:32:48,981][matplotlib][DEBUG] - platform is linux
[2024-09-03 13:32:48,992][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-03 13:32:49,003][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-03 13:32:49,424][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-03 13:32:49,427][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-03 13:32:49,433][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-03 13:32:49,433][wandb.docker.auth][DEBUG] - No config file found
[2024-09-03 13:32:49,516][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-03 13:32:49,517][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-03 13:32:50,168][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-03 13:32:50,252][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-03 13:32:50,254][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-03 13:32:50,255][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-03 13:32:50,255][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-03 13:32:50,256][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-03 13:32:50,260][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-03 13:32:50,301][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-03 13:32:50,312][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-03 13:32:50,313][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-03 13:32:50,313][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-03 13:32:50,313][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-03 13:32:50,382][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-03 13:32:50,385][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-03 13:32:51,013][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-03 13:32:51,070][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-03 13:32:51,241][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-03 13:32:51,261][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-03 13:32:54,187][src.training_pipeline][INFO] - Starting training!
[2024-09-03 13:32:54,251][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-03 13:32:54,252][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 13:33:31,320][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-03 13:33:31,387][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-03 13:33:40,060][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-03 13:33:40,061][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-03 13:33:40,621][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-03 13:33:40,707][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-03 13:33:40,708][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-03 13:33:40,709][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-03 13:33:40,710][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-03 13:33:40,711][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-03 13:33:40,716][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-03 13:33:40,766][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-03 13:33:40,776][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-03 13:33:40,777][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-03 13:33:40,777][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-03 13:33:40,777][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-03 13:33:43,435][src.training_pipeline][INFO] - Starting training!
[2024-09-03 13:33:43,495][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-03 13:33:43,497][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 13:56:52,307][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-03 13:56:52,320][src.training_pipeline][INFO] - Starting testing!
[2024-09-03 13:56:52,321][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v1.ckpt
[2024-09-03 13:56:52,327][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 13:56:52,331][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v1.ckpt
[2024-09-03 13:56:55,780][src.training_pipeline][INFO] - Finalizing!
[2024-09-03 13:57:00,715][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v1.ckpt
[2024-09-03 13:59:21,040][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-03 13:59:21,041][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-03 13:59:21,592][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-03 13:59:21,675][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-03 13:59:21,677][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-03 13:59:21,678][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-03 13:59:21,679][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-03 13:59:21,680][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-03 13:59:21,685][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-03 13:59:21,725][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-03 13:59:21,735][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-03 13:59:21,736][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-03 13:59:21,737][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-03 13:59:21,737][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-03 13:59:24,634][src.training_pipeline][INFO] - Starting training!
[2024-09-03 13:59:24,683][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-03 13:59:24,685][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 13:59:46,902][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-03 13:59:46,904][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-03 13:59:47,456][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-03 13:59:47,540][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-03 13:59:47,542][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-03 13:59:47,543][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-03 13:59:47,544][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-03 13:59:47,544][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-03 13:59:47,549][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-03 13:59:47,593][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-03 13:59:47,599][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-03 13:59:47,600][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-03 13:59:47,600][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-03 13:59:47,600][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-03 13:59:50,282][src.training_pipeline][INFO] - Starting training!
[2024-09-03 13:59:50,350][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-03 13:59:50,351][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 14:00:39,280][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-03 14:00:39,282][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-03 14:00:39,836][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-03 14:00:39,918][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-03 14:00:39,920][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-03 14:00:39,921][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-03 14:00:39,922][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-03 14:00:39,923][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-03 14:00:39,927][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-03 14:00:39,970][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-03 14:00:39,980][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-03 14:00:39,981][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-03 14:00:39,981][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-03 14:00:39,982][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-03 14:00:42,732][src.training_pipeline][INFO] - Starting training!
[2024-09-03 14:00:42,798][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-03 14:00:42,800][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 14:01:06,118][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-09-03 14:02:23,390][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-03 14:02:23,392][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-03 14:02:23,944][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-03 14:02:24,028][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-03 14:02:24,030][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-03 14:02:24,031][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-03 14:02:24,032][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-03 14:02:24,033][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-03 14:02:24,037][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-03 14:02:24,082][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-03 14:02:24,092][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-03 14:02:24,093][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-03 14:02:24,093][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-03 14:02:24,093][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-03 14:02:26,915][src.training_pipeline][INFO] - Starting training!
[2024-09-03 14:02:26,965][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-03 14:02:26,966][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 14:06:14,158][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-03 14:06:14,171][src.training_pipeline][INFO] - Starting testing!
[2024-09-03 14:06:14,173][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v2.ckpt
[2024-09-03 14:06:14,178][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-03 14:06:14,183][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v2.ckpt
[2024-09-03 14:06:17,680][src.training_pipeline][INFO] - Finalizing!
[2024-09-03 14:06:24,890][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v2.ckpt
[2024-09-07 16:28:44,895][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-07 16:28:44,931][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-07 16:28:44,934][matplotlib][DEBUG] - interactive is False
[2024-09-07 16:28:44,934][matplotlib][DEBUG] - platform is linux
[2024-09-07 16:28:45,089][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-07 16:28:45,133][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-07 16:28:47,948][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:28:47,951][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:28:48,030][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-07 16:28:48,030][wandb.docker.auth][DEBUG] - No config file found
[2024-09-07 16:28:48,845][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:28:48,880][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:28:51,460][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:28:52,734][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:28:52,736][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:28:52,737][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:28:52,737][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:28:52,738][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:28:52,742][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:28:52,795][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:28:52,819][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:28:52,820][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:28:52,820][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:28:52,821][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:28:52,877][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:28:52,881][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:28:53,597][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-07 16:28:53,643][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:28:53,677][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:28:53,714][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-07 16:29:00,345][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:29:00,377][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:29:00,379][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:29:08,539][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-07 16:29:08,699][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-07 16:31:50,129][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-07 16:31:50,132][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-07 16:31:50,133][matplotlib][DEBUG] - interactive is False
[2024-09-07 16:31:50,133][matplotlib][DEBUG] - platform is linux
[2024-09-07 16:31:50,144][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-07 16:31:50,155][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-07 16:31:50,576][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:31:50,579][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:31:50,585][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-07 16:31:50,585][wandb.docker.auth][DEBUG] - No config file found
[2024-09-07 16:31:50,668][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:31:50,669][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:31:51,324][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:31:51,410][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:31:51,412][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:31:51,413][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:31:51,414][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:31:51,414][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:31:51,419][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:31:51,440][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:31:51,450][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:31:51,451][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:31:51,451][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:31:51,452][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:31:51,492][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:31:51,494][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:31:51,922][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-07 16:31:51,977][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:31:52,003][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:31:52,021][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-07 16:31:54,070][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:31:54,108][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:31:54,110][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:31:59,601][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000.ckpt
[2024-09-07 16:31:59,605][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v4.ckpt
[2024-09-07 16:31:59,606][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-07 16:31:59,615][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 16:31:59,616][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000.ckpt
[2024-09-07 16:31:59,616][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000.ckpt
[2024-09-07 16:31:59,620][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:31:59,622][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000.ckpt
[2024-09-07 16:32:05,177][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-07 16:32:05,256][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-07 16:32:40,764][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-07 16:32:40,768][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-07 16:32:40,768][matplotlib][DEBUG] - interactive is False
[2024-09-07 16:32:40,769][matplotlib][DEBUG] - platform is linux
[2024-09-07 16:32:40,780][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-07 16:32:40,790][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-07 16:32:41,210][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:32:41,213][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:32:41,219][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-07 16:32:41,219][wandb.docker.auth][DEBUG] - No config file found
[2024-09-07 16:32:41,320][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:32:41,322][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:32:41,977][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:32:42,064][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:32:42,066][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:32:42,067][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:32:42,067][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:32:42,068][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:32:42,073][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:32:42,113][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:32:42,123][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:32:42,124][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:32:42,124][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:32:42,125][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:32:42,167][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:32:42,171][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:32:42,593][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-07 16:32:42,634][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:32:42,661][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:32:42,680][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-07 16:32:44,707][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:32:44,748][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:32:44,750][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:32:49,964][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v1.ckpt
[2024-09-07 16:32:49,968][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v5.ckpt
[2024-09-07 16:32:49,969][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-07 16:32:49,978][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 16:32:49,979][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v1.ckpt
[2024-09-07 16:32:49,979][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v1.ckpt
[2024-09-07 16:32:49,983][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:32:49,985][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v1.ckpt
[2024-09-07 16:32:56,862][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-07 16:32:56,912][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-07 16:33:15,833][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-07 16:33:15,836][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-07 16:33:15,837][matplotlib][DEBUG] - interactive is False
[2024-09-07 16:33:15,837][matplotlib][DEBUG] - platform is linux
[2024-09-07 16:33:15,847][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-07 16:33:15,858][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-07 16:33:16,278][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:33:16,281][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:33:16,287][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-07 16:33:16,287][wandb.docker.auth][DEBUG] - No config file found
[2024-09-07 16:33:16,374][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:33:16,375][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:33:17,029][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:33:17,118][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:33:17,120][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:33:17,121][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:33:17,121][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:33:17,122][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:33:17,127][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:33:17,178][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:33:17,188][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:33:17,189][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:33:17,189][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:33:17,190][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:33:17,231][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:33:17,235][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:33:17,670][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-07 16:33:17,714][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:33:17,743][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:33:17,761][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-07 16:33:19,817][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:33:19,855][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:33:19,857][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:33:24,990][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v2.ckpt
[2024-09-07 16:33:24,993][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v6.ckpt
[2024-09-07 16:33:24,995][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-07 16:33:25,004][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 16:33:25,005][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v2.ckpt
[2024-09-07 16:33:25,005][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v2.ckpt
[2024-09-07 16:33:25,009][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:33:25,011][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v2.ckpt
[2024-09-07 16:33:43,938][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-07 16:33:43,984][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-07 16:33:48,028][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-07 16:33:48,032][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-07 16:33:48,032][matplotlib][DEBUG] - interactive is False
[2024-09-07 16:33:48,033][matplotlib][DEBUG] - platform is linux
[2024-09-07 16:33:48,043][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-07 16:33:48,054][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-07 16:33:48,475][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:33:48,478][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:33:48,494][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-07 16:33:48,494][wandb.docker.auth][DEBUG] - No config file found
[2024-09-07 16:33:48,607][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:33:48,608][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:33:49,269][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:33:49,362][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:33:49,363][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:33:49,363][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:33:49,364][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:33:49,364][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:33:49,367][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:33:49,388][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:33:49,396][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:33:49,396][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:33:49,397][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:33:49,397][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:33:49,439][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:33:49,443][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:33:50,078][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-07 16:33:50,127][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:33:50,158][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:33:50,178][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-07 16:33:52,297][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:33:52,335][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:33:52,338][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:33:54,359][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-09-07 16:33:57,972][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-07 16:33:58,022][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-07 16:34:03,932][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-07 16:34:03,936][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-07 16:34:03,937][matplotlib][DEBUG] - interactive is False
[2024-09-07 16:34:03,937][matplotlib][DEBUG] - platform is linux
[2024-09-07 16:34:03,948][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-07 16:34:03,959][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-07 16:34:04,390][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:34:04,392][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:34:04,406][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-07 16:34:04,407][wandb.docker.auth][DEBUG] - No config file found
[2024-09-07 16:34:04,518][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:34:04,519][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:34:05,176][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:34:05,266][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:34:05,268][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:34:05,269][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:34:05,269][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:34:05,270][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:34:05,275][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:34:05,326][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:34:05,336][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:34:05,337][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:34:05,337][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:34:05,338][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:34:05,379][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:34:05,382][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:34:05,812][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-07 16:34:05,854][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:34:05,881][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:34:05,899][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-07 16:34:07,856][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:34:07,894][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:34:07,896][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:34:13,421][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v3.ckpt
[2024-09-07 16:34:13,425][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v7.ckpt
[2024-09-07 16:34:13,426][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-07 16:34:13,435][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 16:34:13,436][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v3.ckpt
[2024-09-07 16:34:13,436][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v3.ckpt
[2024-09-07 16:34:13,440][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:34:13,442][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v3.ckpt
[2024-09-07 16:34:19,015][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-07 16:34:19,070][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-07 16:34:47,677][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-07 16:34:47,680][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-07 16:34:47,681][matplotlib][DEBUG] - interactive is False
[2024-09-07 16:34:47,681][matplotlib][DEBUG] - platform is linux
[2024-09-07 16:34:47,692][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-07 16:34:47,703][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-07 16:34:48,121][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:34:48,124][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:34:48,139][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-07 16:34:48,139][wandb.docker.auth][DEBUG] - No config file found
[2024-09-07 16:34:48,265][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:34:48,267][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:34:48,920][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:34:49,013][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:34:49,015][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:34:49,016][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:34:49,016][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:34:49,017][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:34:49,022][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:34:49,074][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:34:49,084][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:34:49,084][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:34:49,085][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:34:49,085][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:34:49,124][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:34:49,127][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:34:49,557][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-07 16:34:49,598][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:34:49,630][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:34:49,648][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-07 16:34:51,714][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:34:51,752][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:34:51,755][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:34:56,788][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v4.ckpt
[2024-09-07 16:34:56,791][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v8.ckpt
[2024-09-07 16:34:56,793][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-07 16:34:56,801][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 16:34:56,802][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v4.ckpt
[2024-09-07 16:34:56,803][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v4.ckpt
[2024-09-07 16:34:56,807][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:34:56,809][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v4.ckpt
[2024-09-07 16:35:00,223][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 16:35:06,614][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-07 16:35:06,667][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-07 16:35:06,668][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v4.ckpt
[2024-09-07 16:36:13,552][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-07 16:36:13,555][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-07 16:36:13,556][matplotlib][DEBUG] - interactive is False
[2024-09-07 16:36:13,556][matplotlib][DEBUG] - platform is linux
[2024-09-07 16:36:13,567][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-07 16:36:13,578][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-07 16:36:14,000][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:36:14,003][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:36:14,009][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-07 16:36:14,009][wandb.docker.auth][DEBUG] - No config file found
[2024-09-07 16:36:14,092][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:36:14,093][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:36:14,756][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:36:14,846][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:36:14,848][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:36:14,849][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:36:14,850][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:36:14,850][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:36:14,855][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:36:14,905][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:36:14,915][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:36:14,917][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:36:14,917][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:36:14,917][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:36:14,959][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:36:14,962][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-07 16:36:15,398][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-07 16:36:15,451][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:36:15,480][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-07 16:36:15,497][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-07 16:36:17,417][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:36:17,458][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:36:17,460][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:43:08,606][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-07 16:43:08,672][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-07 16:44:58,224][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:44:58,225][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:44:58,781][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:44:58,867][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:44:58,869][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:44:58,870][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:44:58,871][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:44:58,872][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:44:58,877][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:44:58,922][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:44:58,932][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:44:58,933][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:44:58,934][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:44:58,934][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:45:02,879][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:45:02,918][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:45:02,920][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:45:22,697][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-09-07 16:46:35,662][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 16:46:35,664][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 16:46:36,220][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 16:46:36,308][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 16:46:36,310][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 16:46:36,311][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 16:46:36,312][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 16:46:36,312][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 16:46:36,317][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 16:46:36,365][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 16:46:36,376][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 16:46:36,377][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 16:46:36,377][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 16:46:36,378][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 16:46:38,956][src.training_pipeline][INFO] - Starting training!
[2024-09-07 16:46:39,000][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 16:46:39,002][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:49:11,103][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 16:49:11,116][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 16:49:11,117][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005.ckpt
[2024-09-07 16:49:11,121][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 16:49:11,126][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005.ckpt
[2024-09-07 16:49:14,673][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 16:49:20,565][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005.ckpt
[2024-09-07 20:53:40,474][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 20:53:40,475][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 20:53:41,028][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 20:53:41,118][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 20:53:41,119][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 20:53:41,120][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 20:53:41,121][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 20:53:41,122][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 20:53:41,126][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 20:53:41,178][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 20:53:41,188][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 20:53:41,189][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 20:53:41,189][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 20:53:41,189][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 20:53:44,013][src.training_pipeline][INFO] - Starting training!
[2024-09-07 20:53:44,043][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 20:53:44,044][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 20:56:14,812][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 20:56:14,825][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 20:56:14,826][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v1.ckpt
[2024-09-07 20:56:14,831][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 20:56:14,836][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v1.ckpt
[2024-09-07 20:56:18,387][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 20:56:25,911][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v1.ckpt
[2024-09-07 21:09:00,679][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:09:00,680][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:09:01,233][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:09:01,320][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:09:01,321][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:09:01,322][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:09:01,323][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:09:01,324][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:09:01,328][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:09:01,349][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:09:01,359][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:09:01,360][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:09:01,360][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:09:01,360][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:09:04,151][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:09:04,191][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 21:09:04,193][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:09:16,919][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-09-07 21:13:41,579][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:13:41,580][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:13:42,133][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:13:42,221][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:13:42,223][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:13:42,224][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:13:42,225][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:13:42,225][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:13:42,230][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:13:42,251][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:13:42,261][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:13:42,262][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:13:42,262][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:13:42,263][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:13:44,945][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:13:44,977][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 21:13:44,979][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:16:18,970][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 21:16:18,983][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 21:16:18,984][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v2.ckpt
[2024-09-07 21:16:18,988][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:16:18,993][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v2.ckpt
[2024-09-07 21:16:22,570][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 21:16:28,096][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v2.ckpt
[2024-09-07 21:39:21,892][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:39:21,893][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:39:22,328][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:39:22,355][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:39:22,356][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:39:22,357][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:39:22,357][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:39:22,358][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:39:22,361][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:39:22,382][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:39:22,392][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:39:22,394][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:39:22,394][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:39:22,394][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:39:25,045][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:39:25,083][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 21:39:25,085][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:43:01,341][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 21:43:01,354][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 21:43:01,355][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v3.ckpt
[2024-09-07 21:43:01,364][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:43:01,374][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v3.ckpt
[2024-09-07 21:43:04,771][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 21:43:11,037][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v3.ckpt
[2024-09-07 21:43:11,211][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:43:11,212][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:43:11,477][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:43:11,488][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:43:11,489][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:43:11,489][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:43:11,490][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:43:11,490][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:43:11,492][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:43:11,495][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:43:11,498][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:43:11,498][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:43:11,498][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:43:11,498][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:43:13,929][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:43:13,931][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:44:41,395][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-09-07 21:45:59,688][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:45:59,690][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:46:00,120][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:46:00,147][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:46:00,149][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:46:00,149][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:46:00,150][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:46:00,150][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:46:00,153][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:46:00,175][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:46:00,185][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:46:00,186][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:46:00,186][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:46:00,187][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:46:02,843][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:46:02,880][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-07 21:46:02,882][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:48:37,641][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 21:48:37,654][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 21:48:37,655][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v4.ckpt
[2024-09-07 21:48:37,659][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:48:37,664][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v4.ckpt
[2024-09-07 21:48:41,063][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 21:48:46,256][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v4.ckpt
[2024-09-07 21:48:46,432][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:48:46,432][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:48:46,696][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:48:46,706][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:48:46,707][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:48:46,708][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:48:46,708][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:48:46,709][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:48:46,711][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:48:46,713][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:48:46,716][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:48:46,716][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:48:46,716][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:48:46,717][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:48:49,203][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:48:49,205][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:51:22,036][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 21:51:22,049][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 21:51:22,050][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_002-v2.ckpt
[2024-09-07 21:51:22,059][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:51:22,071][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_002-v2.ckpt
[2024-09-07 21:51:25,534][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 21:51:30,521][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_002-v2.ckpt
[2024-09-07 21:51:30,736][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:51:30,736][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:51:31,001][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:51:31,012][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:51:31,013][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:51:31,014][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:51:31,014][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:51:31,015][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:51:31,017][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:51:31,019][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:51:31,022][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:51:31,022][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:51:31,022][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:51:31,023][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:51:33,469][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:51:33,473][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:54:07,068][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 21:54:07,081][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 21:54:07,082][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v3.ckpt
[2024-09-07 21:54:07,086][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:54:07,092][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v3.ckpt
[2024-09-07 21:54:10,548][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 21:54:17,270][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v3.ckpt
[2024-09-07 21:54:17,446][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:54:17,447][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:54:17,712][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:54:17,723][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:54:17,724][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:54:17,724][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:54:17,725][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:54:17,725][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:54:17,727][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:54:17,730][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:54:17,732][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:54:17,733][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:54:17,733][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:54:17,733][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:54:20,216][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:54:20,221][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:56:53,073][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 21:56:53,086][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 21:56:53,087][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029.ckpt
[2024-09-07 21:56:53,092][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:56:53,097][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029.ckpt
[2024-09-07 21:56:56,526][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 21:57:01,058][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029.ckpt
[2024-09-07 21:57:01,259][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:57:01,260][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:57:01,527][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:57:01,539][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:57:01,540][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:57:01,541][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:57:01,541][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:57:01,541][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:57:01,543][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:57:01,546][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:57:01,549][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:57:01,549][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:57:01,549][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:57:01,549][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:57:04,038][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:57:04,043][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:59:38,206][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 21:59:38,220][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 21:59:38,221][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v1.ckpt
[2024-09-07 21:59:38,226][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 21:59:38,231][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v1.ckpt
[2024-09-07 21:59:41,649][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 21:59:47,535][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v1.ckpt
[2024-09-07 21:59:47,709][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 21:59:47,710][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 21:59:47,970][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 21:59:47,981][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 21:59:47,982][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 21:59:47,983][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 21:59:47,983][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 21:59:47,984][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 21:59:47,986][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 21:59:47,988][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 21:59:47,991][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 21:59:47,991][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 21:59:47,991][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 21:59:47,992][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 21:59:50,627][src.training_pipeline][INFO] - Starting training!
[2024-09-07 21:59:50,632][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:02:24,684][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:02:24,697][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:02:24,698][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006.ckpt
[2024-09-07 22:02:24,709][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:02:24,714][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006.ckpt
[2024-09-07 22:02:28,130][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:02:33,074][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006.ckpt
[2024-09-07 22:02:33,251][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:02:33,251][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:02:33,518][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:02:33,529][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:02:33,530][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:02:33,531][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:02:33,531][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:02:33,532][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:02:33,534][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:02:33,536][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:02:33,539][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:02:33,539][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:02:33,539][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:02:33,539][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:02:36,093][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:02:36,097][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:05:10,257][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:05:10,271][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:05:10,272][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v1.ckpt
[2024-09-07 22:05:10,276][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:05:10,281][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v1.ckpt
[2024-09-07 22:05:13,684][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:05:19,629][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v1.ckpt
[2024-09-07 22:05:19,808][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:05:19,809][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:05:20,072][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:05:20,083][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:05:20,083][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:05:20,084][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:05:20,084][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:05:20,085][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:05:20,087][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:05:20,090][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:05:20,093][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:05:20,093][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:05:20,093][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:05:20,093][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:05:22,624][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:05:22,626][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:07:56,287][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:07:56,300][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:07:56,301][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017.ckpt
[2024-09-07 22:07:56,306][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:07:56,311][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017.ckpt
[2024-09-07 22:07:59,714][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:08:05,745][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017.ckpt
[2024-09-07 22:08:05,949][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:08:05,949][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:08:06,217][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:08:06,228][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:08:06,229][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:08:06,229][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:08:06,230][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:08:06,230][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:08:06,232][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:08:06,235][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:08:06,238][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:08:06,238][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:08:06,238][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:08:06,238][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:08:08,708][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:08:08,710][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:10:41,227][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:10:41,240][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:10:41,241][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007.ckpt
[2024-09-07 22:10:41,246][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:10:41,250][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007.ckpt
[2024-09-07 22:10:44,744][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:10:49,500][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007.ckpt
[2024-09-07 22:10:49,678][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:10:49,678][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:10:49,940][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:10:50,126][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:10:50,127][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:10:50,128][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:10:50,128][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:10:50,129][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:10:50,131][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:10:50,133][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:10:50,136][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:10:50,136][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:10:50,136][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:10:50,136][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:10:52,479][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:10:52,483][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:13:25,899][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:13:25,913][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:13:25,914][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v4.ckpt
[2024-09-07 22:13:25,918][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:13:25,923][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v4.ckpt
[2024-09-07 22:13:29,393][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:13:35,865][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v4.ckpt
[2024-09-07 22:13:36,037][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:13:36,038][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:13:36,300][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:13:36,311][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:13:36,312][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:13:36,313][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:13:36,313][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:13:36,314][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:13:36,316][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:13:36,319][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:13:36,322][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:13:36,322][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:13:36,322][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:13:36,322][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:13:38,756][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:13:38,761][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:16:12,551][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:16:12,565][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:16:12,566][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v5.ckpt
[2024-09-07 22:16:12,574][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:16:12,586][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v5.ckpt
[2024-09-07 22:16:16,035][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:16:21,282][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v5.ckpt
[2024-09-07 22:16:21,455][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:16:21,455][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:16:21,720][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:16:21,731][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:16:21,732][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:16:21,733][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:16:21,733][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:16:21,733][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:16:21,735][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:16:21,738][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:16:21,741][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:16:21,741][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:16:21,741][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:16:21,741][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:16:24,395][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:16:24,397][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:18:59,153][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:18:59,166][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:18:59,167][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v2.ckpt
[2024-09-07 22:18:59,171][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:18:59,176][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v2.ckpt
[2024-09-07 22:19:02,601][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:19:07,064][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v2.ckpt
[2024-09-07 22:19:07,237][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:19:07,238][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:19:07,498][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:19:07,510][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:19:07,511][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:19:07,512][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:19:07,512][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:19:07,512][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:19:07,514][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:19:07,517][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:19:07,520][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:19:07,520][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:19:07,520][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:19:07,520][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:19:10,041][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:19:10,043][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:21:42,058][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:21:42,071][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:21:42,072][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v5.ckpt
[2024-09-07 22:21:42,086][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:21:42,099][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v5.ckpt
[2024-09-07 22:21:45,542][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:21:50,921][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v5.ckpt
[2024-09-07 22:21:51,094][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:21:51,094][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:21:51,355][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:21:51,366][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:21:51,366][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:21:51,367][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:21:51,367][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:21:51,368][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:21:51,370][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:21:51,372][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:21:51,375][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:21:51,376][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:21:51,376][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:21:51,376][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:21:53,925][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:21:53,927][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:24:27,474][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:24:27,487][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:24:27,488][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v6.ckpt
[2024-09-07 22:24:27,492][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:24:27,497][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v6.ckpt
[2024-09-07 22:24:30,900][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:24:37,184][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v6.ckpt
[2024-09-07 22:24:37,356][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:24:37,356][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:24:37,615][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:24:37,625][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:24:37,626][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:24:37,627][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:24:37,627][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:24:37,628][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:24:37,630][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:24:37,632][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:24:37,635][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:24:37,635][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:24:37,635][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:24:37,636][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:24:40,085][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:24:40,087][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:27:13,678][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:27:13,691][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:27:13,692][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v2.ckpt
[2024-09-07 22:27:13,696][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:27:13,701][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v2.ckpt
[2024-09-07 22:27:17,117][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:27:23,055][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v2.ckpt
[2024-09-07 22:27:23,293][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:27:23,293][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:27:23,553][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:27:23,564][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:27:23,565][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:27:23,566][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:27:23,566][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:27:23,567][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:27:23,569][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:27:23,571][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:27:23,574][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:27:23,574][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:27:23,574][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:27:23,575][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:27:26,002][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:27:26,003][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:29:59,110][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:29:59,123][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:29:59,124][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v6.ckpt
[2024-09-07 22:29:59,128][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:29:59,133][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v6.ckpt
[2024-09-07 22:30:02,544][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:30:09,333][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v6.ckpt
[2024-09-07 22:30:09,508][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:30:09,508][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:30:09,771][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:30:09,782][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:30:09,783][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:30:09,783][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:30:09,784][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:30:09,784][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:30:09,786][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:30:09,788][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:30:09,792][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:30:09,792][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:30:09,792][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:30:09,792][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:30:12,310][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:30:12,315][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:32:48,860][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:32:48,873][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:32:48,874][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v3.ckpt
[2024-09-07 22:32:48,879][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:32:48,884][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v3.ckpt
[2024-09-07 22:32:52,281][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:33:02,246][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v3.ckpt
[2024-09-07 22:33:02,474][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:33:02,475][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:33:02,732][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:33:02,913][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:33:02,914][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:33:02,915][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:33:02,915][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:33:02,916][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:33:02,918][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:33:02,920][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:33:02,923][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:33:02,923][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:33:02,923][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:33:02,924][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:33:05,173][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:33:05,179][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:35:39,081][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:35:39,094][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:35:39,095][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v7.ckpt
[2024-09-07 22:35:39,099][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:35:39,104][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v7.ckpt
[2024-09-07 22:35:42,484][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:35:47,701][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v7.ckpt
[2024-09-07 22:35:47,875][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:35:47,875][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:35:48,137][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:35:48,148][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:35:48,149][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:35:48,149][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:35:48,150][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:35:48,150][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:35:48,152][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:35:48,155][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:35:48,158][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:35:48,158][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:35:48,158][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:35:48,158][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:35:50,653][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:35:50,655][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:38:24,458][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:38:24,471][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:38:24,472][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_021.ckpt
[2024-09-07 22:38:24,476][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:38:24,481][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_021.ckpt
[2024-09-07 22:38:27,891][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:38:33,716][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_021.ckpt
[2024-09-07 22:38:33,891][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:38:33,891][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:38:34,152][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:38:34,163][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:38:34,164][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:38:34,165][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:38:34,165][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:38:34,166][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:38:34,168][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:38:34,170][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:38:34,173][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:38:34,173][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:38:34,173][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:38:34,173][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:38:36,646][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:38:36,648][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:41:09,292][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:41:09,305][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:41:09,306][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v7.ckpt
[2024-09-07 22:41:09,314][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:41:09,319][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v7.ckpt
[2024-09-07 22:41:12,787][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:41:17,542][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v7.ckpt
[2024-09-07 22:41:17,742][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:41:17,742][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:41:18,004][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:41:18,014][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:41:18,015][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:41:18,016][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:41:18,016][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:41:18,017][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:41:18,019][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:41:18,021][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:41:18,024][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:41:18,024][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:41:18,024][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:41:18,025][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:41:20,533][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:41:20,538][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:43:52,001][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:43:52,014][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:43:52,015][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v8.ckpt
[2024-09-07 22:43:52,019][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:43:52,024][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v8.ckpt
[2024-09-07 22:43:55,421][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:44:03,313][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v8.ckpt
[2024-09-07 22:44:03,501][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:44:03,502][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:44:03,762][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:44:03,773][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:44:03,774][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:44:03,775][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:44:03,775][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:44:03,776][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:44:03,778][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:44:03,780][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:44:03,783][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:44:03,783][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:44:03,783][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:44:03,783][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:44:06,247][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:44:06,250][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:46:41,438][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:46:41,452][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:46:41,453][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v9.ckpt
[2024-09-07 22:46:41,457][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:46:41,462][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v9.ckpt
[2024-09-07 22:46:44,850][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:46:51,082][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v9.ckpt
[2024-09-07 22:46:51,259][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:46:51,259][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:46:51,521][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:46:51,533][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:46:51,533][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:46:51,534][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:46:51,534][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:46:51,535][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:46:51,537][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:46:51,539][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:46:51,542][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:46:51,542][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:46:51,542][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:46:51,542][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:46:53,937][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:46:53,939][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:49:26,978][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:49:26,992][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:49:26,993][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v8.ckpt
[2024-09-07 22:49:26,997][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:49:27,002][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v8.ckpt
[2024-09-07 22:49:30,369][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:49:34,986][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v8.ckpt
[2024-09-07 22:49:35,159][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:49:35,160][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:49:35,420][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:49:35,431][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:49:35,432][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:49:35,432][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:49:35,433][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:49:35,433][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:49:35,435][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:49:35,438][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:49:35,441][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:49:35,441][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:49:35,441][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:49:35,441][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:49:37,927][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:49:37,932][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:52:13,488][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:52:13,501][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:52:13,502][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003.ckpt
[2024-09-07 22:52:13,506][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:52:13,511][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003.ckpt
[2024-09-07 22:52:16,904][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:52:23,344][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003.ckpt
[2024-09-07 22:52:23,522][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:52:23,522][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:52:23,787][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:52:23,798][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:52:23,799][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:52:23,799][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:52:23,800][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:52:23,800][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:52:23,802][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:52:23,804][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:52:23,807][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:52:23,808][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:52:23,808][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:52:23,808][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:52:26,116][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:52:26,121][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:54:59,540][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:54:59,552][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:54:59,553][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_028.ckpt
[2024-09-07 22:54:59,557][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:54:59,562][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_028.ckpt
[2024-09-07 22:55:02,953][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:55:08,882][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_028.ckpt
[2024-09-07 22:55:09,060][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:55:09,061][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:55:09,319][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:55:09,330][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:55:09,331][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:55:09,332][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:55:09,332][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:55:09,332][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:55:09,334][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:55:09,337][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:55:09,340][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:55:09,340][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:55:09,340][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:55:09,340][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:55:11,794][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:55:11,796][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:57:43,284][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 22:57:43,298][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 22:57:43,299][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v10.ckpt
[2024-09-07 22:57:43,304][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 22:57:43,309][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v10.ckpt
[2024-09-07 22:57:46,683][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 22:57:52,821][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v10.ckpt
[2024-09-07 22:57:52,999][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 22:57:52,999][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 22:57:53,264][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 22:57:53,275][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 22:57:53,275][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 22:57:53,276][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 22:57:53,276][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 22:57:53,277][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 22:57:53,279][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 22:57:53,281][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 22:57:53,284][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 22:57:53,284][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 22:57:53,284][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 22:57:53,285][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 22:57:55,807][src.training_pipeline][INFO] - Starting training!
[2024-09-07 22:57:55,809][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:00:29,029][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:00:29,043][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:00:29,044][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v9.ckpt
[2024-09-07 23:00:29,048][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:00:29,053][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v9.ckpt
[2024-09-07 23:00:32,471][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:00:38,766][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v9.ckpt
[2024-09-07 23:00:38,942][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:00:38,942][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:00:39,203][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:00:39,214][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:00:39,215][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:00:39,215][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:00:39,216][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:00:39,216][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:00:39,218][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:00:39,221][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:00:39,224][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:00:39,224][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:00:39,224][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:00:39,224][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:00:41,661][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:00:41,665][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:03:14,133][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:03:14,146][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:03:14,147][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v10.ckpt
[2024-09-07 23:03:14,152][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:03:14,157][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v10.ckpt
[2024-09-07 23:03:17,537][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:03:22,524][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v10.ckpt
[2024-09-07 23:03:22,700][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:03:22,700][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:03:22,962][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:03:22,975][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:03:22,975][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:03:22,976][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:03:22,976][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:03:22,977][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:03:22,979][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:03:22,981][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:03:22,984][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:03:22,984][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:03:22,984][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:03:22,984][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:03:25,488][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:03:25,490][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:05:59,105][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:05:59,119][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:05:59,120][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v11.ckpt
[2024-09-07 23:05:59,124][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:05:59,129][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v11.ckpt
[2024-09-07 23:06:02,516][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:06:09,096][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v11.ckpt
[2024-09-07 23:06:09,288][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:06:09,288][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:06:09,549][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:06:09,560][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:06:09,561][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:06:09,561][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:06:09,562][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:06:09,562][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:06:09,564][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:06:09,567][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:06:09,570][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:06:09,570][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:06:09,570][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:06:09,570][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:06:12,143][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:06:12,146][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:08:46,154][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:08:46,167][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:08:46,168][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017-v1.ckpt
[2024-09-07 23:08:46,172][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:08:46,177][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017-v1.ckpt
[2024-09-07 23:08:49,585][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:08:55,420][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017-v1.ckpt
[2024-09-07 23:08:55,619][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:08:55,619][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:08:55,883][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:08:55,894][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:08:55,895][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:08:55,895][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:08:55,896][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:08:55,896][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:08:55,898][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:08:55,901][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:08:55,904][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:08:55,904][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:08:55,904][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:08:55,904][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:08:58,304][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:08:58,309][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:11:31,336][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:11:31,349][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:11:31,350][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v12.ckpt
[2024-09-07 23:11:31,358][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:11:31,363][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v12.ckpt
[2024-09-07 23:11:34,785][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:11:41,229][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v12.ckpt
[2024-09-07 23:11:41,417][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:11:41,418][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:11:41,678][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:11:41,689][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:11:41,690][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:11:41,690][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:11:41,691][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:11:41,691][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:11:41,694][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:11:41,696][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:11:41,699][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:11:41,699][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:11:41,699][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:11:41,699][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:11:44,183][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:11:44,184][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:14:18,019][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:14:18,032][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:14:18,033][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v1.ckpt
[2024-09-07 23:14:18,038][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:14:18,043][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v1.ckpt
[2024-09-07 23:14:21,478][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:14:27,268][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v1.ckpt
[2024-09-07 23:14:27,447][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:14:27,448][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:14:27,711][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:14:27,722][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:14:27,723][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:14:27,724][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:14:27,724][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:14:27,725][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:14:27,727][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:14:27,729][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:14:27,732][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:14:27,732][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:14:27,732][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:14:27,732][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:14:30,180][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:14:30,185][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:17:05,220][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:17:05,233][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:17:05,234][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v13.ckpt
[2024-09-07 23:17:05,242][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:17:05,254][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v13.ckpt
[2024-09-07 23:17:08,762][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:17:16,115][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v13.ckpt
[2024-09-07 23:17:16,292][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:17:16,292][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:17:16,552][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:17:16,736][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:17:16,737][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:17:16,738][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:17:16,738][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:17:16,739][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:17:16,741][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:17:16,743][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:17:16,746][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:17:16,746][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:17:16,746][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:17:16,747][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:17:19,095][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:17:19,097][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:19:52,176][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:19:52,190][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:19:52,191][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v11.ckpt
[2024-09-07 23:19:52,195][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:19:52,201][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v11.ckpt
[2024-09-07 23:19:55,615][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:20:00,038][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v11.ckpt
[2024-09-07 23:20:00,214][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:20:00,215][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:20:00,478][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:20:00,489][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:20:00,490][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:20:00,490][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:20:00,491][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:20:00,491][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:20:00,493][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:20:00,496][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:20:00,499][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:20:00,499][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:20:00,499][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:20:00,499][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:20:03,047][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:20:03,050][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:22:33,076][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:22:33,089][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:22:33,090][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v12.ckpt
[2024-09-07 23:22:33,094][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:22:33,100][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v12.ckpt
[2024-09-07 23:22:36,526][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:22:42,068][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v12.ckpt
[2024-09-07 23:22:42,286][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:22:42,287][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:22:42,549][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:22:42,559][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:22:42,560][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:22:42,561][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:22:42,561][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:22:42,562][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:22:42,564][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:22:42,566][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:22:42,569][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:22:42,569][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:22:42,569][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:22:42,570][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:22:44,960][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:22:44,965][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:25:19,273][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:25:19,286][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:25:19,287][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_012.ckpt
[2024-09-07 23:25:19,291][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:25:19,296][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_012.ckpt
[2024-09-07 23:25:22,748][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:25:28,066][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_012.ckpt
[2024-09-07 23:25:28,247][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:25:28,247][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:25:28,509][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:25:28,520][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:25:28,521][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:25:28,522][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:25:28,522][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:25:28,523][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:25:28,525][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:25:28,527][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:25:28,530][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:25:28,530][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:25:28,530][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:25:28,530][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:25:31,064][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:25:31,067][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:28:03,903][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:28:03,917][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:28:03,917][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v13.ckpt
[2024-09-07 23:28:03,922][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:28:03,927][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v13.ckpt
[2024-09-07 23:28:07,367][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:28:13,860][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v13.ckpt
[2024-09-07 23:28:14,038][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:28:14,038][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:28:14,297][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:28:14,308][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:28:14,309][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:28:14,309][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:28:14,310][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:28:14,310][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:28:14,312][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:28:14,314][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:28:14,317][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:28:14,317][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:28:14,317][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:28:14,318][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:28:16,989][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:28:16,995][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:30:48,379][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:30:48,392][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:30:48,393][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v14.ckpt
[2024-09-07 23:30:48,398][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:30:48,403][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v14.ckpt
[2024-09-07 23:30:51,810][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:30:57,493][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v14.ckpt
[2024-09-07 23:30:57,670][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:30:57,670][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:30:57,933][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:30:57,945][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:30:57,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:30:57,947][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:30:57,947][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:30:57,948][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:30:57,950][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:30:57,952][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:30:57,955][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:30:57,955][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:30:57,955][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:30:57,955][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:31:00,514][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:31:00,519][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:33:34,555][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:33:34,568][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:33:34,569][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v15.ckpt
[2024-09-07 23:33:34,574][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:33:34,579][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v15.ckpt
[2024-09-07 23:33:37,992][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:33:43,390][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v15.ckpt
[2024-09-07 23:33:43,567][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:33:43,567][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:33:43,826][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:33:43,837][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:33:43,838][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:33:43,839][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:33:43,839][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:33:43,840][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:33:43,842][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:33:43,844][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:33:43,847][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:33:43,847][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:33:43,847][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:33:43,848][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:33:46,286][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:33:46,291][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:36:19,331][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:36:19,344][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:36:19,345][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_014.ckpt
[2024-09-07 23:36:19,349][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:36:19,354][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_014.ckpt
[2024-09-07 23:36:22,960][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:36:29,677][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_014.ckpt
[2024-09-07 23:36:29,872][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:36:29,873][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:36:30,133][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:36:30,144][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:36:30,145][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:36:30,145][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:36:30,146][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:36:30,146][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:36:30,148][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:36:30,151][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:36:30,154][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:36:30,154][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:36:30,154][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:36:30,154][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:36:32,730][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:36:32,732][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:39:07,000][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:39:07,013][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:39:07,014][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v14.ckpt
[2024-09-07 23:39:07,022][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:39:07,027][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v14.ckpt
[2024-09-07 23:39:10,424][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:39:15,308][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v14.ckpt
[2024-09-07 23:39:15,483][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:39:15,483][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:39:15,744][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:39:15,755][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:39:15,756][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:39:15,756][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:39:15,757][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:39:15,757][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:39:15,759][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:39:15,761][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:39:15,764][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:39:15,764][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:39:15,765][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:39:15,765][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:39:18,257][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:39:18,263][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:41:51,196][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:41:51,209][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:41:51,210][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v15.ckpt
[2024-09-07 23:41:51,218][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:41:51,223][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v15.ckpt
[2024-09-07 23:41:54,644][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:41:58,570][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v15.ckpt
[2024-09-07 23:41:58,750][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:41:58,750][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:41:59,014][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:41:59,039][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:41:59,041][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:41:59,041][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:41:59,042][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:41:59,042][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:41:59,045][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:41:59,048][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:41:59,052][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:41:59,052][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:41:59,052][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:41:59,052][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:42:01,517][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:42:01,523][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:44:34,475][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:44:34,488][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:44:34,489][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v16.ckpt
[2024-09-07 23:44:34,493][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:44:34,498][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v16.ckpt
[2024-09-07 23:44:37,891][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:44:44,647][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v16.ckpt
[2024-09-07 23:44:44,822][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:44:44,822][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:44:45,086][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:44:45,096][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:44:45,097][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:44:45,098][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:44:45,098][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:44:45,099][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:44:45,101][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:44:45,103][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:44:45,106][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:44:45,106][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:44:45,106][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:44:45,107][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:44:47,614][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:44:47,617][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:47:19,979][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:47:19,992][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:47:19,993][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v3.ckpt
[2024-09-07 23:47:19,997][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:47:20,002][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v3.ckpt
[2024-09-07 23:47:23,411][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:47:31,657][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v3.ckpt
[2024-09-07 23:47:31,835][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:47:31,836][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:47:32,097][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:47:32,107][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:47:32,108][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:47:32,109][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:47:32,109][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:47:32,110][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:47:32,112][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:47:32,114][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:47:32,117][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:47:32,117][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:47:32,118][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:47:32,118][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:47:34,600][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:47:34,606][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:50:07,803][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:50:07,816][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:50:07,817][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v16.ckpt
[2024-09-07 23:50:07,821][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:50:07,827][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v16.ckpt
[2024-09-07 23:50:11,253][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:50:17,493][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v16.ckpt
[2024-09-07 23:50:17,669][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:50:17,669][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:50:17,933][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:50:17,944][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:50:17,945][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:50:17,945][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:50:17,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:50:17,946][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:50:17,948][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:50:17,950][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:50:17,953][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:50:17,954][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:50:17,954][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:50:17,954][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:50:20,464][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:50:20,471][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:52:54,146][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:52:54,159][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:52:54,160][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017-v2.ckpt
[2024-09-07 23:52:54,164][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:52:54,169][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017-v2.ckpt
[2024-09-07 23:52:57,604][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:53:03,651][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_017-v2.ckpt
[2024-09-07 23:53:03,828][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:53:03,829][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:53:04,089][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:53:04,101][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:53:04,102][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:53:04,102][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:53:04,103][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:53:04,103][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:53:04,105][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:53:04,108][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:53:04,111][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:53:04,111][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:53:04,111][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:53:04,111][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:53:06,489][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:53:06,491][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:55:39,369][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:55:39,383][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:55:39,384][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v17.ckpt
[2024-09-07 23:55:39,388][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:55:39,393][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v17.ckpt
[2024-09-07 23:55:42,795][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:55:47,847][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v17.ckpt
[2024-09-07 23:55:48,022][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:55:48,023][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:55:48,284][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:55:48,295][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:55:48,296][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:55:48,296][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:55:48,297][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:55:48,297][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:55:48,299][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:55:48,302][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:55:48,305][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:55:48,305][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:55:48,305][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:55:48,305][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:55:50,720][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:55:50,722][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:58:23,471][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-07 23:58:23,484][src.training_pipeline][INFO] - Starting testing!
[2024-09-07 23:58:23,485][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v4.ckpt
[2024-09-07 23:58:23,489][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-07 23:58:23,493][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v4.ckpt
[2024-09-07 23:58:27,101][src.training_pipeline][INFO] - Finalizing!
[2024-09-07 23:58:33,604][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v4.ckpt
[2024-09-07 23:58:33,801][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-07 23:58:33,801][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-07 23:58:34,061][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-07 23:58:34,072][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-07 23:58:34,073][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-07 23:58:34,074][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-07 23:58:34,074][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-07 23:58:34,075][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-07 23:58:34,077][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-07 23:58:34,079][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-07 23:58:34,082][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-07 23:58:34,082][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-07 23:58:34,082][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-07 23:58:34,082][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-07 23:58:36,537][src.training_pipeline][INFO] - Starting training!
[2024-09-07 23:58:36,539][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:01:10,829][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:01:10,842][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:01:10,843][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v4.ckpt
[2024-09-08 00:01:10,846][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:01:10,851][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v4.ckpt
[2024-09-08 00:01:14,257][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:01:19,673][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v4.ckpt
[2024-09-08 00:01:19,849][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:01:19,849][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:01:20,111][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:01:20,122][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:01:20,123][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:01:20,124][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:01:20,124][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:01:20,124][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:01:20,127][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:01:20,129][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:01:20,132][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:01:20,132][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:01:20,132][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:01:20,132][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:01:22,634][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:01:22,636][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:03:55,694][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:03:55,707][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:03:55,708][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v18.ckpt
[2024-09-08 00:03:55,714][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:03:55,719][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v18.ckpt
[2024-09-08 00:03:59,138][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:04:05,102][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v18.ckpt
[2024-09-08 00:04:05,281][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:04:05,281][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:04:05,549][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:04:05,559][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:04:05,560][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:04:05,561][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:04:05,561][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:04:05,562][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:04:05,564][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:04:05,566][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:04:05,570][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:04:05,570][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:04:05,570][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:04:05,570][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:04:08,135][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:04:08,141][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:06:41,348][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:06:41,361][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:06:41,362][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022.ckpt
[2024-09-08 00:06:41,366][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:06:41,371][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022.ckpt
[2024-09-08 00:06:44,792][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:06:51,367][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022.ckpt
[2024-09-08 00:06:51,546][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:06:51,546][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:06:51,809][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:06:51,820][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:06:51,821][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:06:51,821][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:06:51,822][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:06:51,822][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:06:51,824][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:06:51,827][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:06:51,830][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:06:51,830][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:06:51,830][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:06:51,830][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:06:54,288][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:06:54,294][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:09:27,808][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:09:27,825][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:09:27,826][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v1.ckpt
[2024-09-08 00:09:27,832][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:09:27,837][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v1.ckpt
[2024-09-08 00:09:31,230][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:09:37,322][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v1.ckpt
[2024-09-08 00:09:37,496][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:09:37,497][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:09:37,759][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:09:37,770][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:09:37,771][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:09:37,772][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:09:37,772][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:09:37,773][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:09:37,775][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:09:37,777][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:09:37,780][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:09:37,781][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:09:37,781][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:09:37,781][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:09:40,235][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:09:40,241][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:12:13,353][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:12:13,366][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:12:13,367][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003-v1.ckpt
[2024-09-08 00:12:13,374][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:12:13,379][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003-v1.ckpt
[2024-09-08 00:12:16,775][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:12:22,786][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003-v1.ckpt
[2024-09-08 00:12:22,966][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:12:22,966][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:12:23,224][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:12:23,235][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:12:23,236][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:12:23,237][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:12:23,237][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:12:23,237][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:12:23,239][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:12:23,242][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:12:23,244][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:12:23,245][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:12:23,245][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:12:23,245][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:12:25,792][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:12:25,798][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:14:59,374][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:14:59,387][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:14:59,388][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_011.ckpt
[2024-09-08 00:14:59,392][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:14:59,397][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_011.ckpt
[2024-09-08 00:15:02,800][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:15:08,854][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_011.ckpt
[2024-09-08 00:15:09,046][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:15:09,046][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:15:09,312][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:15:09,324][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:15:09,325][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:15:09,326][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:15:09,326][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:15:09,327][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:15:09,329][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:15:09,331][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:15:09,334][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:15:09,334][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:15:09,334][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:15:09,334][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:15:11,709][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:15:11,712][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:17:43,637][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:17:43,650][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:17:43,651][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v17.ckpt
[2024-09-08 00:17:43,658][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:17:43,663][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v17.ckpt
[2024-09-08 00:17:47,076][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:17:52,357][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v17.ckpt
[2024-09-08 00:17:52,535][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:17:52,535][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:17:52,794][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:17:52,805][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:17:52,806][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:17:52,806][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:17:52,807][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:17:52,807][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:17:52,809][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:17:52,811][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:17:52,814][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:17:52,815][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:17:52,815][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:17:52,815][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:17:55,323][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:17:55,326][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:20:28,609][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:20:28,622][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:20:28,623][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v19.ckpt
[2024-09-08 00:20:28,627][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:20:28,633][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v19.ckpt
[2024-09-08 00:20:32,023][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:20:38,672][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v19.ckpt
[2024-09-08 00:20:38,848][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:20:38,848][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:20:39,106][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:20:39,117][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:20:39,118][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:20:39,119][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:20:39,119][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:20:39,120][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:20:39,122][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:20:39,124][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:20:39,127][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:20:39,127][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:20:39,127][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:20:39,128][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:20:41,567][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:20:41,573][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:23:15,922][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:23:15,934][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:23:15,935][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v18.ckpt
[2024-09-08 00:23:15,939][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:23:15,944][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v18.ckpt
[2024-09-08 00:23:19,432][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:23:24,483][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v18.ckpt
[2024-09-08 00:23:24,657][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:23:24,658][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:23:24,915][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:23:24,927][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:23:24,928][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:23:24,929][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:23:24,929][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:23:24,929][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:23:24,931][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:23:24,934][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:23:24,937][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:23:24,937][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:23:24,937][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:23:24,937][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:23:30,383][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:23:30,385][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:26:03,156][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:26:03,169][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:26:03,170][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v2.ckpt
[2024-09-08 00:26:03,181][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:26:03,186][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v2.ckpt
[2024-09-08 00:26:06,650][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:26:11,383][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v2.ckpt
[2024-09-08 00:26:11,558][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:26:11,559][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:26:11,816][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:26:11,827][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:26:11,828][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:26:11,828][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:26:11,829][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:26:11,829][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:26:11,831][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:26:11,834][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:26:11,837][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:26:11,837][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:26:11,837][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:26:11,837][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:26:14,424][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:26:14,430][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:28:47,585][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:28:47,599][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:28:47,599][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v19.ckpt
[2024-09-08 00:28:47,606][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:28:47,611][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v19.ckpt
[2024-09-08 00:28:51,036][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:28:55,230][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v19.ckpt
[2024-09-08 00:28:55,407][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:28:55,408][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:28:55,669][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:28:55,680][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:28:55,681][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:28:55,681][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:28:55,682][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:28:55,682][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:28:55,684][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:28:55,687][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:28:55,690][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:28:55,690][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:28:55,690][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:28:55,690][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:28:58,212][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:28:58,218][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:31:31,639][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:31:31,653][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:31:31,654][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v20.ckpt
[2024-09-08 00:31:31,658][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:31:31,663][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v20.ckpt
[2024-09-08 00:31:35,041][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:31:41,447][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v20.ckpt
[2024-09-08 00:31:41,626][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:31:41,626][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:31:41,892][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:31:41,903][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:31:41,904][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:31:41,904][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:31:41,905][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:31:41,905][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:31:41,908][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:31:41,911][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:31:41,914][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:31:41,914][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:31:41,914][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:31:41,914][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:31:44,497][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:31:44,504][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:34:17,679][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:34:17,693][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:34:17,694][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v5.ckpt
[2024-09-08 00:34:17,699][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:34:17,704][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v5.ckpt
[2024-09-08 00:34:21,119][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:34:27,420][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v5.ckpt
[2024-09-08 00:34:27,615][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:34:27,615][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:34:27,880][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:34:27,891][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:34:27,892][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:34:27,892][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:34:27,893][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:34:27,893][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:34:27,895][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:34:27,898][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:34:27,901][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:34:27,901][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:34:27,901][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:34:27,901][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:34:30,607][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:34:30,609][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:37:04,349][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:37:04,363][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:37:04,364][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v21.ckpt
[2024-09-08 00:37:04,368][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:37:04,373][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v21.ckpt
[2024-09-08 00:37:07,810][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:37:14,076][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v21.ckpt
[2024-09-08 00:37:14,253][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:37:14,254][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:37:14,515][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:37:14,526][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:37:14,527][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:37:14,527][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:37:14,528][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:37:14,528][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:37:14,530][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:37:14,533][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:37:14,536][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:37:14,536][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:37:14,536][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:37:14,536][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:37:17,425][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:37:17,430][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:39:51,461][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:39:51,474][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:39:51,475][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v22.ckpt
[2024-09-08 00:39:51,482][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:39:51,487][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v22.ckpt
[2024-09-08 00:39:54,941][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:40:02,296][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v22.ckpt
[2024-09-08 00:40:02,475][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:40:02,475][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:40:02,737][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:40:02,748][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:40:02,749][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:40:02,749][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:40:02,750][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:40:02,750][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:40:02,752][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:40:02,755][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:40:02,758][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:40:02,758][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:40:02,758][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:40:02,758][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:40:05,232][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:40:05,234][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:42:38,557][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:42:38,570][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:42:38,571][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v23.ckpt
[2024-09-08 00:42:38,577][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:42:38,582][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v23.ckpt
[2024-09-08 00:42:41,969][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:42:47,680][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v23.ckpt
[2024-09-08 00:42:47,856][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:42:47,857][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:42:48,118][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:42:48,129][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:42:48,130][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:42:48,131][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:42:48,131][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:42:48,132][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:42:48,134][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:42:48,136][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:42:48,139][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:42:48,139][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:42:48,139][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:42:48,139][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:42:50,602][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:42:50,604][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:45:23,673][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:45:23,686][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:45:23,687][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v24.ckpt
[2024-09-08 00:45:23,694][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:45:23,698][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v24.ckpt
[2024-09-08 00:45:27,080][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:45:31,400][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v24.ckpt
[2024-09-08 00:45:31,576][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:45:31,577][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:45:31,835][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:45:32,017][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:45:32,018][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:45:32,018][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:45:32,019][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:45:32,019][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:45:32,021][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:45:32,024][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:45:32,027][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:45:32,027][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:45:32,027][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:45:32,027][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:45:34,356][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:45:34,362][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:48:07,197][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:48:07,211][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:48:07,212][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v6.ckpt
[2024-09-08 00:48:07,217][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:48:07,222][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v6.ckpt
[2024-09-08 00:48:10,620][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:48:14,899][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v6.ckpt
[2024-09-08 00:48:15,074][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:48:15,075][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:48:15,337][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:48:15,348][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:48:15,349][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:48:15,350][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:48:15,350][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:48:15,351][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:48:15,353][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:48:15,355][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:48:15,358][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:48:15,358][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:48:15,358][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:48:15,358][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:48:17,816][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:48:17,825][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:50:50,500][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:50:50,513][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:50:50,514][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v25.ckpt
[2024-09-08 00:50:50,522][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:50:50,527][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v25.ckpt
[2024-09-08 00:50:53,940][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:51:01,966][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v25.ckpt
[2024-09-08 00:51:02,145][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:51:02,145][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:51:02,407][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:51:02,418][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:51:02,419][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:51:02,420][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:51:02,420][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:51:02,420][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:51:02,422][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:51:02,425][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:51:02,428][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:51:02,428][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:51:02,428][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:51:02,428][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:51:04,901][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:51:04,907][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:53:36,815][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:53:36,828][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:53:36,829][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v20.ckpt
[2024-09-08 00:53:36,833][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:53:36,838][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v20.ckpt
[2024-09-08 00:53:40,276][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:53:45,583][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v20.ckpt
[2024-09-08 00:53:45,782][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:53:45,783][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:53:46,047][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:53:46,058][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:53:46,059][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:53:46,059][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:53:46,060][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:53:46,060][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:53:46,062][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:53:46,065][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:53:46,068][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:53:46,068][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:53:46,068][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:53:46,068][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:53:48,448][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:53:48,451][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:56:22,495][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:56:22,509][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:56:22,510][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v26.ckpt
[2024-09-08 00:56:22,513][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:56:22,518][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v26.ckpt
[2024-09-08 00:56:25,916][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:56:30,995][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v26.ckpt
[2024-09-08 00:56:31,172][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:56:31,172][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:56:31,431][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:56:31,442][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:56:31,443][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:56:31,443][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:56:31,444][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:56:31,444][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:56:31,446][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:56:31,449][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:56:31,452][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:56:31,452][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:56:31,452][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:56:31,452][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:56:33,870][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:56:33,873][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:59:07,804][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 00:59:07,817][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 00:59:07,818][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v27.ckpt
[2024-09-08 00:59:07,833][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 00:59:07,838][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v27.ckpt
[2024-09-08 00:59:11,277][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 00:59:17,151][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v27.ckpt
[2024-09-08 00:59:17,326][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 00:59:17,326][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 00:59:17,588][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 00:59:17,601][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 00:59:17,602][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 00:59:17,602][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 00:59:17,602][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 00:59:17,603][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 00:59:17,605][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 00:59:17,607][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 00:59:17,610][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 00:59:17,610][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 00:59:17,611][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 00:59:17,611][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 00:59:20,143][src.training_pipeline][INFO] - Starting training!
[2024-09-08 00:59:20,150][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:01:54,370][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:01:54,384][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:01:54,385][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027.ckpt
[2024-09-08 01:01:54,389][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:01:54,394][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027.ckpt
[2024-09-08 01:01:57,846][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:02:02,634][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027.ckpt
[2024-09-08 01:02:02,843][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:02:02,844][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:02:03,113][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:02:03,124][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:02:03,124][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:02:03,125][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:02:03,125][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:02:03,126][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:02:03,128][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:02:03,131][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:02:03,134][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:02:03,134][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:02:03,134][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:02:03,134][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:02:05,592][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:02:05,595][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:04:38,736][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:04:38,749][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:04:38,750][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v28.ckpt
[2024-09-08 01:04:38,754][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:04:38,759][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v28.ckpt
[2024-09-08 01:04:42,196][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:04:48,671][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v28.ckpt
[2024-09-08 01:04:48,848][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:04:48,849][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:04:49,110][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:04:49,121][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:04:49,122][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:04:49,123][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:04:49,123][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:04:49,123][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:04:49,125][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:04:49,128][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:04:49,131][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:04:49,131][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:04:49,131][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:04:49,131][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:04:51,567][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:04:51,573][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:07:24,893][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:07:24,906][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:07:24,907][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v29.ckpt
[2024-09-08 01:07:24,911][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:07:24,916][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v29.ckpt
[2024-09-08 01:07:28,360][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:07:34,787][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v29.ckpt
[2024-09-08 01:07:34,963][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:07:34,963][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:07:35,221][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:07:35,232][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:07:35,233][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:07:35,234][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:07:35,234][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:07:35,235][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:07:35,237][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:07:35,239][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:07:35,242][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:07:35,242][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:07:35,242][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:07:35,242][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:07:37,663][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:07:37,665][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:10:10,893][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:10:10,907][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:10:10,908][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v30.ckpt
[2024-09-08 01:10:10,912][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:10:10,917][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v30.ckpt
[2024-09-08 01:10:14,309][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:10:18,709][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v30.ckpt
[2024-09-08 01:10:18,889][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:10:18,889][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:10:19,152][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:10:19,163][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:10:19,164][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:10:19,164][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:10:19,165][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:10:19,165][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:10:19,167][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:10:19,170][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:10:19,173][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:10:19,173][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:10:19,173][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:10:19,173][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:10:21,710][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:10:21,717][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:12:53,782][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:12:53,795][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:12:53,796][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v31.ckpt
[2024-09-08 01:12:53,801][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:12:53,806][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v31.ckpt
[2024-09-08 01:12:57,203][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:13:04,812][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v31.ckpt
[2024-09-08 01:13:04,988][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:13:04,989][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:13:05,251][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:13:05,262][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:13:05,263][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:13:05,264][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:13:05,264][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:13:05,265][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:13:05,267][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:13:05,269][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:13:05,272][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:13:05,272][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:13:05,272][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:13:05,272][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:13:07,762][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:13:07,765][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:15:41,961][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:15:41,975][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:15:41,975][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v2.ckpt
[2024-09-08 01:15:41,982][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:15:41,986][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v2.ckpt
[2024-09-08 01:15:45,404][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:15:50,420][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v2.ckpt
[2024-09-08 01:15:50,596][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:15:50,597][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:15:50,860][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:15:50,872][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:15:50,873][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:15:50,873][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:15:50,874][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:15:50,874][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:15:50,876][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:15:50,879][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:15:50,882][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:15:50,882][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:15:50,882][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:15:50,882][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:15:53,303][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:15:53,310][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:18:25,723][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:18:25,736][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:18:25,737][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_009.ckpt
[2024-09-08 01:18:25,748][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:18:25,754][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_009.ckpt
[2024-09-08 01:18:29,152][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:18:33,886][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_009.ckpt
[2024-09-08 01:18:34,063][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:18:34,063][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:18:34,326][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:18:34,337][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:18:34,338][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:18:34,338][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:18:34,339][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:18:34,339][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:18:34,341][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:18:34,344][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:18:34,347][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:18:34,347][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:18:34,347][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:18:34,347][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:18:36,804][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:18:36,811][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:21:11,077][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:21:11,091][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:21:11,092][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v32.ckpt
[2024-09-08 01:21:11,096][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:21:11,101][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v32.ckpt
[2024-09-08 01:21:14,523][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:21:19,444][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v32.ckpt
[2024-09-08 01:21:19,644][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:21:19,644][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:21:19,907][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:21:19,918][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:21:19,919][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:21:19,919][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:21:19,920][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:21:19,920][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:21:19,922][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:21:19,925][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:21:19,928][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:21:19,928][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:21:19,928][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:21:19,928][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:21:22,373][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:21:22,380][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:23:56,364][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:23:56,377][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:23:56,378][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_013.ckpt
[2024-09-08 01:23:56,382][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:23:56,387][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_013.ckpt
[2024-09-08 01:23:59,781][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:24:04,996][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_013.ckpt
[2024-09-08 01:24:05,211][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:24:05,211][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:24:05,474][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:24:05,485][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:24:05,486][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:24:05,487][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:24:05,487][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:24:05,487][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:24:05,489][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:24:05,492][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:24:05,495][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:24:05,495][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:24:05,495][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:24:05,495][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:24:07,885][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:24:07,887][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:26:40,301][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:26:40,314][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:26:40,315][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027-v1.ckpt
[2024-09-08 01:26:40,326][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:26:40,339][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027-v1.ckpt
[2024-09-08 01:26:43,759][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:26:49,221][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027-v1.ckpt
[2024-09-08 01:26:49,402][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:26:49,402][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:26:49,663][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:26:49,674][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:26:49,675][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:26:49,676][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:26:49,676][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:26:49,676][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:26:49,678][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:26:49,681][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:26:49,684][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:26:49,684][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:26:49,684][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:26:49,684][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:26:52,169][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:26:52,172][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:29:25,589][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:29:25,602][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:29:25,603][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v3.ckpt
[2024-09-08 01:29:25,607][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:29:25,612][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v3.ckpt
[2024-09-08 01:29:29,072][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:29:34,768][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_007-v3.ckpt
[2024-09-08 01:29:34,943][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:29:34,944][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:29:35,202][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:29:35,386][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:29:35,387][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:29:35,388][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:29:35,388][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:29:35,389][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:29:35,391][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:29:35,393][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:29:35,396][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:29:35,396][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:29:35,396][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:29:35,396][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:29:37,655][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:29:37,657][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:32:09,368][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:32:09,382][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:32:09,382][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v21.ckpt
[2024-09-08 01:32:09,396][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:32:09,401][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v21.ckpt
[2024-09-08 01:32:12,828][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:32:18,006][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v21.ckpt
[2024-09-08 01:32:18,186][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:32:18,186][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:32:18,451][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:32:18,462][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:32:18,462][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:32:18,463][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:32:18,463][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:32:18,464][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:32:18,466][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:32:18,468][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:32:18,471][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:32:18,471][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:32:18,471][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:32:18,472][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:32:20,963][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:32:20,966][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:34:54,377][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:34:54,390][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:34:54,391][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v33.ckpt
[2024-09-08 01:34:54,400][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:34:54,412][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v33.ckpt
[2024-09-08 01:34:57,912][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:35:03,372][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v33.ckpt
[2024-09-08 01:35:03,548][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:35:03,549][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:35:03,810][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:35:03,821][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:35:03,822][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:35:03,822][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:35:03,823][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:35:03,823][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:35:03,825][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:35:03,827][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:35:03,831][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:35:03,831][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:35:03,831][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:35:03,831][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:35:06,238][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:35:06,245][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:37:39,665][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:37:39,679][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:37:39,679][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v34.ckpt
[2024-09-08 01:37:39,684][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:37:39,689][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v34.ckpt
[2024-09-08 01:37:43,111][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:37:49,384][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v34.ckpt
[2024-09-08 01:37:49,559][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:37:49,559][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:37:49,822][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:37:49,834][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:37:49,835][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:37:49,835][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:37:49,836][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:37:49,836][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:37:49,838][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:37:49,840][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:37:49,843][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:37:49,844][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:37:49,844][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:37:49,844][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:37:52,336][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:37:52,339][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:40:25,741][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:40:25,755][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:40:25,756][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_015.ckpt
[2024-09-08 01:40:25,760][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:40:25,765][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_015.ckpt
[2024-09-08 01:40:29,174][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:40:35,320][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_015.ckpt
[2024-09-08 01:40:35,498][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:40:35,498][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:40:35,759][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:40:35,770][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:40:35,770][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:40:35,771][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:40:35,771][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:40:35,772][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:40:35,774][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:40:35,776][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:40:35,779][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:40:35,780][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:40:35,780][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:40:35,780][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:40:38,376][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:40:38,383][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:43:11,655][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:43:11,668][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:43:11,669][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v35.ckpt
[2024-09-08 01:43:11,674][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:43:11,679][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v35.ckpt
[2024-09-08 01:43:15,092][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:43:21,204][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v35.ckpt
[2024-09-08 01:43:21,381][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:43:21,382][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:43:21,645][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:43:21,655][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:43:21,656][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:43:21,657][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:43:21,657][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:43:21,658][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:43:21,660][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:43:21,662][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:43:21,665][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:43:21,665][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:43:21,666][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:43:21,666][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:43:24,210][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:43:24,215][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:45:58,059][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:45:58,073][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:45:58,074][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v3.ckpt
[2024-09-08 01:45:58,078][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:45:58,082][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v3.ckpt
[2024-09-08 01:46:01,490][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:46:07,098][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_022-v3.ckpt
[2024-09-08 01:46:07,275][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:46:07,276][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:46:07,541][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:46:07,552][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:46:07,553][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:46:07,554][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:46:07,554][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:46:07,555][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:46:07,557][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:46:07,559][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:46:07,562][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:46:07,562][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:46:07,562][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:46:07,563][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:46:10,049][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:46:10,052][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:48:43,381][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:48:43,395][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:48:43,395][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v7.ckpt
[2024-09-08 01:48:43,400][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:48:43,405][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v7.ckpt
[2024-09-08 01:48:46,809][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:48:52,645][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v7.ckpt
[2024-09-08 01:48:52,821][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:48:52,821][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:48:53,083][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:48:53,093][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:48:53,094][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:48:53,095][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:48:53,095][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:48:53,096][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:48:53,098][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:48:53,100][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:48:53,103][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:48:53,104][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:48:53,104][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:48:53,104][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:49:01,362][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:49:01,365][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:51:34,956][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:51:34,968][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:51:34,969][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v36.ckpt
[2024-09-08 01:51:34,974][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:51:34,979][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v36.ckpt
[2024-09-08 01:51:38,389][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:51:44,780][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v36.ckpt
[2024-09-08 01:51:44,953][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:51:44,953][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:51:45,214][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:51:45,399][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:51:45,401][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:51:45,401][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:51:45,402][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:51:45,402][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:51:45,404][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:51:45,406][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:51:45,409][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:51:45,410][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:51:45,410][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:51:45,410][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:51:47,740][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:51:47,747][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:54:21,666][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:54:21,680][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:54:21,681][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v22.ckpt
[2024-09-08 01:54:21,685][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:54:21,690][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v22.ckpt
[2024-09-08 01:54:25,136][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:54:32,215][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v22.ckpt
[2024-09-08 01:54:32,400][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:54:32,400][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:54:32,680][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:54:32,691][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:54:32,692][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:54:32,692][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:54:32,693][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:54:32,693][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:54:32,695][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:54:32,698][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:54:32,701][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:54:32,701][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:54:32,701][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:54:32,701][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:54:35,083][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:54:35,085][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:57:07,499][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:57:07,513][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:57:07,514][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v23.ckpt
[2024-09-08 01:57:07,522][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:57:07,533][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v23.ckpt
[2024-09-08 01:57:11,024][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:57:15,665][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v23.ckpt
[2024-09-08 01:57:15,841][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:57:15,842][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:57:16,104][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:57:16,115][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:57:16,116][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:57:16,117][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:57:16,117][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:57:16,118][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:57:16,120][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:57:16,122][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:57:16,125][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:57:16,125][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:57:16,125][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:57:16,125][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 01:57:18,568][src.training_pipeline][INFO] - Starting training!
[2024-09-08 01:57:18,571][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:59:50,632][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 01:59:50,645][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 01:59:50,646][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v37.ckpt
[2024-09-08 01:59:50,652][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 01:59:50,657][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v37.ckpt
[2024-09-08 01:59:54,083][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 01:59:59,447][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v37.ckpt
[2024-09-08 01:59:59,669][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 01:59:59,670][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 01:59:59,933][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 01:59:59,945][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 01:59:59,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 01:59:59,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 01:59:59,947][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 01:59:59,947][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 01:59:59,949][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 01:59:59,952][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 01:59:59,955][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 01:59:59,955][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 01:59:59,955][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 01:59:59,955][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 02:00:02,377][src.training_pipeline][INFO] - Starting training!
[2024-09-08 02:00:02,383][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:02:36,600][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 02:02:36,613][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 02:02:36,614][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v5.ckpt
[2024-09-08 02:02:36,622][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:02:36,627][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v5.ckpt
[2024-09-08 02:02:40,030][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 02:02:44,966][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_006-v5.ckpt
[2024-09-08 02:02:45,143][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 02:02:45,144][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 02:02:45,402][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 02:02:45,413][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 02:02:45,414][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 02:02:45,414][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 02:02:45,415][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 02:02:45,415][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 02:02:45,417][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 02:02:45,420][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 02:02:45,423][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 02:02:45,423][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 02:02:45,423][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 02:02:45,423][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 02:02:47,881][src.training_pipeline][INFO] - Starting training!
[2024-09-08 02:02:47,884][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:05:21,410][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 02:05:21,424][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 02:05:21,425][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v38.ckpt
[2024-09-08 02:05:21,430][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:05:21,435][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v38.ckpt
[2024-09-08 02:05:24,865][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 02:05:30,662][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v38.ckpt
[2024-09-08 02:05:30,840][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 02:05:30,840][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 02:05:31,125][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 02:05:31,136][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 02:05:31,137][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 02:05:31,137][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 02:05:31,138][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 02:05:31,138][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 02:05:31,140][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 02:05:31,142][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 02:05:31,145][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 02:05:31,145][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 02:05:31,146][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 02:05:31,146][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 02:05:33,590][src.training_pipeline][INFO] - Starting training!
[2024-09-08 02:05:33,597][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:08:05,288][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 02:08:05,302][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 02:08:05,303][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v39.ckpt
[2024-09-08 02:08:05,307][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:08:05,312][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v39.ckpt
[2024-09-08 02:08:08,836][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 02:08:14,540][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v39.ckpt
[2024-09-08 02:08:14,714][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 02:08:14,714][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 02:08:14,981][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 02:08:15,004][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 02:08:15,006][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 02:08:15,006][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 02:08:15,007][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 02:08:15,007][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 02:08:15,010][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 02:08:15,013][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 02:08:15,016][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 02:08:15,017][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 02:08:15,017][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 02:08:15,017][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 02:08:17,475][src.training_pipeline][INFO] - Starting training!
[2024-09-08 02:08:17,482][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:10:51,489][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 02:10:51,502][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 02:10:51,503][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v24.ckpt
[2024-09-08 02:10:51,512][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:10:51,523][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v24.ckpt
[2024-09-08 02:10:54,956][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 02:11:10,637][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v24.ckpt
[2024-09-08 02:11:10,813][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 02:11:10,814][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 02:11:11,079][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 02:11:11,090][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 02:11:11,090][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 02:11:11,091][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 02:11:11,091][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 02:11:11,092][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 02:11:11,094][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 02:11:11,096][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 02:11:11,099][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 02:11:11,099][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 02:11:11,099][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 02:11:11,100][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 02:11:13,754][src.training_pipeline][INFO] - Starting training!
[2024-09-08 02:11:13,762][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:13:47,956][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 02:13:47,969][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 02:13:47,970][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027-v2.ckpt
[2024-09-08 02:13:47,974][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:13:47,979][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027-v2.ckpt
[2024-09-08 02:13:51,436][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 02:13:56,965][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_027-v2.ckpt
[2024-09-08 02:13:57,143][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 02:13:57,143][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 02:13:57,403][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 02:13:57,588][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 02:13:57,589][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 02:13:57,590][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 02:13:57,590][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 02:13:57,591][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 02:13:57,593][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 02:13:57,595][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 02:13:57,598][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 02:13:57,598][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 02:13:57,598][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 02:13:57,598][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 02:14:00,245][src.training_pipeline][INFO] - Starting training!
[2024-09-08 02:14:00,253][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:16:33,749][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 02:16:33,763][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 02:16:33,763][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v40.ckpt
[2024-09-08 02:16:33,768][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:16:33,773][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v40.ckpt
[2024-09-08 02:16:37,185][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 02:16:42,905][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v40.ckpt
[2024-09-08 02:16:43,084][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 02:16:43,084][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 02:16:43,351][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 02:16:43,362][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 02:16:43,363][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 02:16:43,364][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 02:16:43,364][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 02:16:43,365][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 02:16:43,367][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 02:16:43,369][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 02:16:43,372][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 02:16:43,372][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 02:16:43,372][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 02:16:43,372][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 02:16:45,869][src.training_pipeline][INFO] - Starting training!
[2024-09-08 02:16:45,872][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:19:18,477][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 02:19:18,491][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 02:19:18,492][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v41.ckpt
[2024-09-08 02:19:18,496][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:19:18,501][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v41.ckpt
[2024-09-08 02:19:21,942][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 02:19:26,792][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v41.ckpt
[2024-09-08 02:19:26,971][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 02:19:26,971][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 02:19:27,235][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 02:19:27,246][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 02:19:27,247][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 02:19:27,247][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 02:19:27,248][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 02:19:27,248][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 02:19:27,251][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 02:19:27,253][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 02:19:27,256][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 02:19:27,256][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 02:19:27,256][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 02:19:27,256][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 02:19:29,682][src.training_pipeline][INFO] - Starting training!
[2024-09-08 02:19:29,685][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:22:02,484][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-08 02:22:02,497][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 02:22:02,498][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v42.ckpt
[2024-09-08 02:22:02,505][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 02:22:02,511][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v42.ckpt
[2024-09-08 02:22:05,977][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 02:22:10,077][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v42.ckpt
[2024-09-08 20:08:11,785][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:08:11,812][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:08:11,814][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:08:11,814][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:08:11,878][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:08:11,903][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:08:12,934][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:08:12,936][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:08:12,943][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:08:12,943][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:08:13,223][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:08:13,288][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:08:15,416][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:08:33,756][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:08:33,759][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:08:33,760][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:08:33,760][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:08:33,771][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:08:33,782][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:08:34,207][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:08:34,209][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:08:34,215][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:08:34,215][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:08:34,299][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:08:34,300][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:08:34,956][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:09:39,691][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:09:39,694][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:09:39,695][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:09:39,695][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:09:39,705][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:09:39,716][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:09:40,143][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:09:40,146][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:09:40,161][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:09:40,162][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:09:40,259][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:09:40,260][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:09:40,916][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:11:15,847][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:11:15,851][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:11:15,852][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:11:15,852][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:11:15,862][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:11:15,873][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:11:16,296][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:11:16,298][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:11:16,314][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:11:16,314][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:11:16,410][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:11:16,411][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:11:17,061][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:12:20,121][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:12:20,124][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:12:20,125][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:12:20,125][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:12:20,136][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:12:20,147][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:12:20,566][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:12:20,569][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:12:20,574][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:12:20,575][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:12:20,680][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:12:20,682][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:12:21,333][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:13:30,807][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:13:30,810][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:13:30,811][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:13:30,811][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:13:30,822][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:13:30,833][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:13:31,257][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:13:31,260][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:13:31,275][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:13:31,276][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:13:31,399][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:13:31,401][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:13:32,056][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:15:13,060][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:15:13,063][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:15:13,064][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:15:13,064][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:15:13,075][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:15:13,086][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:15:13,508][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:15:13,511][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:15:13,517][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:15:13,517][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:15:13,617][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:15:13,618][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:15:14,269][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:15:52,796][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:15:52,800][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:15:52,800][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:15:52,801][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:15:52,811][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:15:52,822][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:15:53,244][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:15:53,247][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:15:53,252][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:15:53,253][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:15:53,363][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:15:53,364][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:15:54,016][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:15:54,131][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 20:15:54,132][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 20:15:54,133][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 20:15:54,134][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 20:15:54,134][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 20:15:54,137][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 20:15:54,158][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 20:15:54,169][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 20:15:54,170][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 20:15:54,170][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 20:15:54,170][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 20:15:54,212][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:15:54,215][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:15:54,652][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 20:15:54,697][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 20:15:54,727][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 20:15:54,746][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 20:15:57,816][src.training_pipeline][INFO] - Starting training!
[2024-09-08 20:15:57,844][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 20:15:57,846][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 20:16:05,004][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 20:16:05,055][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 20:17:19,401][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:17:19,405][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:17:19,405][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:17:19,406][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:17:19,416][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:17:19,427][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:17:19,847][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:17:19,849][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:17:19,855][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:17:19,855][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:17:19,955][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:17:19,957][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:17:20,616][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:17:20,709][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 20:17:20,711][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 20:17:20,712][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 20:17:20,712][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 20:17:20,713][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 20:17:20,718][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 20:17:20,765][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 20:17:20,775][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 20:17:20,776][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 20:17:20,776][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 20:17:20,777][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 20:17:20,817][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:17:20,818][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:17:21,249][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 20:17:21,310][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 20:17:21,341][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 20:17:21,358][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 20:17:23,431][src.training_pipeline][INFO] - Starting training!
[2024-09-08 20:17:23,470][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 20:17:23,475][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 20:18:37,560][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 20:18:37,619][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 20:18:41,558][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 20:18:41,561][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 20:18:41,562][matplotlib][DEBUG] - interactive is False
[2024-09-08 20:18:41,562][matplotlib][DEBUG] - platform is linux
[2024-09-08 20:18:41,573][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 20:18:41,584][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 20:18:42,004][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:18:42,007][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:18:42,023][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 20:18:42,023][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 20:18:42,118][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 20:18:42,119][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 20:18:42,780][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 20:18:42,874][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 20:18:42,876][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 20:18:42,877][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 20:18:42,878][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 20:18:42,878][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 20:18:42,883][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 20:18:42,934][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 20:18:42,944][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 20:18:42,945][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 20:18:42,945][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 20:18:42,945][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 20:18:42,984][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:18:42,986][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 20:18:43,616][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 20:18:43,658][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 20:18:43,688][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 20:18:43,706][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 20:18:45,663][src.training_pipeline][INFO] - Starting training!
[2024-09-08 20:18:45,692][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 20:18:45,694][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 20:18:52,930][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v5.ckpt
[2024-09-08 20:18:52,939][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v116.ckpt
[2024-09-08 20:18:52,942][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-08 20:18:52,954][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 20:18:52,955][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v5.ckpt
[2024-09-08 20:18:52,956][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v5.ckpt
[2024-09-08 20:18:52,960][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 20:18:52,962][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v5.ckpt
[2024-09-08 20:18:56,432][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 20:19:02,688][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 20:19:02,741][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 20:19:02,742][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v5.ckpt
[2024-09-08 21:04:43,206][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 21:04:43,209][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 21:04:43,210][matplotlib][DEBUG] - interactive is False
[2024-09-08 21:04:43,210][matplotlib][DEBUG] - platform is linux
[2024-09-08 21:04:43,221][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 21:04:43,232][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 21:04:43,658][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:04:43,661][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:04:43,667][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 21:04:43,667][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 21:04:43,751][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 21:04:43,752][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 21:04:44,413][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 21:04:44,504][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 21:04:44,506][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 21:04:44,507][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 21:04:44,508][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 21:04:44,509][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 21:04:44,513][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 21:04:44,559][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 21:04:44,569][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 21:04:44,571][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 21:04:44,571][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 21:04:44,571][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 21:04:44,611][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:04:44,613][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:04:45,032][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 21:04:45,081][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:04:45,109][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:04:45,127][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 21:04:47,284][src.training_pipeline][INFO] - Starting training!
[2024-09-08 21:04:47,323][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 21:04:47,328][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:04:54,438][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 21:04:54,519][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 21:05:03,364][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 21:05:03,367][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 21:05:03,368][matplotlib][DEBUG] - interactive is False
[2024-09-08 21:05:03,368][matplotlib][DEBUG] - platform is linux
[2024-09-08 21:05:03,379][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 21:05:03,390][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 21:05:03,814][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:05:03,817][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:05:03,830][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 21:05:03,830][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 21:05:03,942][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 21:05:03,943][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 21:05:04,604][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 21:05:04,698][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 21:05:04,699][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 21:05:04,700][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 21:05:04,701][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 21:05:04,702][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 21:05:04,706][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 21:05:04,754][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 21:05:04,765][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 21:05:04,766][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 21:05:04,766][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 21:05:04,766][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 21:05:04,807][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:05:04,810][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:05:05,439][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 21:05:05,497][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:05:05,537][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:05:05,555][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 21:05:07,564][src.training_pipeline][INFO] - Starting training!
[2024-09-08 21:05:07,596][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 21:05:07,599][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:05:20,667][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 21:05:20,722][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 21:08:50,179][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 21:08:50,183][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 21:08:50,183][matplotlib][DEBUG] - interactive is False
[2024-09-08 21:08:50,184][matplotlib][DEBUG] - platform is linux
[2024-09-08 21:08:50,195][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 21:08:50,205][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 21:08:50,636][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:08:50,638][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:08:50,654][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 21:08:50,654][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 21:08:50,751][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 21:08:50,752][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 21:08:51,406][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-08 21:08:51,488][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 21:08:51,490][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 21:08:51,491][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 21:08:51,492][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 21:08:51,492][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 21:08:51,497][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 21:08:51,518][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 21:08:51,528][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 21:08:51,529][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 21:08:51,529][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 21:08:51,530][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 21:08:51,572][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:08:51,575][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:08:52,206][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 21:08:52,255][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:08:52,286][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:08:52,304][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 21:08:54,379][src.training_pipeline][INFO] - Starting training!
[2024-09-08 21:08:54,419][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 21:08:54,423][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:09:00,100][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v6.ckpt
[2024-09-08 21:09:00,105][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v117.ckpt
[2024-09-08 21:09:00,106][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-08 21:09:00,117][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 21:09:00,118][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v6.ckpt
[2024-09-08 21:09:00,118][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v6.ckpt
[2024-09-08 21:09:00,123][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:09:00,125][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v6.ckpt
[2024-09-08 21:09:03,384][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 21:09:09,188][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 21:09:09,243][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 21:09:09,245][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v6.ckpt
[2024-09-08 21:20:56,402][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 21:20:56,406][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 21:20:56,407][matplotlib][DEBUG] - interactive is False
[2024-09-08 21:20:56,407][matplotlib][DEBUG] - platform is linux
[2024-09-08 21:20:56,418][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 21:20:56,428][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 21:20:56,852][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:20:56,854][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:20:56,871][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 21:20:56,871][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 21:20:56,987][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 21:20:56,988][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 21:20:57,673][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-08 21:20:57,759][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 21:20:57,761][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 21:20:57,762][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 21:20:57,763][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 21:20:57,764][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 21:20:57,769][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 21:20:57,809][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 21:20:57,819][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 21:20:57,820][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 21:20:57,821][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 21:20:57,821][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 21:20:57,865][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:20:57,867][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:20:58,503][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 21:20:58,546][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:20:58,589][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:20:58,606][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 21:21:00,677][src.training_pipeline][INFO] - Starting training!
[2024-09-08 21:21:00,716][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 21:21:00,720][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:21:07,873][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 21:21:07,956][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 21:22:13,234][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 21:22:13,237][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 21:22:13,238][matplotlib][DEBUG] - interactive is False
[2024-09-08 21:22:13,238][matplotlib][DEBUG] - platform is linux
[2024-09-08 21:22:13,249][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 21:22:13,260][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 21:22:13,687][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:22:13,690][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:22:13,707][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 21:22:13,708][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 21:22:13,807][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 21:22:13,808][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 21:22:14,465][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-08 21:22:14,552][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 21:22:14,554][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 21:22:14,555][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 21:22:14,556][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 21:22:14,557][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 21:22:14,562][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 21:22:14,609][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 21:22:14,618][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 21:22:14,619][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 21:22:14,619][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 21:22:14,620][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 21:22:14,661][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:22:14,663][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:22:15,295][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 21:22:15,346][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:22:15,373][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:22:15,391][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 21:22:17,506][src.training_pipeline][INFO] - Starting training!
[2024-09-08 21:22:17,543][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 21:22:17,547][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:22:24,756][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 21:22:24,847][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 21:27:56,703][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 21:27:56,706][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 21:27:56,707][matplotlib][DEBUG] - interactive is False
[2024-09-08 21:27:56,707][matplotlib][DEBUG] - platform is linux
[2024-09-08 21:27:56,718][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 21:27:56,728][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 21:27:57,157][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:27:57,160][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:27:57,176][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 21:27:57,176][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 21:27:57,300][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 21:27:57,302][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 21:27:57,986][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-08 21:27:58,071][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 21:27:58,073][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 21:27:58,074][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 21:27:58,074][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 21:27:58,075][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 21:27:58,080][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 21:27:58,128][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 21:27:58,138][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 21:27:58,139][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 21:27:58,140][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 21:27:58,140][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 21:27:58,181][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:27:58,183][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:27:58,815][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 21:27:58,866][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:27:58,895][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:27:58,912][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 21:28:01,729][src.training_pipeline][INFO] - Starting training!
[2024-09-08 21:28:01,768][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 21:28:01,772][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:28:13,937][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v7.ckpt
[2024-09-08 21:28:13,942][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v118.ckpt
[2024-09-08 21:28:13,944][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-08 21:28:13,954][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 21:28:13,955][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v7.ckpt
[2024-09-08 21:28:13,956][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v7.ckpt
[2024-09-08 21:28:13,961][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:28:13,963][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v7.ckpt
[2024-09-08 21:28:20,850][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 21:28:21,010][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 21:31:08,461][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-08 21:31:08,464][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-08 21:31:08,465][matplotlib][DEBUG] - interactive is False
[2024-09-08 21:31:08,465][matplotlib][DEBUG] - platform is linux
[2024-09-08 21:31:08,476][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-08 21:31:08,487][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-08 21:31:08,915][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:31:08,917][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:31:08,924][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-08 21:31:08,924][wandb.docker.auth][DEBUG] - No config file found
[2024-09-08 21:31:09,008][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-08 21:31:09,009][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-08 21:31:09,665][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-08 21:31:09,748][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-08 21:31:09,750][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-08 21:31:09,751][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-08 21:31:09,752][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-08 21:31:09,752][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-08 21:31:09,757][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-08 21:31:09,807][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-08 21:31:09,818][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-08 21:31:09,819][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-08 21:31:09,819][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-08 21:31:09,820][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-08 21:31:09,862][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:31:09,865][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-08 21:31:10,503][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-08 21:31:10,575][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:31:10,608][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-08 21:31:10,627][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-08 21:31:12,699][src.training_pipeline][INFO] - Starting training!
[2024-09-08 21:31:12,738][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-08 21:31:12,743][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:31:24,273][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v8.ckpt
[2024-09-08 21:31:24,279][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/last-v119.ckpt
[2024-09-08 21:31:24,280][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=1` reached.
[2024-09-08 21:31:24,290][src.training_pipeline][INFO] - Starting testing!
[2024-09-08 21:31:24,291][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v8.ckpt
[2024-09-08 21:31:24,291][fsspec.local][DEBUG] - open file: /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v8.ckpt
[2024-09-08 21:31:24,297][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-08 21:31:24,299][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v8.ckpt
[2024-09-08 21:31:27,398][src.training_pipeline][INFO] - Finalizing!
[2024-09-08 21:31:33,597][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-08 21:31:33,650][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-08 21:31:33,652][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_000-v8.ckpt
[2024-09-09 20:16:57,774][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:16:57,777][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:16:57,778][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:16:57,778][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:16:57,789][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:16:57,800][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:16:58,231][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:16:58,233][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:16:58,239][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:16:58,240][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:16:58,324][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:16:58,326][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:16:58,988][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:16:59,076][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 20:16:59,078][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 20:16:59,079][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 20:16:59,079][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 20:16:59,080][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 20:16:59,085][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 20:16:59,135][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 20:16:59,144][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 20:16:59,145][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 20:16:59,145][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 20:16:59,146][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 20:16:59,200][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:16:59,203][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:16:59,834][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-09 20:16:59,889][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:16:59,919][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:16:59,939][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-09 20:17:01,997][src.training_pipeline][INFO] - Starting training!
[2024-09-09 20:17:02,059][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 20:17:02,062][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 20:17:21,995][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:17:21,998][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:17:21,999][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:17:21,999][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:17:22,010][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:17:22,020][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:17:22,444][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:17:22,446][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:17:22,462][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:17:22,462][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:17:22,594][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:17:22,595][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:17:23,252][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:17:23,339][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 20:17:23,341][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 20:17:23,342][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 20:17:23,343][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 20:17:23,344][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 20:17:23,349][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 20:17:23,391][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 20:17:23,402][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 20:17:23,403][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 20:17:23,403][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 20:17:23,404][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 20:17:23,451][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:17:23,454][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:17:24,091][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-09 20:17:24,135][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:17:24,178][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:17:24,214][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-09 20:17:26,282][src.training_pipeline][INFO] - Starting training!
[2024-09-09 20:17:26,332][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 20:17:26,335][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 20:19:07,660][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:19:07,663][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:19:07,664][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:19:07,664][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:19:07,676][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:19:07,687][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:19:13,479][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:19:13,482][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:19:13,483][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:19:13,483][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:19:13,494][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:19:13,505][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:19:13,936][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:19:13,938][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:19:13,944][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:19:13,944][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:19:14,048][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:19:14,050][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:19:14,716][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:19:14,813][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 20:19:14,814][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 20:19:14,815][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 20:19:14,816][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 20:19:14,816][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 20:19:14,819][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 20:19:14,870][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 20:19:14,880][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 20:19:14,882][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 20:19:14,882][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 20:19:14,882][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 20:19:14,925][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:19:14,927][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:19:15,562][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-09 20:19:15,612][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:19:15,647][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:19:15,667][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-09 20:19:17,728][src.training_pipeline][INFO] - Starting training!
[2024-09-09 20:19:17,790][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 20:19:17,793][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 20:20:43,303][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:20:43,306][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:20:43,307][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:20:43,307][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:20:43,318][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:20:43,329][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:20:43,752][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:20:43,754][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:20:43,761][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:20:43,761][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:20:43,844][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:20:43,846][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:20:44,508][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:21:01,082][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:21:01,085][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:21:01,086][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:21:01,086][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:21:01,097][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:21:01,108][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:21:01,535][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:21:01,537][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:21:01,543][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:21:01,543][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:21:01,627][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:21:01,628][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:21:02,285][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:23:36,769][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:23:36,772][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:23:36,773][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:23:36,773][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:23:36,784][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:23:36,795][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:23:37,219][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:23:37,222][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:23:37,235][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:23:37,235][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:23:37,323][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:23:37,324][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:23:37,988][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:23:38,075][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 20:23:38,077][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 20:23:38,078][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 20:23:38,079][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 20:23:38,080][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 20:23:38,085][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 20:23:38,123][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 20:23:38,134][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 20:23:38,135][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 20:23:38,135][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 20:23:38,135][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 20:23:38,177][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:23:38,179][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:23:38,598][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-09 20:23:38,648][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:23:38,680][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:23:38,697][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-09 20:23:40,820][src.training_pipeline][INFO] - Starting training!
[2024-09-09 20:23:40,868][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 20:23:40,871][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 20:25:07,971][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:25:07,975][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:25:07,975][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:25:07,976][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:25:07,986][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:25:07,997][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:25:08,424][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:25:08,427][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:25:08,442][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:25:08,442][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:25:08,575][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:25:08,576][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:25:09,234][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:25:09,309][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 20:25:09,310][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 20:25:09,311][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 20:25:09,312][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 20:25:09,313][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 20:25:09,318][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 20:25:09,368][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 20:25:09,378][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 20:25:09,379][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 20:25:09,379][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 20:25:09,380][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 20:25:09,422][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:25:09,424][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:25:10,055][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-09 20:25:10,099][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:25:10,133][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:25:10,151][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-09 20:25:12,167][src.training_pipeline][INFO] - Starting training!
[2024-09-09 20:25:12,215][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 20:25:12,218][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 20:39:12,574][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:39:12,577][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:39:12,578][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:39:12,578][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:39:12,589][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:39:12,600][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:39:13,033][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:39:13,036][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:39:13,051][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:39:13,051][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:39:13,152][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:39:13,153][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:39:13,812][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:41:42,412][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:41:42,415][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:41:42,416][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:41:42,416][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:41:42,427][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:41:42,438][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:41:42,873][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:41:42,876][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:41:42,883][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:41:42,883][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:41:42,968][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:41:42,969][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:41:43,641][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:42:20,335][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:42:20,338][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:42:20,339][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:42:20,339][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:42:20,350][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:42:20,361][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:42:20,791][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:42:20,794][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:42:20,810][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:42:20,810][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:42:20,908][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:42:20,909][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:42:21,578][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:42:54,443][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:42:54,446][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:42:54,447][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:42:54,447][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:42:54,458][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:42:54,469][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:42:54,903][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:42:54,906][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:42:54,922][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:42:54,922][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:42:55,021][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:42:55,022][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:42:55,685][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:43:12,474][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:43:12,477][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:43:12,478][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:43:12,478][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:43:12,489][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:43:12,499][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:43:12,928][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:43:12,931][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:43:12,937][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:43:12,937][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:43:13,044][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:43:13,045][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:43:13,718][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:43:13,792][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 20:43:13,794][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 20:43:13,795][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 20:43:13,796][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 20:43:13,797][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 20:43:13,802][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 20:43:13,842][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 20:43:13,845][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 20:43:13,846][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 20:43:13,846][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 20:43:13,846][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 20:43:13,882][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:43:13,886][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:43:14,321][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-09 20:43:14,375][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:43:14,403][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:43:14,421][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-09 20:43:16,545][src.training_pipeline][INFO] - Starting training!
[2024-09-09 20:43:16,608][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 20:43:16,611][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 20:43:23,741][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-09 20:43:23,793][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-09 20:49:03,449][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:49:03,452][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:49:03,453][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:49:03,453][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:49:03,464][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:49:03,475][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:49:03,907][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:49:03,910][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:49:03,916][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:49:03,916][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:49:04,021][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:49:04,023][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:49:04,705][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:49:14,354][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:49:14,357][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:49:14,358][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:49:14,358][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:49:14,370][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:49:14,380][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:49:14,813][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:49:14,816][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:49:14,823][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:49:14,823][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:49:14,926][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:49:14,927][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:49:15,590][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:49:15,666][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 20:49:15,668][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 20:49:15,669][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 20:49:15,670][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 20:49:15,671][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 20:49:15,676][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 20:49:15,714][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 20:49:15,718][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 20:49:15,718][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 20:49:15,718][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 20:49:15,718][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 20:49:15,751][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:49:15,754][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:49:16,387][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-09 20:49:16,437][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:49:16,467][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:49:16,485][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-09 20:49:18,560][src.training_pipeline][INFO] - Starting training!
[2024-09-09 20:49:18,607][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 20:49:18,609][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 20:49:25,735][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-09 20:49:25,793][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-09 20:58:34,989][matplotlib][DEBUG] - matplotlib data path: /local/scratch/a/ko120/miniconda3/envs/dm_real/lib/python3.10/site-packages/matplotlib/mpl-data
[2024-09-09 20:58:34,993][matplotlib][DEBUG] - CONFIGDIR=/local/scratch/a/ko120/.config/matplotlib
[2024-09-09 20:58:34,993][matplotlib][DEBUG] - interactive is False
[2024-09-09 20:58:34,993][matplotlib][DEBUG] - platform is linux
[2024-09-09 20:58:35,004][matplotlib][DEBUG] - CACHEDIR=/local/scratch/a/ko120/.cache/matplotlib
[2024-09-09 20:58:35,015][matplotlib.font_manager][DEBUG] - Using fontManager instance from /local/scratch/a/ko120/.cache/matplotlib/fontlist-v390.json
[2024-09-09 20:58:35,440][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:58:35,443][git.cmd][DEBUG] - Popen(['git', 'version'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:58:35,449][wandb.docker.auth][DEBUG] - Trying paths: ['/local/scratch/a/ko120/.docker/config.json', '/local/scratch/a/ko120/.dockercfg']
[2024-09-09 20:58:35,450][wandb.docker.auth][DEBUG] - No config file found
[2024-09-09 20:58:35,532][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 20:58:35,533][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 20:58:36,188][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 20:58:36,275][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 20:58:36,277][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 20:58:36,278][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 20:58:36,279][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 20:58:36,279][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 20:58:36,284][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 20:58:36,325][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 20:58:36,335][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 20:58:36,336][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 20:58:36,337][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 20:58:36,337][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 20:58:36,379][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:58:36,382][git.cmd][DEBUG] - Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=None, shell=False, universal_newlines=False)
[2024-09-09 20:58:37,019][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): api.wandb.ai:443
[2024-09-09 20:58:37,074][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:58:37,104][urllib3.connectionpool][DEBUG] - https://api.wandb.ai:443 "POST /graphql HTTP/11" 200 None
[2024-09-09 20:58:37,124][git.cmd][DEBUG] - Popen(['git', 'cat-file', '--batch-check'], cwd=/local/scratch/a/ko120/DM-Benchmark, stdin=<valid stream>, shell=False, universal_newlines=False)
[2024-09-09 20:58:39,249][src.training_pipeline][INFO] - Starting training!
[2024-09-09 20:58:39,321][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 20:58:39,327][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:00:25,378][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
[2024-09-09 21:00:25,426][urllib3.connectionpool][DEBUG] - https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/11" 200 0
[2024-09-09 21:00:43,098][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:00:43,099][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:00:43,656][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:00:43,742][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:00:43,744][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:00:43,745][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:00:43,746][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:00:43,746][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:00:43,751][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:00:43,773][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:00:43,784][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:00:43,785][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:00:43,785][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:00:43,786][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:00:46,890][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:00:46,955][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:00:46,959][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:05:19,881][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-09 21:05:19,895][src.training_pipeline][INFO] - Starting testing!
[2024-09-09 21:05:19,896][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v25.ckpt
[2024-09-09 21:05:19,899][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:05:19,904][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v25.ckpt
[2024-09-09 21:05:26,167][src.training_pipeline][INFO] - Finalizing!
[2024-09-09 21:05:36,045][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v25.ckpt
[2024-09-09 21:10:37,515][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:10:37,516][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:10:38,068][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:10:38,153][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:10:38,155][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:10:38,156][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:10:38,156][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:10:38,157][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:10:38,162][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:10:38,214][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:10:38,224][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:10:38,225][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:10:38,225][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:10:38,225][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:10:41,070][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:10:41,129][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:10:41,132][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:17:06,529][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:17:06,530][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:17:07,092][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:17:07,194][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:17:07,196][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:17:07,197][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:17:07,198][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:17:07,198][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:17:07,201][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:17:07,249][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:17:07,260][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:17:07,261][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:17:07,261][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:17:07,261][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:17:10,080][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:17:10,141][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:17:10,144][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:20:24,841][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:20:24,843][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:20:25,400][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:20:25,487][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:20:25,489][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:20:25,490][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:20:25,490][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:20:25,491][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:20:25,496][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:20:25,537][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:20:25,548][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:20:25,549][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:20:25,549][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:20:25,550][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:20:31,569][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:20:31,620][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:20:31,622][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:26:00,052][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:26:00,054][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:26:00,612][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:26:00,697][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:26:00,699][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:26:00,700][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:26:00,701][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:26:00,701][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:26:00,706][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:26:00,750][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:26:00,760][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:26:00,761][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:26:00,761][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:26:00,761][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:26:03,284][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:26:03,336][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:26:03,339][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:26:14,655][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:26:14,656][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:26:15,208][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:26:15,294][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:26:15,296][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:26:15,297][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:26:15,297][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:26:15,298][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:26:15,303][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:26:15,325][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:26:15,335][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:26:15,336][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:26:15,336][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:26:15,337][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:26:17,995][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:26:18,042][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:26:18,045][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:35:47,339][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:35:47,340][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:35:47,900][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:35:47,987][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:35:47,989][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:35:47,990][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:35:47,990][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:35:47,991][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:35:47,996][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:35:48,038][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:35:48,048][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:35:48,050][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:35:48,050][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:35:48,050][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:35:50,972][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:35:51,019][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:35:51,022][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:36:17,000][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:36:17,002][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:36:17,560][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:36:17,645][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:36:17,647][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:36:17,648][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:36:17,649][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:36:17,649][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:36:17,654][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:36:17,697][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:36:17,704][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:36:17,705][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:36:17,705][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:36:17,706][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:36:20,561][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:36:20,623][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:36:20,627][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:36:49,680][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:36:49,681][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:36:50,245][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:36:50,330][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:36:50,332][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:36:50,333][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:36:50,334][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:36:50,334][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:36:50,339][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:36:50,378][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:36:50,382][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:36:50,382][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:36:50,383][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:36:50,383][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:36:52,987][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:36:53,046][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:36:53,049][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:38:51,674][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:38:51,675][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:38:52,229][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:38:52,314][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:38:52,316][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:38:52,317][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:38:52,318][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:38:52,319][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:38:52,325][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:38:52,374][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:38:52,384][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:38:52,385][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:38:52,385][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:38:52,386][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:38:55,147][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:38:55,207][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:38:55,210][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:40:48,597][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:40:48,599][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:40:49,180][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:40:49,266][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:40:49,268][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:40:49,269][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:40:49,269][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:40:49,270][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:40:49,275][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:40:49,326][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:40:49,336][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:40:49,337][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:40:49,337][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:40:49,338][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:40:51,930][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:40:51,980][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:40:51,982][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-09 21:41:21,327][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-09 21:41:21,329][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-09 21:41:21,896][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-09 21:41:21,980][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-09 21:41:21,981][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-09 21:41:21,982][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-09 21:41:21,983][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-09 21:41:21,984][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-09 21:41:21,989][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-09 21:41:22,011][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-09 21:41:22,021][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-09 21:41:22,022][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-09 21:41:22,022][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-09 21:41:22,023][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-09 21:41:24,979][src.training_pipeline][INFO] - Starting training!
[2024-09-09 21:41:25,028][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-09 21:41:25,030][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 16:55:11,982][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 16:55:12,051][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 16:55:15,777][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 16:55:17,080][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 16:55:17,084][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 16:55:17,086][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 16:55:17,087][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 16:55:17,089][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 16:55:17,097][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 16:55:17,150][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 16:55:17,174][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 16:55:17,175][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 16:55:17,176][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 16:55:17,176][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 16:55:22,456][src.training_pipeline][INFO] - Starting training!
[2024-09-27 16:55:22,484][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 16:55:22,486][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 16:57:01,348][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 16:57:01,350][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 16:57:02,182][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 16:57:02,418][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 16:57:02,419][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 16:57:02,419][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 16:57:02,420][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 16:57:02,420][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 16:57:02,424][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 16:57:02,466][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 16:57:02,469][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 16:57:02,470][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 16:57:02,470][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 16:57:02,470][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 16:57:05,434][src.training_pipeline][INFO] - Starting training!
[2024-09-27 16:57:05,469][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 16:57:05,471][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 16:57:58,163][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 16:57:58,165][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 16:57:59,174][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 16:57:59,381][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 16:57:59,382][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 16:57:59,383][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 16:57:59,383][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 16:57:59,384][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 16:57:59,387][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 16:57:59,430][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 16:57:59,439][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 16:57:59,440][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 16:57:59,441][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 16:57:59,441][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 16:58:03,081][src.training_pipeline][INFO] - Starting training!
[2024-09-27 16:58:03,111][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 16:58:03,112][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 16:58:58,347][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 16:58:58,349][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 16:58:59,142][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 16:58:59,383][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 16:58:59,385][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 16:58:59,385][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 16:58:59,386][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 16:58:59,386][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 16:58:59,389][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 16:58:59,443][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 16:58:59,453][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 16:58:59,455][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 16:58:59,455][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 16:58:59,455][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 16:59:03,249][src.training_pipeline][INFO] - Starting training!
[2024-09-27 16:59:03,287][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 16:59:03,289][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:00:08,320][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:00:08,322][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:00:09,046][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:00:35,371][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:00:35,372][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:00:35,373][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:00:35,373][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:00:35,374][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:00:35,379][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:00:35,433][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:00:35,436][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:00:35,437][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:00:35,437][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:00:35,437][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:00:38,416][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:00:38,453][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:00:38,455][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:01:11,534][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:01:11,536][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:01:12,210][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:01:12,437][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:01:12,438][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:01:12,439][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:01:12,439][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:01:12,440][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:01:12,443][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:01:12,495][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:01:12,505][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:01:12,506][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:01:12,506][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:01:12,507][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:01:15,597][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:01:15,628][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:01:15,629][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:02:46,288][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:02:46,289][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:02:47,111][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:03:32,856][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:03:32,858][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:03:33,522][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:03:33,729][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:03:33,730][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:03:33,731][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:03:33,731][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:03:33,732][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:03:33,735][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:03:33,776][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:03:33,779][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:03:33,780][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:03:33,780][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:03:33,780][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:03:47,911][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:03:47,957][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:03:47,959][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:12:39,821][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:12:39,823][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:12:40,666][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:12:40,914][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:12:40,916][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:12:40,916][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:12:40,917][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:12:40,917][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:12:40,920][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:12:40,960][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:12:40,968][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:12:40,970][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:12:40,970][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:12:40,970][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:12:43,851][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:12:43,892][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:12:43,894][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:14:07,704][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:14:07,706][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:14:08,354][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:14:08,583][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:14:08,584][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:14:08,585][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:14:08,585][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:14:08,586][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:14:08,589][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:14:08,644][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:14:08,654][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:14:08,655][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:14:08,655][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:14:08,655][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:14:11,569][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:14:11,607][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:14:11,609][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:14:45,290][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:14:45,291][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:14:46,074][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:14:46,302][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:14:46,303][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:14:46,304][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:14:46,304][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:14:46,305][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:14:46,308][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:14:46,350][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:14:46,356][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:14:46,357][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:14:46,357][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:14:46,358][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:14:49,178][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:14:49,214][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:14:49,215][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:15:16,178][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:15:16,179][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:15:16,814][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:15:17,032][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:15:17,033][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:15:17,034][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:15:17,034][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:15:17,035][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:15:17,038][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:15:17,078][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:15:17,084][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:15:17,085][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:15:17,085][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:15:17,086][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:15:20,305][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:15:20,335][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:15:20,336][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:17:33,596][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-27 17:17:33,610][src.training_pipeline][INFO] - Starting testing!
[2024-09-27 17:17:33,611][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008.ckpt
[2024-09-27 17:17:33,616][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:17:33,621][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008.ckpt
[2024-09-27 17:18:19,490][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:18:19,491][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:18:20,267][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:18:20,501][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:18:20,502][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:18:20,503][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:18:20,503][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:18:20,503][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:18:20,507][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:18:20,549][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:18:20,554][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:18:20,554][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:18:20,555][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:18:20,555][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:18:23,434][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:18:23,463][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:18:23,465][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:20:34,431][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-27 17:20:34,445][src.training_pipeline][INFO] - Starting testing!
[2024-09-27 17:20:34,446][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v1.ckpt
[2024-09-27 17:20:34,452][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:20:34,456][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v1.ckpt
[2024-09-27 17:20:37,447][src.training_pipeline][INFO] - Finalizing!
[2024-09-27 17:20:45,806][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v1.ckpt
[2024-09-27 17:21:48,508][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:21:48,509][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:21:49,206][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:21:49,430][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:21:49,431][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:21:49,432][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:21:49,432][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:21:49,433][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:21:49,436][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:21:49,483][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:21:49,487][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:21:49,488][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:21:49,488][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:21:49,488][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:21:52,394][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:21:52,429][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:21:52,431][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:22:18,830][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 17:22:18,831][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 17:22:19,650][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 17:22:19,890][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 17:22:19,891][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 17:22:19,892][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 17:22:19,892][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 17:22:19,893][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 17:22:19,896][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 17:22:19,917][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 17:22:19,921][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 17:22:19,921][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 17:22:19,921][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 17:22:19,922][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 17:22:25,194][src.training_pipeline][INFO] - Starting training!
[2024-09-27 17:22:25,230][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 17:22:25,232][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:26:03,979][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-27 17:26:03,993][src.training_pipeline][INFO] - Starting testing!
[2024-09-27 17:26:03,994][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v2.ckpt
[2024-09-27 17:26:03,999][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 17:26:04,003][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v2.ckpt
[2024-09-27 17:26:07,023][src.training_pipeline][INFO] - Finalizing!
[2024-09-27 17:27:19,589][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v2.ckpt
[2024-09-27 18:31:44,457][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 18:31:44,459][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 18:31:45,042][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 18:31:45,146][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 18:31:45,147][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 18:31:45,148][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 18:31:45,148][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 18:31:45,149][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 18:31:45,152][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 18:31:45,193][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 18:31:45,200][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 18:31:45,201][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 18:31:45,201][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 18:31:45,201][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 18:31:48,162][src.training_pipeline][INFO] - Starting training!
[2024-09-27 18:31:48,203][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 18:31:48,205][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 18:56:36,267][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 18:56:36,268][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 18:56:36,857][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 18:56:36,939][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 18:56:36,941][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 18:56:36,942][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 18:56:36,943][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 18:56:36,944][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 18:56:36,949][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 18:56:36,989][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 18:56:36,996][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 18:56:36,997][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 18:56:36,997][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 18:56:36,998][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 18:56:39,836][src.training_pipeline][INFO] - Starting training!
[2024-09-27 18:56:39,879][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 18:56:39,882][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 18:57:55,445][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 18:57:55,447][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 18:57:56,070][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 18:57:56,168][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 18:57:56,170][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 18:57:56,171][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 18:57:56,171][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 18:57:56,172][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 18:57:56,175][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 18:57:56,219][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 18:57:56,228][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 18:57:56,229][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 18:57:56,230][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 18:57:56,230][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 18:57:59,250][src.training_pipeline][INFO] - Starting training!
[2024-09-27 18:57:59,296][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 18:57:59,298][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:00:45,450][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:00:45,451][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:00:46,033][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:00:46,110][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:00:46,112][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:00:46,113][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:00:46,114][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:00:46,115][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:00:46,120][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:00:46,144][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:00:46,148][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:00:46,148][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:00:46,149][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:00:46,149][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:00:49,007][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:00:49,057][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:00:49,059][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:02:01,975][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:02:01,976][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:02:02,573][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:02:02,648][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:02:02,650][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:02:02,651][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:02:02,652][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:02:02,652][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:02:02,657][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:02:02,698][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:02:02,702][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:02:02,703][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:02:02,703][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:02:02,703][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:02:05,583][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:02:05,625][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:02:05,626][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:04:08,408][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:04:08,409][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:04:08,995][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:04:09,099][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:04:09,101][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:04:09,101][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:04:09,102][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:04:09,102][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:04:09,106][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:04:09,152][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:04:09,162][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:04:09,163][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:04:09,163][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:04:09,164][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:04:12,019][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:04:12,070][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:04:12,072][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:22:25,324][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:22:25,326][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:22:25,918][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:22:26,021][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:22:26,022][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:22:26,023][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:22:26,023][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:22:26,024][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:22:26,028][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:22:26,047][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:22:26,051][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:22:26,052][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:22:26,052][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:22:26,052][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:22:28,993][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:22:29,036][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:22:29,037][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:26:39,284][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:26:39,285][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:26:39,857][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:26:39,941][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:26:39,943][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:26:39,944][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:26:39,944][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:26:39,945][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:26:39,949][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:26:39,987][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:26:39,992][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:26:39,992][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:26:39,992][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:26:39,993][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:26:42,986][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:26:43,027][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:26:43,029][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:27:09,719][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:27:09,720][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:27:10,298][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:27:10,403][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:27:10,404][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:27:10,405][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:27:10,405][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:27:10,406][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:27:10,409][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:27:10,451][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:27:10,458][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:27:10,459][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:27:10,459][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:27:10,459][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:27:13,640][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:27:13,692][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:27:13,693][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:27:46,300][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:27:46,301][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:27:46,882][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:27:46,966][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:27:46,968][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:27:46,969][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:27:46,969][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:27:46,970][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:27:46,974][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:27:47,017][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:27:47,023][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:27:47,024][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:27:47,024][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:27:47,025][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:27:49,898][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:27:49,940][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:27:49,941][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:31:49,585][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-27 19:31:49,598][src.training_pipeline][INFO] - Starting testing!
[2024-09-27 19:31:49,599][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v3.ckpt
[2024-09-27 19:31:49,605][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:31:49,609][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v3.ckpt
[2024-09-27 19:31:52,600][src.training_pipeline][INFO] - Finalizing!
[2024-09-27 19:31:59,730][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v3.ckpt
[2024-09-27 19:33:53,225][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:33:53,226][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:33:53,810][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:33:53,910][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:33:53,911][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:33:53,912][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:33:53,912][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:33:53,913][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:33:53,916][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:33:53,958][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:33:53,965][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:33:53,966][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:33:53,966][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:33:53,967][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:33:56,799][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:33:56,839][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:33:56,840][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:37:37,188][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-27 19:37:37,202][src.training_pipeline][INFO] - Starting testing!
[2024-09-27 19:37:37,203][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v4.ckpt
[2024-09-27 19:37:37,208][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:37:37,212][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v4.ckpt
[2024-09-27 19:37:40,201][src.training_pipeline][INFO] - Finalizing!
[2024-09-27 19:37:46,333][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v4.ckpt
[2024-09-27 19:44:03,849][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:44:03,851][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:44:04,430][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:44:04,535][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:44:04,536][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:44:04,537][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:44:04,537][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:44:04,538][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:44:04,541][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:44:04,581][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:44:04,588][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:44:04,589][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:44:04,589][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:44:04,590][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:44:07,525][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:44:07,567][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:44:07,568][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:47:46,581][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-27 19:47:46,594][src.training_pipeline][INFO] - Starting testing!
[2024-09-27 19:47:46,595][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v5.ckpt
[2024-09-27 19:47:46,600][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:47:46,608][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v5.ckpt
[2024-09-27 19:47:49,590][src.training_pipeline][INFO] - Finalizing!
[2024-09-27 19:47:55,092][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v5.ckpt
[2024-09-27 19:48:20,813][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:48:20,814][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:48:21,394][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:51:08,919][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 19:51:08,920][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 19:51:09,502][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 19:51:09,597][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 19:51:09,599][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 19:51:09,600][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 19:51:09,600][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 19:51:09,601][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 19:51:09,604][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 19:51:09,645][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 19:51:09,651][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 19:51:09,652][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 19:51:09,652][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 19:51:09,653][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 19:51:12,656][src.training_pipeline][INFO] - Starting training!
[2024-09-27 19:51:12,708][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 19:51:12,709][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:55:13,383][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-27 19:55:13,397][src.training_pipeline][INFO] - Starting testing!
[2024-09-27 19:55:13,397][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v6.ckpt
[2024-09-27 19:55:13,403][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 19:55:13,407][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v6.ckpt
[2024-09-27 19:55:17,022][src.training_pipeline][INFO] - Finalizing!
[2024-09-27 19:55:22,495][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v6.ckpt
[2024-09-27 21:20:52,997][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:20:53,028][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:20:54,724][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:20:55,090][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:20:55,092][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:20:55,093][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:20:55,094][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:20:55,094][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:20:55,097][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:20:55,120][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:20:55,131][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:20:55,132][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:20:55,132][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:20:55,132][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:20:59,179][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:20:59,234][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:20:59,235][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:21:58,489][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:21:58,492][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:21:59,553][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:21:59,638][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:21:59,640][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:21:59,641][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:21:59,641][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:21:59,642][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:21:59,647][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:21:59,697][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:21:59,707][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:21:59,708][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:21:59,709][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:21:59,709][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:22:02,894][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:22:02,945][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:22:02,946][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:24:01,443][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:24:01,444][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:24:02,333][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:24:02,408][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:24:02,410][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:24:02,411][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:24:02,412][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:24:02,413][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:24:02,418][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:24:02,471][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:24:02,481][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:24:02,481][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:24:02,482][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:24:02,482][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:24:05,521][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:24:05,574][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:24:05,575][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:24:35,733][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:24:35,735][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:24:36,569][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:24:36,656][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:24:36,658][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:24:36,659][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:24:36,660][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:24:36,661][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:24:36,666][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:24:36,718][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:24:36,728][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:24:36,729][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:24:36,729][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:24:36,730][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:24:39,565][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:24:39,627][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:24:39,629][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:25:36,144][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:25:36,145][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:25:37,033][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:25:37,134][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:25:37,135][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:25:37,137][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:25:37,137][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:25:37,138][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:25:37,141][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:25:37,190][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:25:37,200][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:25:37,202][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:25:37,202][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:25:37,203][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:25:40,129][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:25:40,180][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:25:40,181][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:29:20,318][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-09-27 21:29:20,332][src.training_pipeline][INFO] - Starting testing!
[2024-09-27 21:29:20,332][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v7.ckpt
[2024-09-27 21:29:20,338][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:29:20,342][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v7.ckpt
[2024-09-27 21:29:23,344][src.training_pipeline][INFO] - Finalizing!
[2024-09-27 21:29:29,484][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v7.ckpt
[2024-09-27 21:34:37,507][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:34:37,508][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:34:38,497][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:34:38,584][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:34:38,588][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:34:38,589][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:34:38,589][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:34:38,590][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:34:38,595][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:34:38,616][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:34:38,623][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:34:38,624][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:34:38,624][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:34:38,625][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:34:41,508][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:34:41,572][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:34:41,574][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:49:53,555][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:49:53,556][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:49:54,250][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:49:54,326][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:49:54,328][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:49:54,329][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:49:54,329][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:49:54,330][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:49:54,335][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:49:54,385][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:49:54,396][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:49:54,397][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:49:54,397][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:49:54,398][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:49:57,324][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:49:57,390][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:49:57,391][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:50:27,823][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:50:27,824][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:50:28,797][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:50:28,882][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:50:28,884][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:50:28,885][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:50:28,885][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:50:28,886][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:50:28,891][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:50:28,942][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:50:28,952][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:50:28,953][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:50:28,953][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:50:28,954][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:50:32,984][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:50:33,050][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:50:33,052][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:51:47,183][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:51:47,185][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:51:48,069][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 21:51:48,154][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 21:51:48,156][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 21:51:48,157][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 21:51:48,158][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 21:51:48,158][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 21:51:48,163][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 21:51:48,211][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 21:51:48,217][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 21:51:48,217][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 21:51:48,217][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 21:51:48,218][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 21:51:50,882][src.training_pipeline][INFO] - Starting training!
[2024-09-27 21:51:50,945][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 21:51:50,947][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 21:57:07,072][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:57:07,074][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:57:21,657][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:57:21,659][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 21:57:47,572][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 21:57:47,573][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 22:00:17,122][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 22:00:17,123][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 22:00:19,716][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 22:00:19,738][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 22:00:19,740][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 22:00:19,741][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 22:00:19,741][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 22:00:19,742][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 22:00:19,746][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 22:00:19,799][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 22:00:19,809][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 22:00:19,810][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 22:00:19,810][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 22:00:19,810][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 22:00:22,529][src.training_pipeline][INFO] - Starting training!
[2024-09-27 22:00:22,594][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 22:00:22,596][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 22:00:58,726][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 22:00:58,728][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 22:00:59,648][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 22:00:59,671][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 22:00:59,673][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 22:00:59,674][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 22:00:59,674][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 22:00:59,675][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 22:00:59,678][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 22:00:59,700][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 22:00:59,711][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 22:00:59,712][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 22:00:59,712][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 22:00:59,712][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 22:01:02,633][src.training_pipeline][INFO] - Starting training!
[2024-09-27 22:01:02,707][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 22:01:02,709][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 22:01:31,895][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 22:01:31,896][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 22:01:32,817][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 22:01:32,840][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 22:01:32,842][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 22:01:32,843][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 22:01:32,843][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 22:01:32,844][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 22:01:32,847][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 22:01:32,868][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 22:01:32,879][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 22:01:32,880][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 22:01:32,880][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 22:01:32,880][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 22:01:35,869][src.training_pipeline][INFO] - Starting training!
[2024-09-27 22:01:35,931][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 22:01:35,933][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 22:03:12,752][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 22:03:12,753][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 22:03:13,672][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 22:03:13,696][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 22:03:13,698][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 22:03:13,698][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 22:03:13,699][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 22:03:13,700][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 22:03:13,703][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 22:03:13,725][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 22:03:13,735][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 22:03:13,736][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 22:03:13,736][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 22:03:13,737][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 22:03:16,605][src.training_pipeline][INFO] - Starting training!
[2024-09-27 22:03:16,672][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 22:03:16,674][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 22:04:39,328][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 22:04:39,330][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 22:04:40,245][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 22:06:13,956][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 22:06:13,957][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-27 22:06:14,868][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-09-27 22:06:14,891][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-27 22:06:14,892][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-27 22:06:14,893][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-27 22:06:14,893][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-27 22:06:14,894][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-27 22:06:14,897][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-27 22:06:14,947][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-27 22:06:14,956][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-27 22:06:14,957][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-27 22:06:14,957][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-27 22:06:14,958][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-27 22:06:18,358][src.training_pipeline][INFO] - Starting training!
[2024-09-27 22:06:18,410][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-27 22:06:18,412][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-27 22:10:45,739][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-27 22:10:45,741][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 20:15:57,603][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 20:15:57,657][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 20:21:09,821][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 20:21:09,822][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 20:22:34,962][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 20:22:35,032][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 21:55:09,006][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 21:55:09,007][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 21:55:09,956][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 21:55:20,885][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 21:55:20,887][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 21:55:21,897][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:21:34,603][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:21:34,604][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:21:35,586][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:22:01,647][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:22:01,649][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:22:02,607][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:22:02,689][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:22:02,691][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:22:02,692][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:22:02,693][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:22:02,694][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:22:02,714][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:22:02,770][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:22:02,796][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:22:02,797][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:22:02,798][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:22:02,798][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:22:11,823][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:23:06,267][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:23:06,268][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:23:07,246][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:23:07,258][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:23:07,259][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:23:07,260][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:23:07,260][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:23:07,261][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:23:07,264][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:23:07,315][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:23:07,325][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:23:07,326][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:23:07,326][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:23:07,326][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:23:10,350][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:23:10,419][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:23:10,420][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:25:34,957][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:25:34,958][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:25:35,939][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:25:35,953][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:25:35,954][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:25:35,955][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:25:35,955][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:25:35,955][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:25:35,959][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:25:35,979][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:25:35,982][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:25:35,983][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:25:35,983][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:25:35,983][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:25:38,835][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:25:38,896][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:25:38,898][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:25:50,486][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:25:50,487][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:25:51,467][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:25:51,481][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:25:51,482][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:25:51,482][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:25:51,483][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:25:51,483][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:25:51,486][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:25:51,506][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:25:51,511][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:25:51,511][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:25:51,511][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:25:51,511][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:25:54,503][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:25:54,556][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:25:54,558][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:26:59,271][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:26:59,272][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:27:00,188][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:27:00,201][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:27:00,202][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:27:00,203][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:27:00,203][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:27:00,204][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:27:00,207][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:27:00,228][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:27:00,238][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:27:00,238][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:27:00,239][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:27:00,239][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:27:03,222][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:27:03,286][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:27:03,288][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:27:59,505][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:27:59,507][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:28:00,423][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:28:00,436][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:28:00,437][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:28:00,438][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:28:00,438][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:28:00,439][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:28:00,442][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:28:00,463][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:28:00,474][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:28:00,475][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:28:00,475][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:28:00,475][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:28:03,433][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:28:03,496][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:28:03,497][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:28:53,592][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:28:53,595][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:28:54,515][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:28:54,529][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:28:54,530][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:28:54,531][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:28:54,531][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:28:54,531][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:28:54,535][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:28:54,587][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:28:54,597][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:28:54,598][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:28:54,598][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:28:54,599][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:28:57,539][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:28:57,590][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:28:57,594][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:35:03,267][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:35:03,268][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:35:04,189][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:35:04,203][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:35:04,204][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:35:04,205][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:35:04,205][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:35:04,206][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:35:04,209][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:35:04,258][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:35:04,268][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:35:04,269][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:35:04,270][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:35:04,270][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:35:07,031][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:35:07,107][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:35:07,110][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:39:19,903][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:39:19,904][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:39:20,824][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:39:20,839][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:39:20,840][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:39:20,840][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:39:20,841][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:39:20,841][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:39:20,844][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:39:20,896][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:39:20,905][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:39:20,906][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:39:20,906][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:39:20,907][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:39:23,621][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:39:23,682][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:39:23,685][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:41:33,695][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:41:33,696][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:41:34,618][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:41:34,632][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:41:34,633][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:41:34,634][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:41:34,634][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:41:34,635][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:41:34,638][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:41:34,659][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:41:34,670][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:41:34,670][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:41:34,670][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:41:34,671][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:41:37,584][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:41:37,656][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:41:37,659][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:56:59,924][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:56:59,925][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:57:00,854][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:57:00,881][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:57:00,883][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:57:00,884][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:57:00,884][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:57:00,885][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:57:00,888][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:57:00,939][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:57:00,948][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:57:00,949][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:57:00,949][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:57:00,950][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:57:03,810][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:57:03,873][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:57:03,874][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:58:12,376][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:58:12,377][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:58:13,289][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:58:13,302][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:58:13,303][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:58:13,304][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:58:13,304][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:58:13,304][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:58:13,307][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:58:13,329][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:58:13,339][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:58:13,340][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:58:13,341][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:58:13,341][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:58:16,219][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:58:16,268][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:58:16,269][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 22:59:16,852][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 22:59:16,854][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 22:59:17,772][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 22:59:17,786][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 22:59:17,787][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 22:59:17,787][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 22:59:17,788][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 22:59:17,788][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 22:59:17,791][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 22:59:17,812][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 22:59:17,822][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 22:59:17,823][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 22:59:17,823][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 22:59:17,824][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 22:59:20,511][src.training_pipeline][INFO] - Starting training!
[2024-09-29 22:59:20,562][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 22:59:20,563][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:08:26,160][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 23:08:26,161][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 23:08:27,074][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 23:08:27,087][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 23:08:27,089][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 23:08:27,089][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 23:08:27,089][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 23:08:27,090][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 23:08:27,093][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 23:08:27,114][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 23:08:27,124][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 23:08:27,125][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 23:08:27,125][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 23:08:27,125][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 23:08:30,042][src.training_pipeline][INFO] - Starting training!
[2024-09-29 23:08:30,104][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 23:08:30,106][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:08:49,557][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 23:08:49,559][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 23:08:50,481][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 23:08:50,495][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 23:08:50,496][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 23:08:50,497][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 23:08:50,497][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 23:08:50,498][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 23:08:50,501][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 23:08:50,522][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 23:08:50,532][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 23:08:50,533][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 23:08:50,533][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 23:08:50,533][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 23:08:53,528][src.training_pipeline][INFO] - Starting training!
[2024-09-29 23:08:53,596][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 23:08:53,598][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:10:34,168][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 23:10:34,170][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 23:10:35,089][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 23:10:35,103][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 23:10:35,104][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 23:10:35,105][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 23:10:35,105][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 23:10:35,105][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 23:10:35,108][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 23:10:35,130][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 23:10:35,140][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 23:10:35,141][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 23:10:35,141][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 23:10:35,141][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 23:10:38,006][src.training_pipeline][INFO] - Starting training!
[2024-09-29 23:10:38,067][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 23:10:38,069][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:13:39,751][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 23:13:39,752][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 23:13:40,673][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 23:13:40,687][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 23:13:40,688][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 23:13:40,689][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 23:13:40,689][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 23:13:40,690][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 23:13:40,693][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 23:13:40,714][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 23:13:40,724][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 23:13:40,725][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 23:13:40,725][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 23:13:40,726][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 23:13:43,348][src.training_pipeline][INFO] - Starting training!
[2024-09-29 23:13:43,413][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 23:13:43,415][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:14:45,183][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 23:14:45,184][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 23:14:46,097][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 23:14:46,111][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 23:14:46,112][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 23:14:46,112][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 23:14:46,113][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 23:14:46,113][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 23:14:46,116][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 23:14:46,137][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 23:14:46,147][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 23:14:46,147][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 23:14:46,148][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 23:14:46,148][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 23:14:49,104][src.training_pipeline][INFO] - Starting training!
[2024-09-29 23:14:49,165][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 23:14:49,168][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:17:20,827][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 23:17:20,828][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 23:17:21,743][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 23:17:21,757][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 23:17:21,758][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 23:17:21,759][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 23:17:21,759][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 23:17:21,759][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 23:17:21,763][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 23:17:21,784][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 23:17:21,794][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 23:17:21,795][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 23:17:21,795][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 23:17:21,795][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 23:17:24,846][src.training_pipeline][INFO] - Starting training!
[2024-09-29 23:17:24,904][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 23:17:24,906][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:19:03,112][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-09-29 23:19:03,113][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-09-29 23:19:04,027][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-09-29 23:19:04,041][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-09-29 23:19:04,042][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-09-29 23:19:04,042][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-09-29 23:19:04,043][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-09-29 23:19:04,043][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-09-29 23:19:04,046][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-09-29 23:19:04,094][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-09-29 23:19:04,104][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-09-29 23:19:04,106][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-09-29 23:19:04,106][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-09-29 23:19:04,106][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-09-29 23:19:07,046][src.training_pipeline][INFO] - Starting training!
[2024-09-29 23:19:07,105][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-09-29 23:19:07,106][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:22:24,471][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-09-29 23:22:24,485][src.training_pipeline][INFO] - Starting testing!
[2024-09-29 23:22:24,486][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029.ckpt
[2024-09-29 23:22:24,491][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-09-29 23:22:24,496][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029.ckpt
[2024-09-29 23:22:27,691][src.training_pipeline][INFO] - Finalizing!
[2024-09-29 23:22:33,725][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029.ckpt
[2024-10-04 12:29:58,194][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 12:29:58,285][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 12:30:04,953][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 12:30:05,080][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 12:30:05,083][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 12:30:05,085][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 12:30:05,086][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 12:30:05,088][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 12:30:05,096][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 12:30:05,150][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 12:30:05,179][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 12:30:05,181][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 12:30:05,181][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 12:30:05,182][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 12:30:15,600][src.training_pipeline][INFO] - Starting training!
[2024-10-04 12:30:15,632][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 12:30:15,634][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 12:30:57,072][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 12:30:57,073][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 12:31:10,628][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 12:31:10,643][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 12:31:10,644][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 12:31:10,645][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 12:31:10,645][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 12:31:10,646][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 12:31:10,649][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 12:31:10,692][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 12:31:10,697][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 12:31:10,697][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 12:31:10,697][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 12:31:10,698][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 12:31:14,136][src.training_pipeline][INFO] - Starting training!
[2024-10-04 12:31:14,186][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 12:31:14,188][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 12:35:27,687][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-04 12:35:27,728][src.training_pipeline][INFO] - Starting testing!
[2024-10-04 12:35:27,729][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v1.ckpt
[2024-10-04 12:35:27,738][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 12:35:27,808][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v1.ckpt
[2024-10-04 12:35:32,628][src.training_pipeline][INFO] - Finalizing!
[2024-10-04 12:35:38,294][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v1.ckpt
[2024-10-04 12:37:05,569][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 12:37:05,570][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 12:37:12,265][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 12:37:12,282][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 12:37:12,283][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 12:37:12,284][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 12:37:12,284][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 12:37:12,285][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 12:37:12,288][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 12:37:12,330][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 12:37:12,339][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 12:37:12,340][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 12:37:12,340][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 12:37:12,340][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 12:37:15,432][src.training_pipeline][INFO] - Starting training!
[2024-10-04 12:37:15,471][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 12:37:15,473][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 12:41:34,121][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-04 12:41:34,137][src.training_pipeline][INFO] - Starting testing!
[2024-10-04 12:41:34,137][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v2.ckpt
[2024-10-04 12:41:34,146][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 12:41:34,153][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v2.ckpt
[2024-10-04 12:41:39,703][src.training_pipeline][INFO] - Finalizing!
[2024-10-04 12:41:45,628][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v2.ckpt
[2024-10-04 13:01:20,952][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 13:01:20,953][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 13:01:23,352][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 13:01:23,368][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 13:01:23,369][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 13:01:23,370][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 13:01:23,370][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 13:01:23,371][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 13:01:23,374][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 13:01:23,423][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 13:01:23,433][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 13:01:23,434][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 13:01:23,434][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 13:01:23,434][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 13:01:26,534][src.training_pipeline][INFO] - Starting training!
[2024-10-04 13:01:26,563][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 13:01:26,565][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 13:02:06,369][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 13:02:06,370][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 13:02:08,539][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 13:02:08,551][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 13:02:08,553][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 13:02:08,553][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 13:02:08,555][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 13:02:08,556][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 13:02:08,559][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 13:02:08,611][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 13:02:08,622][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 13:02:08,623][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 13:02:08,624][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 13:02:08,624][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 13:02:11,713][src.training_pipeline][INFO] - Starting training!
[2024-10-04 13:02:11,745][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 13:02:11,748][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 13:02:44,969][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 13:02:44,970][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 13:02:47,155][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 13:02:47,171][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 13:02:47,172][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 13:02:47,173][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 13:02:47,173][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 13:02:47,173][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 13:02:47,177][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 13:02:47,221][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 13:02:47,225][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 13:02:47,226][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 13:02:47,226][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 13:02:47,226][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 13:02:50,349][src.training_pipeline][INFO] - Starting training!
[2024-10-04 13:02:50,387][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 13:02:50,389][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 13:03:19,670][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 13:03:19,672][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 13:03:21,619][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 13:03:21,636][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 13:03:21,637][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 13:03:21,638][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 13:03:21,638][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 13:03:21,639][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 13:03:21,642][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 13:03:21,681][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 13:03:21,689][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 13:03:21,690][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 13:03:21,690][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 13:03:21,691][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 13:03:24,809][src.training_pipeline][INFO] - Starting training!
[2024-10-04 13:03:24,841][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 13:03:24,843][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 13:10:46,266][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 13:10:46,268][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 13:10:54,928][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 13:10:54,944][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 13:10:54,945][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 13:10:54,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 13:10:54,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 13:10:54,947][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 13:10:54,950][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 13:10:55,002][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 13:10:55,012][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 13:10:55,013][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 13:10:55,014][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 13:10:55,014][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 13:10:58,057][src.training_pipeline][INFO] - Starting training!
[2024-10-04 13:10:58,087][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 13:10:58,089][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 13:13:31,054][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 13:13:31,056][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 13:13:35,621][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 13:13:35,635][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 13:13:35,637][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 13:13:35,637][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 13:13:35,638][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 13:13:35,638][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 13:13:35,642][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 13:13:35,691][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 13:13:35,695][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 13:13:35,696][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 13:13:35,696][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 13:13:35,696][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 13:13:39,811][src.training_pipeline][INFO] - Starting training!
[2024-10-04 13:13:39,839][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 13:13:39,841][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 13:14:47,855][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 13:14:47,856][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 13:14:59,435][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 13:14:59,449][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 13:14:59,450][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 13:14:59,451][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 13:14:59,452][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 13:14:59,452][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 13:14:59,455][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 13:14:59,499][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 13:14:59,503][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 13:14:59,504][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 13:14:59,504][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 13:14:59,504][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 13:15:02,616][src.training_pipeline][INFO] - Starting training!
[2024-10-04 13:15:02,656][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 13:15:02,658][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 13:16:05,558][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 13:16:05,560][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 13:16:13,578][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 13:16:13,591][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 13:16:13,592][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 13:16:13,593][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 13:16:13,593][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 13:16:13,594][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 13:16:13,597][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 13:16:13,636][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 13:16:13,647][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 13:16:13,648][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 13:16:13,648][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 13:16:13,648][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 13:16:16,750][src.training_pipeline][INFO] - Starting training!
[2024-10-04 13:16:16,779][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 13:16:16,781][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 21:45:29,353][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 21:45:29,385][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 21:45:35,708][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 21:45:51,124][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 21:45:51,126][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 21:45:52,618][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 21:46:19,847][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 21:46:19,848][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 21:46:21,279][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 21:46:21,290][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 21:46:21,291][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 21:46:21,292][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 21:46:21,292][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 21:46:21,293][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 21:46:21,296][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 21:46:21,318][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 21:46:21,328][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 21:46:21,329][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 21:46:21,329][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 21:46:21,329][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 21:46:27,565][src.training_pipeline][INFO] - Starting training!
[2024-10-04 21:46:27,632][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 21:46:27,634][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 21:46:58,420][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 21:46:58,421][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 21:46:59,991][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 21:47:00,004][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 21:47:00,005][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 21:47:00,006][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 21:47:00,006][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 21:47:00,007][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 21:47:00,010][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 21:47:00,031][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 21:47:00,041][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 21:47:00,042][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 21:47:00,042][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 21:47:00,042][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 21:47:03,232][src.training_pipeline][INFO] - Starting training!
[2024-10-04 21:47:03,286][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 21:47:03,288][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 21:48:42,793][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-04 21:48:42,807][src.training_pipeline][INFO] - Starting testing!
[2024-10-04 21:48:42,808][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003.ckpt
[2024-10-04 21:48:42,812][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 21:48:42,816][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003.ckpt
[2024-10-04 21:48:44,482][src.training_pipeline][INFO] - Finalizing!
[2024-10-04 21:48:51,150][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_003.ckpt
[2024-10-04 21:49:10,373][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 21:49:10,374][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 21:49:11,839][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 21:49:11,851][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 21:49:11,852][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 21:49:11,853][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 21:49:11,853][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 21:49:11,854][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 21:49:11,857][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 21:49:11,877][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 21:49:11,880][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 21:49:11,881][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 21:49:11,881][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 21:49:11,881][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 21:49:14,807][src.training_pipeline][INFO] - Starting training!
[2024-10-04 21:49:14,858][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 21:49:14,861][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 21:50:46,723][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-04 21:50:46,736][src.training_pipeline][INFO] - Starting testing!
[2024-10-04 21:50:46,737][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004.ckpt
[2024-10-04 21:50:46,742][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 21:50:46,747][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004.ckpt
[2024-10-04 21:50:48,390][src.training_pipeline][INFO] - Finalizing!
[2024-10-04 21:50:54,465][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004.ckpt
[2024-10-04 22:00:11,773][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-04 22:00:11,783][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-04 22:00:13,184][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-04 22:00:13,198][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-04 22:00:13,199][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-04 22:00:13,200][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-04 22:00:13,200][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-04 22:00:13,200][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-04 22:00:13,203][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-04 22:00:13,246][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-04 22:00:13,254][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-04 22:00:13,254][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-04 22:00:13,255][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-04 22:00:13,255][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-04 22:00:16,295][src.training_pipeline][INFO] - Starting training!
[2024-10-04 22:00:16,366][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-04 22:00:16,368][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 22:01:48,721][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-04 22:01:48,735][src.training_pipeline][INFO] - Starting testing!
[2024-10-04 22:01:48,736][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v1.ckpt
[2024-10-04 22:01:48,742][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-04 22:01:48,746][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v1.ckpt
[2024-10-04 22:01:50,426][src.training_pipeline][INFO] - Finalizing!
[2024-10-04 22:01:56,095][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v1.ckpt
[2024-10-05 13:49:39,062][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 13:49:39,151][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 13:49:46,935][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 13:49:47,062][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 13:49:47,066][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 13:49:47,067][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 13:49:47,069][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 13:49:47,070][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 13:49:47,086][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 13:49:47,118][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 13:49:47,143][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 13:49:47,144][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 13:49:47,144][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 13:49:47,145][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 13:49:53,841][src.training_pipeline][INFO] - Starting training!
[2024-10-05 13:49:53,909][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 13:49:53,911][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 13:51:32,003][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-05 13:51:32,016][src.training_pipeline][INFO] - Starting testing!
[2024-10-05 13:51:32,017][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005.ckpt
[2024-10-05 13:51:32,021][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 13:51:32,026][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005.ckpt
[2024-10-05 13:51:33,745][src.training_pipeline][INFO] - Finalizing!
[2024-10-05 13:51:39,180][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005.ckpt
[2024-10-05 16:59:12,967][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 16:59:13,083][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 16:59:19,417][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 16:59:19,487][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 16:59:19,490][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 16:59:19,492][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 16:59:19,494][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 16:59:19,495][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 16:59:19,503][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 16:59:19,532][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 16:59:19,564][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 16:59:19,565][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 16:59:19,565][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 16:59:19,566][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 16:59:25,030][src.training_pipeline][INFO] - Starting training!
[2024-10-05 16:59:25,113][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 16:59:25,117][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 17:38:29,455][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 17:38:29,456][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 17:38:33,777][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 17:38:33,825][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 17:38:33,827][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 17:38:33,827][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 17:38:33,828][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 17:38:33,829][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 17:38:33,833][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 17:38:33,872][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 17:38:33,876][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 17:38:33,877][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 17:38:33,877][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 17:38:33,877][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 17:38:38,202][src.training_pipeline][INFO] - Starting training!
[2024-10-05 17:38:38,230][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 17:38:38,231][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 17:46:41,810][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 17:46:41,812][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 17:46:44,255][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 17:46:44,272][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 17:46:44,273][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 17:46:44,274][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 17:46:44,275][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 17:46:44,275][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 17:46:44,279][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 17:46:44,323][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 17:46:44,328][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 17:46:44,328][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 17:46:44,329][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 17:46:44,329][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 17:46:47,127][src.training_pipeline][INFO] - Starting training!
[2024-10-05 17:46:47,161][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 17:46:47,163][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 18:01:30,181][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 18:01:30,183][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 18:01:33,002][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 18:01:33,021][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 18:01:33,022][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 18:01:33,023][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 18:01:33,024][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 18:01:33,024][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 18:01:33,027][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 18:01:33,070][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 18:01:33,080][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 18:01:33,081][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 18:01:33,082][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 18:01:33,082][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 18:01:35,978][src.training_pipeline][INFO] - Starting training!
[2024-10-05 18:01:36,010][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 18:01:36,011][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 18:02:54,403][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 18:02:54,404][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 18:02:56,334][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 18:02:56,348][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 18:02:56,349][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 18:02:56,349][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 18:02:56,350][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 18:02:56,350][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 18:02:56,353][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 18:02:56,392][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 18:02:56,396][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 18:02:56,396][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 18:02:56,396][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 18:02:56,397][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 18:02:59,741][src.training_pipeline][INFO] - Starting training!
[2024-10-05 18:02:59,774][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 18:02:59,775][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 18:06:09,570][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-05 18:06:09,584][src.training_pipeline][INFO] - Starting testing!
[2024-10-05 18:06:09,585][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v1.ckpt
[2024-10-05 18:06:09,589][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 18:06:09,594][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v1.ckpt
[2024-10-05 18:06:14,682][src.training_pipeline][INFO] - Finalizing!
[2024-10-05 18:06:21,689][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v1.ckpt
[2024-10-05 20:12:38,535][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 20:12:38,537][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 20:12:40,659][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 20:12:40,674][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 20:12:40,675][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 20:12:40,676][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 20:12:40,676][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 20:12:40,677][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 20:12:40,680][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 20:12:40,723][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 20:12:40,733][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 20:12:40,735][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 20:12:40,735][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 20:12:40,735][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 20:12:44,652][src.training_pipeline][INFO] - Starting training!
[2024-10-05 20:12:44,688][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 20:12:44,690][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 20:20:54,047][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 20:20:54,049][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 20:20:56,679][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 20:20:56,694][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 20:20:56,696][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 20:20:56,696][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 20:20:56,697][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 20:20:56,697][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 20:20:56,700][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 20:20:56,744][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 20:20:56,756][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 20:20:56,757][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 20:20:56,757][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 20:20:56,757][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 20:20:59,488][src.training_pipeline][INFO] - Starting training!
[2024-10-05 20:20:59,525][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 20:20:59,527][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 20:57:53,137][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 20:57:53,153][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 20:57:55,259][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 20:57:55,279][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 20:57:55,280][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 20:57:55,280][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 20:57:55,281][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 20:57:55,281][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 20:57:55,284][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 20:57:55,326][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 20:57:55,334][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 20:57:55,335][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 20:57:55,335][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 20:57:55,335][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 20:57:59,588][src.training_pipeline][INFO] - Starting training!
[2024-10-05 20:57:59,619][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 20:57:59,621][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 20:58:33,558][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 20:58:33,559][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 20:58:35,373][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 20:58:35,401][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 20:58:35,403][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 20:58:35,404][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 20:58:35,405][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 20:58:35,405][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 20:58:35,409][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 20:58:35,452][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 20:58:35,456][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 20:58:35,457][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 20:58:35,457][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 20:58:35,457][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 20:58:38,591][src.training_pipeline][INFO] - Starting training!
[2024-10-05 20:58:38,618][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 20:58:38,619][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 21:01:45,073][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-05 21:01:45,087][src.training_pipeline][INFO] - Starting testing!
[2024-10-05 21:01:45,088][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v2.ckpt
[2024-10-05 21:01:45,092][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 21:01:45,096][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v2.ckpt
[2024-10-05 21:01:49,034][src.training_pipeline][INFO] - Finalizing!
[2024-10-05 21:01:56,584][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_005-v2.ckpt
[2024-10-05 21:05:18,909][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 21:05:18,911][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 21:05:20,067][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 21:05:20,083][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 21:05:20,084][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 21:05:20,085][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 21:05:20,085][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 21:05:20,086][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 21:05:20,089][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 21:05:20,109][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 21:05:20,113][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 21:05:20,114][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 21:05:20,114][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 21:05:20,114][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 21:05:23,028][src.training_pipeline][INFO] - Starting training!
[2024-10-05 21:05:23,097][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 21:05:23,101][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 21:49:30,837][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 21:49:30,872][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 21:49:37,008][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 21:49:37,153][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 21:49:37,157][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 21:49:37,158][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 21:49:37,158][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 21:49:37,159][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 21:49:37,162][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 21:49:37,208][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 21:49:37,214][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 21:49:37,214][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 21:49:37,214][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 21:49:37,215][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 21:49:41,902][src.training_pipeline][INFO] - Starting training!
[2024-10-05 21:49:41,939][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 21:49:41,941][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 21:50:20,617][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-10-05 21:53:18,048][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 21:53:18,050][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 21:53:20,982][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 21:53:21,000][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 21:53:21,001][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 21:53:21,002][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 21:53:21,002][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 21:53:21,003][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 21:53:21,006][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 21:53:21,046][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 21:53:21,051][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 21:53:21,052][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 21:53:21,052][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 21:53:21,052][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 21:53:25,061][src.training_pipeline][INFO] - Starting training!
[2024-10-05 21:53:25,092][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 21:53:25,094][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 21:55:10,629][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-10-05 21:58:02,412][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 21:58:02,413][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 21:58:06,879][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 21:58:06,908][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 21:58:06,910][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 21:58:06,911][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 21:58:06,911][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 21:58:06,912][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 21:58:06,916][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 21:58:06,958][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 21:58:06,965][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 21:58:06,966][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 21:58:06,966][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 21:58:06,967][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 21:58:31,345][src.training_pipeline][INFO] - Starting training!
[2024-10-05 21:58:31,376][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 21:58:31,378][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 21:59:51,697][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 21:59:51,698][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 21:59:56,450][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 21:59:56,468][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 21:59:56,469][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 21:59:56,470][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 21:59:56,470][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 21:59:56,471][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 21:59:56,474][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 21:59:56,513][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 21:59:56,518][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 21:59:56,519][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 21:59:56,519][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 21:59:56,520][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 21:59:59,686][src.training_pipeline][INFO] - Starting training!
[2024-10-05 21:59:59,723][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 21:59:59,725][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-05 22:18:17,447][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-05 22:18:17,449][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-05 22:18:22,049][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-05 22:18:22,163][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-05 22:18:22,164][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-05 22:18:22,165][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-05 22:18:22,165][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-05 22:18:22,166][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-05 22:18:22,169][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-05 22:18:22,210][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-05 22:18:22,216][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-05 22:18:22,216][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-05 22:18:22,216][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-05 22:18:22,217][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-05 22:18:25,373][src.training_pipeline][INFO] - Starting training!
[2024-10-05 22:18:25,407][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-05 22:18:25,409][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-06 19:44:46,314][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-06 19:44:46,408][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-06 19:44:53,640][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-06 20:01:01,040][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-06 20:01:01,110][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-06 20:01:05,435][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-06 20:01:05,702][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-06 20:01:05,704][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-06 20:01:05,705][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-06 20:01:05,706][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-06 20:01:05,707][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-06 20:01:05,729][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-06 20:01:05,782][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-06 20:01:05,803][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-06 20:01:05,804][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-06 20:01:05,804][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-06 20:01:05,804][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-06 20:01:16,258][src.training_pipeline][INFO] - Starting training!
[2024-10-06 20:01:16,304][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-06 20:01:16,306][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-06 20:23:28,370][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-06 20:23:28,372][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-06 20:23:30,019][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-06 20:23:30,112][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-06 20:23:30,114][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-06 20:23:30,114][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-06 20:23:30,115][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-06 20:23:30,115][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-06 20:23:30,118][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-06 20:23:30,157][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-06 20:23:30,162][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-06 20:23:30,163][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-06 20:23:30,163][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-06 20:23:30,164][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-06 20:23:33,033][src.training_pipeline][INFO] - Starting training!
[2024-10-06 20:23:33,087][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-06 20:23:33,088][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-06 20:24:04,412][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-06 20:24:04,413][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-06 20:24:05,550][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-06 20:24:05,592][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-06 20:24:05,593][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-06 20:24:05,594][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-06 20:24:05,594][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-06 20:24:05,595][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-06 20:24:05,598][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-06 20:24:05,648][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-06 20:24:05,658][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-06 20:24:05,658][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-06 20:24:05,659][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-06 20:24:05,659][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-06 20:24:08,579][src.training_pipeline][INFO] - Starting training!
[2024-10-06 20:24:08,651][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-06 20:24:08,654][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:09:59,625][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:09:59,689][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:10:06,716][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:10:06,978][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:10:06,980][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:10:06,981][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:10:06,982][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:10:06,983][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:10:06,992][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:10:07,048][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:10:07,072][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:10:07,073][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:10:07,074][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:10:07,074][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:10:13,133][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:10:13,204][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:10:13,206][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:10:32,624][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:10:32,625][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:10:33,540][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:10:33,558][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:10:33,559][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:10:33,560][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:10:33,560][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:10:33,561][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:10:33,564][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:10:33,614][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:10:33,625][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:10:33,626][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:10:33,626][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:10:33,626][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:10:36,421][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:10:36,481][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:10:36,482][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:15:33,402][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:15:33,403][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:15:34,336][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:15:34,354][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:15:34,356][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:15:34,356][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:15:34,357][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:15:34,357][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:15:34,360][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:15:34,410][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:15:34,420][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:15:34,421][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:15:34,421][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:15:34,422][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:15:37,493][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:15:37,562][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:15:37,565][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:20:41,573][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:20:41,576][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:20:42,507][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:20:42,527][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:20:42,528][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:20:42,528][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:20:42,529][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:20:42,529][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:20:42,532][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:20:42,581][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:20:42,591][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:20:42,592][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:20:42,592][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:20:42,593][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:20:45,362][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:20:45,414][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:20:45,415][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:22:08,316][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:22:08,317][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:22:09,227][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:22:09,246][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:22:09,247][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:22:09,248][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:22:09,248][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:22:09,249][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:22:09,252][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:22:09,303][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:22:09,313][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:22:09,314][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:22:09,315][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:22:09,315][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:22:12,072][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:22:12,136][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:22:12,138][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:23:11,198][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:23:11,200][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:23:12,118][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:23:12,137][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:23:12,138][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:23:12,139][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:23:12,139][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:23:12,140][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:23:12,143][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:23:12,164][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:23:12,174][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:23:12,175][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:23:12,175][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:23:12,175][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:23:14,857][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:23:14,921][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:23:14,923][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:23:53,494][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-10-07 13:26:17,489][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:26:17,490][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:26:18,411][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:26:18,430][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:26:18,431][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:26:18,432][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:26:18,432][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:26:18,433][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:26:18,436][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:26:18,455][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:26:18,459][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:26:18,459][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:26:18,459][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:26:18,459][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:26:21,718][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:26:21,781][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:26:21,783][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:26:37,876][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:26:37,877][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:26:38,787][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:26:38,806][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:26:38,807][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:26:38,808][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:26:38,808][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:26:38,809][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:26:38,812][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:26:38,833][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:26:38,844][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:26:38,845][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:26:38,845][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:26:38,845][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:26:42,138][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:26:42,205][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:26:42,207][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:26:55,190][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-10-07 13:29:44,588][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:29:44,590][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:29:45,506][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:29:45,525][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:29:45,526][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:29:45,527][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:29:45,527][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:29:45,528][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:29:45,531][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:29:45,583][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:29:45,593][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:29:45,593][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:29:45,594][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:29:45,594][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:29:48,882][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:29:48,946][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:29:48,947][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:31:46,336][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:31:46,338][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:31:47,269][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:31:47,288][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:31:47,289][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:31:47,290][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:31:47,290][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:31:47,291][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:31:47,294][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:31:47,314][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:31:47,318][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:31:47,318][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:31:47,318][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:31:47,318][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:31:51,077][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:31:51,141][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:31:51,143][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:32:05,233][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:32:05,234][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:32:06,155][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:32:06,174][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:32:06,175][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:32:06,176][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:32:06,176][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:32:06,176][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:32:06,180][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:32:06,200][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:32:06,203][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:32:06,204][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:32:06,204][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:32:06,204][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:32:10,222][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:32:10,280][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:32:10,282][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 13:32:30,708][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 13:32:30,709][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 13:32:31,639][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 13:32:31,658][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 13:32:31,659][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 13:32:31,659][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 13:32:31,660][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 13:32:31,660][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 13:32:31,663][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 13:32:31,685][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 13:32:31,695][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 13:32:31,696][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 13:32:31,697][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 13:32:31,697][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 13:32:35,083][src.training_pipeline][INFO] - Starting training!
[2024-10-07 13:32:35,149][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 13:32:35,151][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 15:49:50,471][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 15:49:50,549][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 15:49:53,094][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 15:49:53,113][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 15:49:53,114][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 15:49:53,115][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 15:49:53,115][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 15:49:53,116][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 15:49:53,119][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 15:49:53,140][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 15:49:53,144][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 15:49:53,144][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 15:49:53,144][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 15:49:53,144][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 15:49:57,169][src.training_pipeline][INFO] - Starting training!
[2024-10-07 15:49:57,231][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 15:49:57,233][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 15:50:25,190][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 15:50:25,192][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 15:50:26,920][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 15:50:26,939][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 15:50:26,940][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 15:50:26,941][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 15:50:26,941][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 15:50:26,942][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 15:50:26,945][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 15:50:26,985][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 15:50:26,990][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 15:50:26,990][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 15:50:26,990][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 15:50:26,991][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 15:50:30,046][src.training_pipeline][INFO] - Starting training!
[2024-10-07 15:50:30,101][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 15:50:30,103][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 15:51:58,170][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 15:51:58,171][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 15:51:59,841][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 15:51:59,860][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 15:51:59,861][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 15:51:59,862][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 15:51:59,862][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 15:51:59,863][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 15:51:59,866][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 15:51:59,914][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 15:51:59,925][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 15:51:59,925][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 15:51:59,926][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 15:51:59,926][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 15:52:02,893][src.training_pipeline][INFO] - Starting training!
[2024-10-07 15:52:02,946][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 15:52:02,948][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 15:54:26,512][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 15:54:26,514][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 15:54:28,697][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 15:54:28,716][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 15:54:28,717][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 15:54:28,718][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 15:54:28,718][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 15:54:28,718][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 15:54:28,722][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 15:54:28,742][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 15:54:28,746][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 15:54:28,746][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 15:54:28,746][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 15:54:28,747][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 15:54:31,716][src.training_pipeline][INFO] - Starting training!
[2024-10-07 15:54:31,778][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 15:54:31,780][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 15:54:56,794][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 15:54:56,797][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 15:54:58,090][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 15:54:58,109][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 15:54:58,110][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 15:54:58,111][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 15:54:58,111][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 15:54:58,111][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 15:54:58,114][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 15:54:58,158][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 15:54:58,164][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 15:54:58,165][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 15:54:58,165][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 15:54:58,166][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 15:55:01,051][src.training_pipeline][INFO] - Starting training!
[2024-10-07 15:55:01,109][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 15:55:01,110][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 15:55:46,579][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-10-07 16:00:03,293][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:00:03,294][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:00:04,676][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:00:04,696][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:00:04,697][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:00:04,698][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:00:04,698][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:00:04,699][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:00:04,702][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:00:04,743][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:00:04,749][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:00:04,749][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:00:04,749][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:00:04,750][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:00:07,415][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:00:07,472][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:00:07,474][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:00:19,180][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:00:19,183][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:00:20,554][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:00:20,571][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:00:20,573][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:00:20,573][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:00:20,574][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:00:20,574][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:00:20,577][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:00:20,598][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:00:20,601][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:00:20,601][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:00:20,601][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:00:20,602][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:00:23,563][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:00:23,636][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:00:23,638][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:00:46,385][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:00:46,386][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:00:47,664][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:00:47,684][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:00:47,685][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:00:47,685][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:00:47,686][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:00:47,686][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:00:47,689][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:00:47,740][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:00:47,750][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:00:47,751][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:00:47,751][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:00:47,751][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:00:50,720][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:00:50,775][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:00:50,779][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:01:11,136][pytorch_lightning.utilities.rank_zero][INFO] - 
Detected KeyboardInterrupt, attempting graceful shutdown ...
[2024-10-07 16:03:22,557][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:03:22,559][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:03:23,971][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:03:23,988][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:03:23,990][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:03:23,990][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:03:23,991][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:03:23,991][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:03:23,994][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:03:24,044][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:03:24,053][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:03:24,053][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:03:24,054][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:03:24,054][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:03:27,088][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:03:27,152][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:03:27,154][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:05:32,889][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 16:05:32,906][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 16:05:32,907][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010.ckpt
[2024-10-07 16:05:32,916][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:05:32,922][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010.ckpt
[2024-10-07 16:05:34,705][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 16:05:40,602][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010.ckpt
[2024-10-07 16:09:18,396][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:09:18,397][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:09:19,854][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:09:19,873][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:09:19,874][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:09:19,875][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:09:19,875][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:09:19,876][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:09:19,879][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:09:19,900][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:09:19,910][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:09:19,911][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:09:19,912][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:09:19,912][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:09:24,590][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:09:24,666][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:09:24,669][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:11:31,884][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 16:11:31,900][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 16:11:31,901][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v1.ckpt
[2024-10-07 16:11:31,917][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:11:31,923][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v1.ckpt
[2024-10-07 16:11:33,607][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 16:11:40,782][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v1.ckpt
[2024-10-07 16:14:02,729][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:14:02,730][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:14:04,189][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:14:04,209][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:14:04,210][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:14:04,211][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:14:04,211][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:14:04,211][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:14:04,214][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:14:04,266][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:14:04,277][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:14:04,278][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:14:04,278][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:14:04,279][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:14:06,955][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:14:07,004][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:14:07,006][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:16:11,702][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 16:16:11,718][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 16:16:11,719][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v2.ckpt
[2024-10-07 16:16:11,733][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:16:11,742][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v2.ckpt
[2024-10-07 16:16:13,466][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 16:16:21,067][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v2.ckpt
[2024-10-07 16:20:10,583][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:20:10,584][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:20:12,017][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:20:12,036][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:20:12,037][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:20:12,038][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:20:12,038][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:20:12,039][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:20:12,042][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:20:12,062][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:20:12,066][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:20:12,067][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:20:12,067][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:20:12,067][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:20:16,179][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:20:16,254][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:20:16,257][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:20:28,488][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:20:28,489][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:20:29,925][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:20:29,944][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:20:29,945][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:20:29,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:20:29,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:20:29,947][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:20:29,950][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:20:29,998][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:20:30,007][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:20:30,009][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:20:30,009][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:20:30,009][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:20:32,978][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:20:33,026][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:20:33,028][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:22:39,668][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 16:22:39,684][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 16:22:39,685][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v3.ckpt
[2024-10-07 16:22:39,699][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:22:39,708][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v3.ckpt
[2024-10-07 16:22:41,409][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 16:22:48,575][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v3.ckpt
[2024-10-07 16:24:52,184][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:24:52,185][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:24:53,632][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:24:53,653][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:24:53,654][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:24:53,655][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:24:53,655][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:24:53,655][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:24:53,658][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:24:53,710][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:24:53,720][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:24:53,721][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:24:53,721][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:24:53,722][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:24:56,729][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:24:56,783][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:24:56,785][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:25:57,588][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:25:57,589][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:25:59,081][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:25:59,104][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:25:59,105][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:25:59,106][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:25:59,106][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:25:59,107][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:25:59,110][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:25:59,155][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:25:59,163][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:25:59,164][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:25:59,164][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:25:59,165][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:26:02,178][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:26:02,230][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:26:02,232][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:26:32,303][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:26:32,304][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:26:33,717][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:26:33,736][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:26:33,737][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:26:33,738][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:26:33,738][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:26:33,739][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:26:33,742][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:26:33,783][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:26:33,793][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:26:33,794][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:26:33,794][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:26:33,794][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:26:36,792][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:26:36,852][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:26:36,854][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:28:43,948][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 16:28:43,965][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 16:28:43,966][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v3.ckpt
[2024-10-07 16:28:43,976][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:28:43,982][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v3.ckpt
[2024-10-07 16:28:45,675][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 16:28:52,292][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v3.ckpt
[2024-10-07 16:32:17,045][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:32:17,046][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:32:18,497][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:32:18,532][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:32:18,534][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:32:18,534][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:32:18,535][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:32:18,535][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:32:18,538][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:32:18,581][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:32:18,592][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:32:18,593][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:32:18,593][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:32:18,594][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:32:21,560][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:32:21,616][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:32:21,618][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:34:24,458][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 16:34:24,475][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 16:34:24,475][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v4.ckpt
[2024-10-07 16:34:24,486][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:34:24,492][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v4.ckpt
[2024-10-07 16:34:26,208][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 16:34:34,829][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v4.ckpt
[2024-10-07 16:35:49,107][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:35:49,108][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:35:50,576][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:35:50,630][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:35:50,631][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:35:50,632][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:35:50,632][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:35:50,633][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:35:50,636][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:35:50,678][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:35:50,682][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:35:50,683][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:35:50,683][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:35:50,683][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:35:54,344][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:35:54,407][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:35:54,409][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:36:11,474][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:36:11,476][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:36:12,926][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:36:12,945][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:36:12,946][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:36:12,947][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:36:12,947][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:36:12,948][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:36:12,951][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:36:12,994][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:36:13,000][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:36:13,001][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:36:13,001][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:36:13,001][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:36:15,910][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:36:15,973][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:36:15,974][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:43:34,195][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:43:34,196][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:43:35,497][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:43:35,516][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:43:35,517][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:43:35,518][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:43:35,518][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:43:35,519][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:43:35,522][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:43:35,571][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:43:35,581][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:43:35,581][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:43:35,582][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:43:35,582][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:43:38,486][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:43:38,537][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:43:38,539][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:43:56,636][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:43:56,637][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:43:58,044][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:43:58,064][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:43:58,066][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:43:58,066][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:43:58,067][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:43:58,067][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:43:58,070][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:43:58,119][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:43:58,128][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:43:58,128][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:43:58,129][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:43:58,129][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:44:01,013][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:44:01,079][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:44:01,081][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:49:24,826][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:49:24,827][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:49:26,179][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:49:26,199][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:49:26,200][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:49:26,200][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:49:26,201][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:49:26,201][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:49:26,204][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:49:26,256][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:49:26,265][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:49:26,265][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:49:26,266][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:49:26,266][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:49:29,254][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:49:29,324][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:49:29,326][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:51:37,274][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 16:51:37,291][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 16:51:37,291][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v5.ckpt
[2024-10-07 16:51:37,302][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:51:37,308][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v5.ckpt
[2024-10-07 16:51:39,009][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 16:51:45,141][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v5.ckpt
[2024-10-07 16:52:42,774][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 16:52:42,776][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 16:52:44,083][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 16:52:44,102][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 16:52:44,104][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 16:52:44,104][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 16:52:44,105][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 16:52:44,105][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 16:52:44,108][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 16:52:44,160][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 16:52:44,170][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 16:52:44,171][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 16:52:44,171][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 16:52:44,172][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 16:52:47,076][src.training_pipeline][INFO] - Starting training!
[2024-10-07 16:52:47,148][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 16:52:47,151][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:54:53,783][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 16:54:53,799][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 16:54:53,800][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v6.ckpt
[2024-10-07 16:54:53,815][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 16:54:53,828][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v6.ckpt
[2024-10-07 16:54:55,531][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 16:55:01,138][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v6.ckpt
[2024-10-07 17:01:23,423][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 17:01:23,424][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 17:01:24,795][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 17:01:24,814][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 17:01:24,816][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 17:01:24,816][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 17:01:24,817][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 17:01:24,817][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 17:01:24,820][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 17:01:24,872][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 17:01:24,882][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 17:01:24,882][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 17:01:24,883][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 17:01:24,883][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 17:01:27,661][src.training_pipeline][INFO] - Starting training!
[2024-10-07 17:01:27,736][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 17:01:27,739][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:41:29,386][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 19:41:29,387][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 19:41:30,497][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 19:41:30,517][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 19:41:30,518][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 19:41:30,519][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 19:41:30,519][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 19:41:30,520][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 19:41:30,523][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 19:41:30,544][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 19:41:30,554][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 19:41:30,555][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 19:41:30,555][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 19:41:30,556][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 19:41:33,663][src.training_pipeline][INFO] - Starting training!
[2024-10-07 19:41:33,721][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 19:41:33,722][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:43:38,384][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 19:43:38,401][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 19:43:38,402][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v7.ckpt
[2024-10-07 19:43:38,412][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:43:38,418][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v7.ckpt
[2024-10-07 19:43:40,114][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 19:43:47,908][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v7.ckpt
[2024-10-07 19:46:38,920][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 19:46:38,937][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 19:46:42,033][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 19:46:42,047][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 19:46:42,048][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 19:46:42,049][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 19:46:42,049][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 19:46:42,050][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 19:46:42,053][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 19:46:42,075][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 19:46:42,085][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 19:46:42,086][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 19:46:42,087][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 19:46:42,087][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 19:46:47,229][src.training_pipeline][INFO] - Starting training!
[2024-10-07 19:46:47,286][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 19:46:47,288][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:47:09,828][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 19:47:09,829][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 19:47:10,766][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 19:47:10,782][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 19:47:10,783][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 19:47:10,783][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 19:47:10,784][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 19:47:10,784][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 19:47:10,788][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 19:47:10,809][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 19:47:10,819][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 19:47:10,820][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 19:47:10,820][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 19:47:10,820][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 19:47:13,586][src.training_pipeline][INFO] - Starting training!
[2024-10-07 19:47:13,643][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 19:47:13,644][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:48:46,579][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 19:48:46,593][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 19:48:46,594][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v8.ckpt
[2024-10-07 19:48:46,600][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:48:46,605][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v8.ckpt
[2024-10-07 19:48:48,256][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 19:48:55,730][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_008-v8.ckpt
[2024-10-07 19:50:16,592][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 19:50:16,593][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 19:50:17,616][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 19:50:17,629][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 19:50:17,630][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 19:50:17,631][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 19:50:17,631][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 19:50:17,632][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 19:50:17,635][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 19:50:17,656][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 19:50:17,666][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 19:50:17,667][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 19:50:17,667][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 19:50:17,667][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 19:50:20,537][src.training_pipeline][INFO] - Starting training!
[2024-10-07 19:50:20,592][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 19:50:20,593][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:53:16,814][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 19:53:16,827][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 19:53:16,828][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v2.ckpt
[2024-10-07 19:53:16,842][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:53:16,848][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v2.ckpt
[2024-10-07 19:53:20,807][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 19:53:26,097][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v2.ckpt
[2024-10-07 19:55:13,144][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 19:55:13,146][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 19:55:14,071][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 19:55:14,091][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 19:55:14,092][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 19:55:14,092][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 19:55:14,093][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 19:55:14,093][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 19:55:14,096][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 19:55:14,147][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 19:55:14,157][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 19:55:14,158][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 19:55:14,159][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 19:55:14,159][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 19:55:16,814][src.training_pipeline][INFO] - Starting training!
[2024-10-07 19:55:16,868][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 19:55:16,870][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:56:57,344][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 19:56:57,345][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 19:56:58,347][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 19:56:58,365][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 19:56:58,366][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 19:56:58,367][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 19:56:58,367][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 19:56:58,367][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 19:56:58,370][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 19:56:58,422][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 19:56:58,432][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 19:56:58,433][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 19:56:58,433][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 19:56:58,433][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 19:57:01,114][src.training_pipeline][INFO] - Starting training!
[2024-10-07 19:57:01,180][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 19:57:01,182][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:57:44,191][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 19:57:44,192][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 19:57:45,121][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 19:57:45,140][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 19:57:45,141][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 19:57:45,142][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 19:57:45,143][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 19:57:45,143][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 19:57:45,146][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 19:57:45,185][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 19:57:45,191][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 19:57:45,192][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 19:57:45,192][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 19:57:45,192][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 19:57:47,833][src.training_pipeline][INFO] - Starting training!
[2024-10-07 19:57:47,888][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 19:57:47,890][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:59:54,642][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 19:59:54,659][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 19:59:54,659][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v4.ckpt
[2024-10-07 19:59:54,670][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 19:59:54,675][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v4.ckpt
[2024-10-07 19:59:56,361][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 20:00:03,631][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v4.ckpt
[2024-10-07 20:01:54,856][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 20:01:54,875][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 20:01:55,815][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 20:01:55,898][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 20:01:55,901][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 20:01:55,902][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 20:01:55,903][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 20:01:55,904][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 20:01:55,910][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 20:01:55,936][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 20:01:55,946][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 20:01:55,947][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 20:01:55,947][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 20:01:55,947][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 20:01:59,081][src.training_pipeline][INFO] - Starting training!
[2024-10-07 20:01:59,159][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 20:01:59,161][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:04:04,075][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 20:04:04,076][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 20:04:05,238][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 20:04:05,258][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 20:04:05,259][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 20:04:05,260][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 20:04:05,260][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 20:04:05,261][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 20:04:05,264][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 20:04:05,306][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 20:04:05,312][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 20:04:05,313][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 20:04:05,313][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 20:04:05,314][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 20:04:08,947][src.training_pipeline][INFO] - Starting training!
[2024-10-07 20:04:08,997][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 20:04:09,001][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:05:01,412][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 20:05:01,414][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 20:05:02,343][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 20:05:02,361][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 20:05:02,362][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 20:05:02,363][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 20:05:02,363][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 20:05:02,364][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 20:05:02,367][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 20:05:02,419][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 20:05:02,429][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 20:05:02,430][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 20:05:02,430][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 20:05:02,431][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 20:05:05,795][src.training_pipeline][INFO] - Starting training!
[2024-10-07 20:05:05,862][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 20:05:05,864][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:09:39,408][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 20:09:39,409][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 20:09:40,352][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 20:09:40,371][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 20:09:40,372][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 20:09:40,372][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 20:09:40,373][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 20:09:40,373][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 20:09:40,383][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 20:09:40,440][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 20:09:40,450][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 20:09:40,450][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 20:09:40,451][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 20:09:40,451][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 20:09:43,563][src.training_pipeline][INFO] - Starting training!
[2024-10-07 20:09:43,600][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 20:09:43,603][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:11:52,474][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 20:11:52,490][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 20:11:52,491][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v5.ckpt
[2024-10-07 20:11:52,500][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:11:52,506][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v5.ckpt
[2024-10-07 20:11:54,218][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 20:12:01,857][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v5.ckpt
[2024-10-07 20:13:28,116][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 20:13:28,140][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 20:13:30,513][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 20:13:30,533][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 20:13:30,535][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 20:13:30,535][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 20:13:30,536][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 20:13:30,536][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 20:13:30,539][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 20:13:30,563][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 20:13:30,573][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 20:13:30,574][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 20:13:30,574][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 20:13:30,574][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 20:13:34,976][src.training_pipeline][INFO] - Starting training!
[2024-10-07 20:13:35,021][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 20:13:35,025][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:13:57,249][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 20:13:57,250][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 20:13:58,253][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 20:13:58,266][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 20:13:58,267][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 20:13:58,268][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 20:13:58,268][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 20:13:58,269][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 20:13:58,272][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 20:13:58,322][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 20:13:58,332][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 20:13:58,333][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 20:13:58,333][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 20:13:58,334][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 20:14:01,283][src.training_pipeline][INFO] - Starting training!
[2024-10-07 20:14:01,311][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 20:14:01,314][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:16:58,056][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 20:16:58,071][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 20:16:58,072][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v3.ckpt
[2024-10-07 20:16:58,077][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:16:58,082][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v3.ckpt
[2024-10-07 20:17:02,055][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 20:17:09,074][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v3.ckpt
[2024-10-07 20:19:42,147][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 20:19:42,148][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 20:19:43,349][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-07 20:19:43,369][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 20:19:43,370][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 20:19:43,371][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 20:19:43,371][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 20:19:43,372][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 20:19:43,375][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 20:19:43,396][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 20:19:43,406][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 20:19:43,407][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 20:19:43,407][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 20:19:43,408][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 20:19:46,159][src.training_pipeline][INFO] - Starting training!
[2024-10-07 20:19:46,203][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 20:19:46,207][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:23:31,004][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=30` reached.
[2024-10-07 20:23:31,021][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 20:23:31,022][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v8.ckpt
[2024-10-07 20:23:31,034][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 20:23:31,040][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v8.ckpt
[2024-10-07 20:23:32,758][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 20:23:39,884][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_029-v8.ckpt
[2024-10-07 21:57:27,625][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 21:57:27,627][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 21:57:30,454][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 21:57:45,641][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 21:57:45,642][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 21:57:52,371][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:05:22,089][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:05:22,090][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:05:23,542][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:05:23,555][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:05:23,556][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:05:23,557][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:05:23,557][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:05:23,557][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:05:23,560][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:05:23,601][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:05:23,609][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:05:23,610][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:05:23,610][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:05:23,610][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:05:27,827][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:05:27,867][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:05:27,870][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:06:36,862][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:06:36,864][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:06:40,504][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:06:40,516][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:06:40,517][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:06:40,518][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:06:40,518][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:06:40,519][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:06:40,522][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:06:40,562][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:06:40,572][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:06:40,572][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:06:40,573][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:06:40,573][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:06:43,943][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:06:43,984][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:06:43,987][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:08:33,015][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:08:33,016][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:08:36,948][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:08:37,011][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:08:37,014][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:08:37,016][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:08:37,017][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:08:37,018][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:08:37,024][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:08:37,071][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:08:37,080][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:08:37,082][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:08:37,082][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:08:37,082][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:08:40,165][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:08:40,209][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:08:40,213][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:09:16,010][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:09:16,011][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:09:21,693][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:09:21,705][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:09:21,706][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:09:21,707][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:09:21,707][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:09:21,708][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:09:21,711][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:09:21,761][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:09:21,771][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:09:21,772][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:09:21,772][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:09:21,773][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:09:24,908][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:09:24,940][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:09:24,943][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:11:09,608][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:11:09,610][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:11:16,973][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:11:16,987][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:11:16,988][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:11:16,989][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:11:16,989][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:11:16,990][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:11:16,993][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:11:17,036][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:11:17,046][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:11:17,047][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:11:17,047][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:11:17,048][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:11:20,078][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:11:20,115][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:11:20,118][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:15:06,699][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-10-07 22:15:06,713][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 22:15:06,713][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v4.ckpt
[2024-10-07 22:15:06,719][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:15:06,722][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v4.ckpt
[2024-10-07 22:18:16,496][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:18:16,497][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:18:21,503][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:18:21,523][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:18:21,524][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:18:21,524][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:18:21,525][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:18:21,525][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:18:21,528][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:18:21,578][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:18:21,587][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:18:21,588][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:18:21,588][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:18:21,588][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:18:24,531][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:18:24,572][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:18:24,575][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:22:11,965][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-10-07 22:22:11,979][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 22:22:11,979][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v5.ckpt
[2024-10-07 22:22:11,985][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:22:11,988][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v5.ckpt
[2024-10-07 22:22:14,978][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 22:22:20,300][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_004-v5.ckpt
[2024-10-07 22:25:47,352][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:25:47,353][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:25:48,945][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:25:48,956][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:25:48,957][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:25:48,958][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:25:48,958][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:25:48,959][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:25:48,962][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:25:49,009][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:25:49,019][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:25:49,021][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:25:49,021][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:25:49,021][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:25:53,093][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:25:53,128][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:25:53,131][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:27:46,918][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=20` reached.
[2024-10-07 22:27:46,931][src.training_pipeline][INFO] - Starting testing!
[2024-10-07 22:27:46,932][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_019.ckpt
[2024-10-07 22:27:46,936][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:27:46,939][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_019.ckpt
[2024-10-07 22:27:50,101][src.training_pipeline][INFO] - Finalizing!
[2024-10-07 22:27:57,200][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_019.ckpt
[2024-10-07 22:31:04,844][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:31:04,845][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:31:08,075][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:31:08,086][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:31:08,087][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:31:08,088][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:31:08,089][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:31:08,089][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:31:08,092][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:31:08,143][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:31:08,152][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:31:08,153][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:31:08,153][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:31:08,153][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:31:11,074][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:31:11,113][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:31:11,117][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-07 22:31:52,888][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-07 22:31:52,889][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-07 22:31:54,901][src.training_pipeline][INFO] - Instantiating model <src.models.fair_lightening_module.fair_ClassificationLitModule>
[2024-10-07 22:31:54,912][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-07 22:31:54,913][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-07 22:31:54,914][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-07 22:31:54,914][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-07 22:31:54,915][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-07 22:31:54,918][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-07 22:31:54,967][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-07 22:31:54,977][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-07 22:31:54,978][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-07 22:31:54,978][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-07 22:31:54,978][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-07 22:31:58,279][src.training_pipeline][INFO] - Starting training!
[2024-10-07 22:31:58,311][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-07 22:31:58,313][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 15:41:02,302][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-08 15:41:02,363][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-08 15:41:10,388][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-08 15:41:55,059][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-08 15:41:55,061][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-08 15:41:57,214][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-08 15:43:21,308][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-08 15:43:21,310][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-08 15:43:22,581][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-08 15:43:22,627][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-08 15:43:22,629][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-08 15:43:22,630][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-08 15:43:22,631][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-08 15:43:22,632][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-08 15:43:22,638][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-08 15:43:22,691][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-08 15:43:22,716][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-08 15:43:22,717][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-08 15:43:22,718][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-08 15:43:22,718][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-08 15:43:31,845][src.training_pipeline][INFO] - Starting training!
[2024-10-08 15:43:31,917][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-08 15:43:31,921][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 16:31:28,135][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-08 16:31:28,205][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-08 16:31:34,229][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-08 16:31:34,290][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-08 16:31:34,291][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-08 16:31:34,292][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-08 16:31:34,292][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-08 16:31:34,293][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-08 16:31:34,296][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-08 16:31:34,337][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-08 16:31:34,341][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-08 16:31:34,342][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-08 16:31:34,342][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-08 16:31:34,342][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-08 16:31:42,955][src.training_pipeline][INFO] - Starting training!
[2024-10-08 16:31:42,990][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-08 16:31:42,992][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 16:53:05,798][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-08 16:53:05,835][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-08 16:53:10,396][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-08 16:53:10,430][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-08 16:53:10,431][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-08 16:53:10,432][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-08 16:53:10,432][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-08 16:53:10,433][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-08 16:53:10,437][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-08 16:53:10,459][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-08 16:53:10,463][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-08 16:53:10,463][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-08 16:53:10,463][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-08 16:53:10,463][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-08 16:53:13,672][src.training_pipeline][INFO] - Starting training!
[2024-10-08 16:53:13,704][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-08 16:53:13,707][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 16:55:21,059][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=15` reached.
[2024-10-08 16:55:21,075][src.training_pipeline][INFO] - Starting testing!
[2024-10-08 16:55:21,076][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v6.ckpt
[2024-10-08 16:55:21,101][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 16:55:21,109][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v6.ckpt
[2024-10-08 16:55:23,801][src.training_pipeline][INFO] - Finalizing!
[2024-10-08 16:55:31,419][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v6.ckpt
[2024-10-08 16:56:21,690][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-08 16:56:21,691][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-08 16:56:25,211][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-08 16:56:25,239][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-08 16:56:25,241][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-08 16:56:25,241][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-08 16:56:25,242][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-08 16:56:25,242][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-08 16:56:25,245][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-08 16:56:25,285][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-08 16:56:25,292][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-08 16:56:25,293][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-08 16:56:25,293][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-08 16:56:25,293][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-08 16:56:28,161][src.training_pipeline][INFO] - Starting training!
[2024-10-08 16:56:28,196][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-08 16:56:28,200][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 16:58:51,148][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-08 16:58:51,150][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-08 16:58:53,735][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-08 16:58:53,754][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-08 16:58:53,756][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-08 16:58:53,756][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-08 16:58:53,757][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-08 16:58:53,757][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-08 16:58:53,760][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-08 16:58:53,780][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-08 16:58:53,784][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-08 16:58:53,784][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-08 16:58:53,784][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-08 16:58:53,784][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-08 16:58:56,784][src.training_pipeline][INFO] - Starting training!
[2024-10-08 16:58:56,815][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-08 16:58:56,818][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 16:59:46,229][lightning_fabric.utilities.seed][INFO] - Seed set to 12345
[2024-10-08 16:59:46,230][src.training_pipeline][INFO] - Instantiating datamodule <src.datamodules.classification_datamodule.ClassificationDataModule>
[2024-10-08 16:59:48,357][src.training_pipeline][INFO] - Instantiating model <src.models.lightening_module.ClassificationLitModule>
[2024-10-08 16:59:48,381][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2024-10-08 16:59:48,383][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2024-10-08 16:59:48,384][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2024-10-08 16:59:48,384][src.training_pipeline][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
[2024-10-08 16:59:48,385][src.training_pipeline][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2024-10-08 16:59:48,388][src.training_pipeline][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-10-08 16:59:48,432][pytorch_lightning.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-10-08 16:59:48,436][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
[2024-10-08 16:59:48,436][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-10-08 16:59:48,436][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-10-08 16:59:48,436][src.training_pipeline][INFO] - Logging hyperparameters!
[2024-10-08 16:59:51,429][src.training_pipeline][INFO] - Starting training!
[2024-10-08 16:59:51,460][pytorch_lightning.utilities.rank_zero][INFO] - You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[2024-10-08 16:59:51,461][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 17:02:06,618][pytorch_lightning.utilities.rank_zero][INFO] - `Trainer.fit` stopped: `max_epochs=15` reached.
[2024-10-08 17:02:06,665][src.training_pipeline][INFO] - Starting testing!
[2024-10-08 17:02:06,666][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v7.ckpt
[2024-10-08 17:02:06,676][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2024-10-08 17:02:06,689][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from the checkpoint at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v7.ckpt
[2024-10-08 17:02:08,814][src.training_pipeline][INFO] - Finalizing!
[2024-10-08 17:02:15,362][src.training_pipeline][INFO] - Best model ckpt at /local/scratch/a/ko120/DM-Benchmark/checkpoints/epoch_010-v7.ckpt
